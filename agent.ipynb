{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competitors research agent\n",
    "Steps for competitors research:\n",
    "1. Find list of competitors  \n",
    "2. Go to their website  \n",
    "3. Collect information including:  \n",
    "\t- standout features\n",
    "\t- product and pricing tiers\n",
    "\t- unique service proposition\n",
    "\t- marketing messages  \n",
    "4. Analyze competitors  \n",
    "\t- identify common patterns  \n",
    "\t- spot potential gaps  \n",
    "\t- compare pricing strategies  \n",
    "\t- compare messaging themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "import asyncio\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "\n",
    "from playwright.async_api import async_playwright\n",
    "from typing import Any, Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchLogger:\n",
    "    def __init__(self, research_id: str):\n",
    "        self.research_id = research_id\n",
    "        self.log_file = f\"research_logs/{research_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "        self.json_log_file = f\"research_logs/{research_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_data.json\"\n",
    "        \n",
    "        # Create logs directory if it doesn't exist\n",
    "        os.makedirs(\"research_logs\", exist_ok=True)\n",
    "        \n",
    "        # Set up file handler\n",
    "        self.file_handler = logging.FileHandler(self.log_file)\n",
    "        self.file_handler.setFormatter(\n",
    "            logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        )\n",
    "        \n",
    "        # Add file handler to root logger\n",
    "        logging.getLogger('').addHandler(self.file_handler)\n",
    "        \n",
    "        self.research_data = {\n",
    "            \"research_id\": research_id,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"steps\": []\n",
    "        }\n",
    "\n",
    "    def log_step(self, agent_name: str, action: str, input_data: Any, output_data: Any):\n",
    "        \"\"\"Log a research step with both input and output data\"\"\"\n",
    "        step_data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"agent\": agent_name,\n",
    "            \"action\": action,\n",
    "            \"input\": input_data,\n",
    "            \"output\": output_data\n",
    "        }\n",
    "        \n",
    "        self.research_data[\"steps\"].append(step_data)\n",
    "        \n",
    "        # Log to file\n",
    "        logger.info(f\"Agent: {agent_name} | Action: {action}\")\n",
    "        try:\n",
    "            logger.debug(f\"Input: {json.dumps(input_data, indent=2)}\")\n",
    "            logger.debug(f\"Output: {json.dumps(output_data, indent=2)}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error logging input/output data: {str(e)}\")\n",
    "            logger.debug(f\"Input: {input_data}\")\n",
    "            logger.debug(f\"Output: {output_data}\")\n",
    "        \n",
    "        # Save updated research data to JSON file\n",
    "        self._save_research_data()\n",
    "\n",
    "    def log_error(self, agent_name: str, action: str, error: Exception, context: Dict = None):\n",
    "        \"\"\"Log error information\"\"\"\n",
    "        error_data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"agent\": agent_name,\n",
    "            \"action\": action,\n",
    "            \"error\": str(error),\n",
    "            \"error_type\": type(error).__name__,\n",
    "            \"context\": context\n",
    "        }\n",
    "        \n",
    "        self.research_data[\"errors\"] = self.research_data.get(\"errors\", [])\n",
    "        self.research_data[\"errors\"].append(error_data)\n",
    "        \n",
    "        logger.error(f\"Error in {agent_name} during {action}: {str(error)}\")\n",
    "        if context:\n",
    "            logger.error(f\"Context: {json.dumps(context, indent=2)}\")\n",
    "        \n",
    "        self._save_research_data()\n",
    "\n",
    "    def _save_research_data(self):\n",
    "        \"\"\"Save the complete research data to JSON file\"\"\"\n",
    "        with open(self.json_log_file, 'w') as f:\n",
    "            json.dump(self.research_data, f, indent=2)\n",
    "\n",
    "    def get_research_summary(self) -> Dict:\n",
    "        \"\"\"Generate a summary of the research process\"\"\"\n",
    "        return {\n",
    "            \"research_id\": self.research_id,\n",
    "            \"total_steps\": len(self.research_data[\"steps\"]),\n",
    "            \"errors\": len(self.research_data.get(\"errors\", [])),\n",
    "            \"agents_involved\": list(set(step[\"agent\"] for step in self.research_data[\"steps\"])),\n",
    "            \"duration\": (datetime.now() - datetime.fromisoformat(self.research_data[\"timestamp\"])).total_seconds()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimiter:\n",
    "    def __init__(self, calls_per_minute: int):\n",
    "        self.calls_per_minute = calls_per_minute\n",
    "        self.calls = []\n",
    "        \n",
    "    async def wait_if_needed(self):\n",
    "        now = datetime.now()\n",
    "        self.calls = [call for call in self.calls \n",
    "                     if (now - call).total_seconds() < 60]\n",
    "        \n",
    "        if len(self.calls) >= self.calls_per_minute:\n",
    "            sleep_time = 60 - (now - self.calls[0]).total_seconds()\n",
    "            await asyncio.sleep(sleep_time)\n",
    "            \n",
    "        self.calls.append(now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.gemini_client = genai.Client(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "        self.rate_limiter = RateLimiter(calls_per_minute=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper:\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.subscription_key = os.getenv('BING_API_KEY')\n",
    "        self.endpoint = 'https://api.bing.microsoft.com/v7.0/search'\n",
    "        self.playwright = None\n",
    "        self.browser = None\n",
    "\n",
    "    async def initialize(self):\n",
    "        self.playwright = await async_playwright().start()\n",
    "        self.browser = await self.playwright.chromium.launch(\n",
    "            headless=True,\n",
    "            args=[\n",
    "            '--disable-blink-features=AutomationControlled',\n",
    "            '--no-sandbox',\n",
    "            '--disable-setuid-sandbox',\n",
    "            '--disable-dev-shm-usage',\n",
    "            '--disable-accelerated-2d-canvas',\n",
    "            '--disable-gpu'\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    async def cleanup(self):\n",
    "        \"\"\"Cleanup Playwright resources\"\"\"\n",
    "        if self.browser:\n",
    "            await self.browser.close()\n",
    "        if self.playwright:\n",
    "            await self.playwright.stop()\n",
    "\n",
    "    async def get_website_url(self, company_name: str) -> str:\n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"search_company_website\",\n",
    "                input_data={\"company_name\": company_name},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            headers = {'Ocp-Apim-Subscription-Key': self.subscription_key}\n",
    "            params = {\n",
    "                'q': f\"{company_name} official website\",\n",
    "                'count': 1\n",
    "            }\n",
    "            \n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(self.endpoint, headers=headers, params=params) as response:\n",
    "                    data = await response.json()\n",
    "                    website_url = data['webPages']['value'][0]['url']\n",
    "                    \n",
    "                    self.logger.log_step(\n",
    "                        agent_name=\"WebScraper\",\n",
    "                        action=\"found_website_url\",\n",
    "                        input_data={\"company_name\": company_name},\n",
    "                        output_data={\"website_url\": website_url}\n",
    "                    )\n",
    "                    \n",
    "                    return website_url\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"get_website_url\",\n",
    "                error=e,\n",
    "                context={\"company_name\": company_name}\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    async def _scroll_page(self, page):\n",
    "        \"\"\"Helper method to scroll the page and ensure content is loaded\"\"\"\n",
    "        try:\n",
    "            await page.evaluate(\"\"\"\n",
    "                async () => {\n",
    "                    await new Promise((resolve) => {\n",
    "                        let totalHeight = 0;\n",
    "                        const distance = 100;\n",
    "                        const timer = setInterval(() => {\n",
    "                            const scrollHeight = document.body.scrollHeight;\n",
    "                            window.scrollBy(0, distance);\n",
    "                            totalHeight += distance;\n",
    "                            \n",
    "                            if(totalHeight >= scrollHeight){\n",
    "                                clearInterval(timer);\n",
    "                                resolve();\n",
    "                            }\n",
    "                        }, 100);\n",
    "                    });\n",
    "                }\n",
    "            \"\"\")\n",
    "        except Exception:\n",
    "            # If scrolling fails, continue anyway\n",
    "            pass\n",
    "\n",
    "    async def _handle_page_error(self, error, url):\n",
    "        \"\"\"Handle page errors, ignoring LocalStorageUtil errors\"\"\"\n",
    "        error_str = str(error)\n",
    "\n",
    "        ignorable_errors = [\n",
    "            'LocalStorageUtil',\n",
    "            'already been declared',\n",
    "            'Minified React error',\n",
    "            'visit https://react.dev/errors'\n",
    "        ]\n",
    "\n",
    "        if not any(ignore_err in error_str for ignore_err in ignorable_errors):\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"page_javascript_error\",\n",
    "                error=error_str,\n",
    "                context={\"url\": url}\n",
    "            )\n",
    "\n",
    "    async def extract_page_content(self, url: str) -> str:\n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"start_page_extraction\",\n",
    "                input_data={\"url\": url},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            if not self.browser:\n",
    "                await self.initialize()\n",
    "\n",
    "            context = await self.browser.new_context(\n",
    "                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "                viewport={'width': 1920, 'height': 1080},\n",
    "                java_script_enabled=True\n",
    "            )\n",
    "            \n",
    "            page = await context.new_page()\n",
    "\n",
    "            try:\n",
    "                # Configure error handling\n",
    "                page.on(\"pageerror\", lambda err: self._handle_page_error(err, url))\n",
    "\n",
    "                # Handle client-side errors\n",
    "                await page.route(\"**/*\", lambda route: self._handle_route(route))\n",
    "\n",
    "                response = await page.goto(\n",
    "                    url,\n",
    "                    wait_until='domcontentloaded',\n",
    "                    timeout=15000\n",
    "                )\n",
    "\n",
    "                if response is None or not response.ok:\n",
    "                    raise Exception(f\"Failed to load page: {url}\")\n",
    "\n",
    "                # Check for error messages\n",
    "                error_selectors = [\n",
    "                    \"text='Application error'\",\n",
    "                    \"text='Client-side exception'\",\n",
    "                    \"text='Error'\",\n",
    "                    \".error-message\",\n",
    "                    \"#error-message\"\n",
    "                ]\n",
    "\n",
    "                has_error = False\n",
    "                for selector in error_selectors:\n",
    "                    try:\n",
    "                        error_element = await page.wait_for_selector(selector, timeout=1000)\n",
    "                        if error_element:\n",
    "                            has_error = True\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                if has_error:\n",
    "                    # Try reloading with different settings\n",
    "                    await page.reload(\n",
    "                        wait_until='networkidle',\n",
    "                        timeout=20000\n",
    "                    )\n",
    "                    await page.wait_for_timeout(2000)\n",
    "\n",
    "                # Wait for critical content\n",
    "                try:\n",
    "                    await page.wait_for_selector('main, #content, .content, article, body', \n",
    "                                            timeout=5000,\n",
    "                                            state='visible')\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                # Ensure JavaScript execution is complete\n",
    "                await page.wait_for_load_state('domcontentloaded')\n",
    "                await page.wait_for_timeout(2000)  # Additional wait for dynamic content\n",
    "\n",
    "                # Get the page content\n",
    "                content = await page.content()\n",
    "                \n",
    "                # Close the context\n",
    "                await context.close()\n",
    "\n",
    "                # Process content\n",
    "                soup = BeautifulSoup(content, 'html.parser')\n",
    "                \n",
    "                # Remove error messages and unnecessary elements\n",
    "                error_classes = ['error-message', 'error-container', 'error-page']\n",
    "                for error_class in error_classes:\n",
    "                    for element in soup.find_all(class_=error_class):\n",
    "                        element.decompose()\n",
    "\n",
    "                for script in soup([\"script\", \"style\", \"noscript\"]):\n",
    "                    script.decompose()\n",
    "\n",
    "                text = soup.get_text()\n",
    "                lines = (line.strip() for line in text.splitlines())\n",
    "                chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "                text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "                # If the content is too short or contains error messages, try fallback method\n",
    "                if len(text.strip()) < 100 or \"Application error\" in text:\n",
    "                    text = await self._fallback_content_extraction(url)\n",
    "\n",
    "                self.logger.log_step(\n",
    "                    agent_name=\"WebScraper\",\n",
    "                    action=\"complete_page_extraction\",\n",
    "                    input_data={\"url\": url},\n",
    "                    output_data={\n",
    "                        \"content_length\": len(text),\n",
    "                        \"content_preview\": text[:50]\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                return text\n",
    "\n",
    "            except Exception as page_error:\n",
    "                self.logger.log_error(\n",
    "                    agent_name=\"WebScraper\",\n",
    "                    action=\"page_load_error\",\n",
    "                    error=page_error,\n",
    "                    context={\"url\": url}\n",
    "                )\n",
    "                return await self._fallback_content_extraction(url)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"extract_page_content\",\n",
    "                error=e,\n",
    "                context={\"url\": url}\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    async def _handle_route(self, route):\n",
    "        \"\"\"Handle route requests and errors\"\"\"\n",
    "        try:\n",
    "            if route.request.resource_type in ['image', 'stylesheet', 'font']:\n",
    "                await route.abort()\n",
    "            else:\n",
    "                await route.continue_()\n",
    "        except:\n",
    "            await route.continue_()\n",
    "\n",
    "    async def _fallback_content_extraction(self, url: str) -> str:\n",
    "        \"\"\"Fallback method for content extraction\"\"\"\n",
    "        try:\n",
    "            context = await self.browser.new_context(\n",
    "                java_script_enabled=False,  # Disable JavaScript\n",
    "                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            )\n",
    "            \n",
    "            page = await context.new_page()\n",
    "            \n",
    "            # Simple GET request without waiting for JavaScript\n",
    "            await page.goto(url, wait_until='commit', timeout=10000)\n",
    "            \n",
    "            content = await page.content()\n",
    "            await context.close()\n",
    "            \n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "                \n",
    "            text = ' '.join(soup.get_text().split())\n",
    "            \n",
    "            return text\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"fallback_content_extraction\",\n",
    "                error=e,\n",
    "                context={\"url\": url}\n",
    "            )\n",
    "            return \"\"\n",
    "\n",
    "    async def _is_error_page(self, page) -> bool:\n",
    "        \"\"\"Check if the current page is showing an error\"\"\"\n",
    "        error_indicators = [\n",
    "            \"Application error\",\n",
    "            \"Client-side exception\",\n",
    "            \"Something went wrong\",\n",
    "            \"404 Not Found\",\n",
    "            \"500 Internal Server Error\"\n",
    "        ]\n",
    "        \n",
    "        page_text = await page.text_content('body')\n",
    "        return any(indicator in page_text for indicator in error_indicators)\n",
    "    \n",
    "    async def process_pages_concurrently(self, urls: List[str], max_concurrent: int = 5) -> Dict[str, str]:\n",
    "        \"\"\"Process multiple pages concurrently with rate limiting\"\"\"\n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"start_concurrent_extraction\",\n",
    "                input_data={\"urls\": urls, \"max_concurrent\": max_concurrent},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            if not self.browser:\n",
    "                await self.initialize()\n",
    "\n",
    "            async def process_single_url(url: str) -> Tuple[str, str]:\n",
    "                try:\n",
    "                    content = await self.extract_page_content(url)\n",
    "                    return url, content\n",
    "                except Exception as e:\n",
    "                    self.logger.log_error(\n",
    "                        agent_name=\"WebScraper\",\n",
    "                        action=\"process_single_url\",\n",
    "                        error=e,\n",
    "                        context={\"url\": url}\n",
    "                    )\n",
    "                    return url, \"\"\n",
    "\n",
    "            # Process URLs in chunks to limit concurrent operations\n",
    "            results = {}\n",
    "            for i in range(0, len(urls), max_concurrent):\n",
    "                chunk = urls[i:i + max_concurrent]\n",
    "                chunk_tasks = [process_single_url(url) for url in chunk]\n",
    "                chunk_results = await asyncio.gather(*chunk_tasks, return_exceptions=True)\n",
    "                \n",
    "                for url, content in chunk_results:\n",
    "                    if content:  # Only store successful results\n",
    "                        results[url] = content\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"process_pages_concurrently\",\n",
    "                error=e,\n",
    "                context={\"urls\": urls}\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    async def search_company_page(self, company_name: str, page_type: str) -> str:\n",
    "        \"\"\"Search for a specific type of page for a company\"\"\"\n",
    "        try:\n",
    "            search_terms = {\n",
    "                'pricing': ['pricing', 'plans', 'packages'],\n",
    "                'features': ['features', 'product features'],\n",
    "                'products': ['products', 'solutions'],\n",
    "                'about': ['about', 'company information'],\n",
    "            }\n",
    "            \n",
    "            search_term = search_terms.get(page_type, [page_type])[0]\n",
    "            query = f\"{company_name} {search_term}\"\n",
    "            \n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"search_company_page\",\n",
    "                input_data={\"company_name\": company_name, \"page_type\": page_type},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            headers = {'Ocp-Apim-Subscription-Key': self.subscription_key}\n",
    "            params = {\n",
    "                'q': query,\n",
    "                'count': 1\n",
    "            }\n",
    "            \n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(self.endpoint, headers=headers, params=params) as response:\n",
    "                    data = await response.json()\n",
    "                    if 'webPages' in data and data['webPages']['value']:\n",
    "                        page_url = data['webPages']['value'][0]['url']\n",
    "                        \n",
    "                        self.logger.log_step(\n",
    "                            agent_name=\"WebScraper\",\n",
    "                            action=\"found_page_url\",\n",
    "                            input_data={\"query\": query},\n",
    "                            output_data={\"page_url\": page_url}\n",
    "                        )\n",
    "                        \n",
    "                        return page_url\n",
    "                    return None\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"search_company_page\",\n",
    "                error=e,\n",
    "                context={\"company_name\": company_name, \"page_type\": page_type}\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    async def analyze_company(self, company_name: str) -> Dict:\n",
    "      try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"start_company_analysis\",\n",
    "                input_data={\"company_name\": company_name},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            website_url = await self.get_website_url(company_name)\n",
    "            page_types = ['pricing', 'features', 'products', 'about']\n",
    "\n",
    "            urls_to_process = [website_url]\n",
    "            page_urls = {}\n",
    "\n",
    "            for page_type in page_types:\n",
    "                try:\n",
    "                    page_url = await self.search_company_page(company_name, page_type)\n",
    "                    if page_url:\n",
    "                        urls_to_process.append(page_url)\n",
    "                        page_urls[page_type] = page_url\n",
    "                except Exception as e:\n",
    "                    self.logger.log_error(\n",
    "                        agent_name=\"WebScraper\",\n",
    "                        action=\"get_page_url\",\n",
    "                        error=e,\n",
    "                        context={\"company_name\": company_name, \"page_type\": page_type}\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "            all_content = await self.process_pages_concurrently(urls_to_process)\n",
    "\n",
    "            company_data = {\n",
    "                \"name\": company_name,\n",
    "                \"website\": website_url,\n",
    "                \"pages\": {\n",
    "                    \"home\": {\n",
    "                        \"url\": website_url,\n",
    "                        \"content\": all_content.get(website_url, \"\")\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            for page_type, url in page_urls.items():\n",
    "                company_data[\"pages\"][page_type] = {\n",
    "                    \"url\": url,\n",
    "                    \"content\": all_content.get(url, \"\")\n",
    "                }\n",
    "\n",
    "            return company_data\n",
    "\n",
    "      except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"analyze_company\",\n",
    "                error=e,\n",
    "                context={\"company_name\": company_name}\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    async def analyze_competitors(self, competitors_data: Dict) -> Dict:\n",
    "        print(type(competitors_data))\n",
    "        if isinstance(competitors_data, str):\n",
    "            competitors_data = json.loads(competitors_data)\n",
    "            \n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"start_competitors_analysis\",\n",
    "                input_data={\"competitors\": [comp[\"name\"] for comp in competitors_data[\"competitors\"]]},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            results = {}\n",
    "            for competitor in competitors_data[\"competitors\"]:\n",
    "                try:\n",
    "                    results[competitor[\"name\"]] = await self.analyze_company(competitor[\"name\"])\n",
    "                except Exception as e:\n",
    "                    self.logger.log_error(\n",
    "                        agent_name=\"WebScraper\",\n",
    "                        action=\"analyze_competitor\",\n",
    "                        error=e,\n",
    "                        context={\"competitor\": competitor}\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"complete_competitors_analysis\",\n",
    "                input_data={\"competitors\": [comp[\"name\"] for comp in competitors_data[\"competitors\"]]},\n",
    "                output_data={\n",
    "                    \"competitors_analyzed\": list(results.keys()),\n",
    "                    \"total_competitors\": len(competitors_data[\"competitors\"]),\n",
    "                    \"successful_analyses\": len(results)\n",
    "                }\n",
    "            )\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"analyze_competitors\",\n",
    "                error=e,\n",
    "                context={\"competitors_data\": competitors_data}\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    async def extract_structured_data(self, content: str, data_type: str) -> Dict:\n",
    "        \"\"\"Extract specific types of data from page content\"\"\"\n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"start_structured_data_extraction\",\n",
    "                input_data={\"data_type\": data_type},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            # Use Gemini to extract structured data\n",
    "            prompt = f\"\"\"\n",
    "            Extract the following type of information: {data_type}\n",
    "            From the following content:\n",
    "            # TODO: Remove content limit?\n",
    "            {content[:1000]}  # Limit content length for API\n",
    "            \n",
    "            Return the information in JSON format.\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.config.gemini_client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash-exp\",\n",
    "                contents=prompt,\n",
    "                config=genai.types.GenerateContentConfig(\n",
    "                  temperature= 0.1,\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            structured_data = json.loads(response.text)\n",
    "            \n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"complete_structured_data_extraction\",\n",
    "                input_data={\"data_type\": data_type},\n",
    "                output_data=structured_data\n",
    "            )\n",
    "            \n",
    "            return structured_data\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"extract_structured_data\",\n",
    "                error=e,\n",
    "                context={\"data_type\": data_type}\n",
    "            )\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.system_prompt = \"\"\n",
    "        self.role = \"\"\n",
    "        self.goal = \"\"\n",
    "        self.backstory = \"\"\n",
    "        self.temperature = 1.0\n",
    "\n",
    "    async def execute(self, input_data: Any) -> Any:\n",
    "        await self.config.rate_limiter.wait_if_needed()\n",
    "        \n",
    "        try:\n",
    "            # Log the start of execution\n",
    "            self.logger.log_step(\n",
    "                agent_name=self.__class__.__name__,\n",
    "                action=\"start_execution\",\n",
    "                input_data=input_data,\n",
    "                output_data=None\n",
    "            )\n",
    "            \n",
    "            prompt = f\"\"\"\n",
    "            Role: {self.role}\n",
    "            Goal: {self.goal}\n",
    "            Backstory: {self.backstory}\n",
    "            System Instructions: {self.system_prompt}\n",
    "            \n",
    "            Input Data:\n",
    "            {json.dumps(input_data, indent=2)}\n",
    "            \n",
    "            Please provide your analysis based on the above information.\n",
    "            \"\"\"\n",
    "\n",
    "            response = self.config.gemini_client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash-exp\",\n",
    "                contents=prompt,\n",
    "                config=genai.types.GenerateContentConfig(\n",
    "                  temperature=self.temperature,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            self.logger.log_step(\n",
    "                agent_name=self.__class__.__name__,\n",
    "                action=\"loading_response_into_json\",\n",
    "                input_data=input_data,\n",
    "                output_data=response.text\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "              result = json.loads(response.text.replace('```json', '').replace('```', '').strip())\n",
    "            except Exception as e:\n",
    "              result = response.text\n",
    "              self.logger.log_error(\n",
    "                  agent_name=self.__class__.__name__,\n",
    "                  action=\"loading_response_into_json\",\n",
    "                  error=e,\n",
    "                  context={\"response\": result}\n",
    "            )\n",
    "\n",
    "            # Log the successful execution\n",
    "            self.logger.log_step(\n",
    "                agent_name=self.__class__.__name__,\n",
    "                action=\"complete_execution\",\n",
    "                input_data=input_data,\n",
    "                output_data=result\n",
    "            )\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Log the error\n",
    "            self.logger.log_error(\n",
    "                agent_name=self.__class__.__name__,\n",
    "                action=\"execute\",\n",
    "                error=e,\n",
    "                context={\"input_data\": input_data}\n",
    "            )\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketIntelligenceScout(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Expert market researcher specializing in competitor identification\"\n",
    "        self.goal = \"Identify and categorize the most relevant competitors\"\n",
    "        self.backstory = \"Former market research director with 15 years of experience\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        1. Search for top competitors in the given market\n",
    "        2. Return results in JSON format with the following structure:\n",
    "        {\n",
    "            \"competitors\": [\n",
    "                {\n",
    "                    \"name\": \"\",\n",
    "                    \"website\": \"\",\n",
    "                    \"industry\": \"\",\n",
    "                    \"threat_level\": \"\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitalProductAnalyst(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Product analysis specialist\"\n",
    "        self.goal = \"Analyze product features and capabilities\"\n",
    "        self.backstory = \"Previously a product manager at major tech companies\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Analyze each competitor's product features and capabilities.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"competitor_name\": {\n",
    "                \"key_features\": [],\n",
    "                \"unique_capabilities\": [],\n",
    "                \"user_experience\": \"\",\n",
    "                \"product_maturity\": \"\"\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketingMessageDecoder(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Marketing communications analyst\"\n",
    "        self.goal = \"Decode and analyze competitors' marketing strategies\"\n",
    "        self.backstory = \"Former copywriter turned marketing strategist\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Analyze marketing messages and positioning.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"competitor_name\": {\n",
    "                \"value_propositions\": [],\n",
    "                \"messaging_tone\": \"\",\n",
    "                \"target_audience\": \"\",\n",
    "                \"unique_selling_points\": []\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechnicalFeatureComparator(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Technical analyst specializing in feature comparison\"\n",
    "        self.goal = \"Provide detailed technical comparison of competitor products\"\n",
    "        self.backstory = \"Senior solutions architect with cross-industry experience\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Compare technical features across competitors.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"competitor_name\": {\n",
    "                \"tech_stack\": [],\n",
    "                \"api_capabilities\": [],\n",
    "                \"scalability_features\": [],\n",
    "                \"technical_advantages\": [],\n",
    "                \"technical_limitations\": []\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PricingStrategySpecialist(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Pricing analysis expert\"\n",
    "        self.goal = \"Analyze and compare pricing models and strategies\"\n",
    "        self.backstory = \"Former pricing consultant in SaaS industry\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Analyze pricing strategies and models.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"competitor_name\": {\n",
    "                \"pricing_tiers\": [],\n",
    "                \"pricing_model\": \"\",\n",
    "                \"discount_strategies\": [],\n",
    "                \"pricing_positioning\": \"\"\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetitiveStrategyAnalyst(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Strategic analyst specializing in competitive analysis\"\n",
    "        self.goal = \"Synthesize competitive intelligence into strategic insights\"\n",
    "        self.backstory = \"Strategy consultant from major consulting firms\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Synthesize all competitive data into strategic insights.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"market_patterns\": [],\n",
    "            \"competitive_advantages\": {},\n",
    "            \"market_gaps\": [],\n",
    "            \"strategic_recommendations\": [],\n",
    "            \"threat_assessment\": {}\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.temperature = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetitiveIntelligenceReportSpecialist(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Report creation specialist\"\n",
    "        self.goal = \"Create clear, actionable reports from competitive analysis\"\n",
    "        self.backstory = \"Communications expert from McKinsey, has a history of writing insightful reports on the target market for clients\"\n",
    "        self.temperature = 0.5\n",
    "\n",
    "    async def generate_executive_and_market(self, data: Dict) -> Dict:\n",
    "        temp_agent = BaseAgent(self.config, self.logger)\n",
    "        temp_agent.role = self.role\n",
    "        temp_agent.goal = self.goal\n",
    "        temp_agent.backstory = self.backstory\n",
    "        temp_agent.temperature = self.temperature\n",
    "        temp_agent.system_prompt = \"\"\"\n",
    "        Create the executive summary and market overview sections of the competitive analysis report.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"executive_summary\": \"\",\n",
    "            \"market_overview\": \"\"\n",
    "        }\n",
    "        \"\"\"\n",
    "        return await temp_agent.execute(data)\n",
    "\n",
    "    async def generate_analysis_and_findings(self, data: Dict) -> Dict:\n",
    "        temp_agent = BaseAgent(self.config, self.logger)\n",
    "        temp_agent.role = self.role\n",
    "        temp_agent.goal = self.goal\n",
    "        temp_agent.backstory = self.backstory\n",
    "        temp_agent.temperature = self.temperature\n",
    "        temp_agent.system_prompt = \"\"\"\n",
    "        Create the key findings and detailed analysis sections of the competitive analysis report.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"key_findings\": [],\n",
    "            \"detailed_analysis\": {}\n",
    "        }\n",
    "        \"\"\"\n",
    "        return await temp_agent.execute(data)\n",
    "\n",
    "    async def generate_recommendations_and_appendix(self, data: Dict) -> Dict:\n",
    "        temp_agent = BaseAgent(self.config, self.logger)\n",
    "        temp_agent.role = self.role\n",
    "        temp_agent.goal = self.goal\n",
    "        temp_agent.backstory = self.backstory\n",
    "        temp_agent.temperature = self.temperature\n",
    "        temp_agent.system_prompt = \"\"\"\n",
    "        Create the recommendations and appendix sections of the competitive analysis report.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"recommendations\": [],\n",
    "            \"appendix\": {}\n",
    "        }\n",
    "        \"\"\"\n",
    "        return await temp_agent.execute(data)\n",
    "\n",
    "    async def execute(self, data: Dict) -> Dict:\n",
    "        try:\n",
    "            # Generate report sections sequentially to respect rate limits\n",
    "            exec_market = await self.generate_executive_and_market(data)\n",
    "            analysis_findings = await self.generate_analysis_and_findings(data)\n",
    "            rec_appendix = await self.generate_recommendations_and_appendix(data)\n",
    "\n",
    "            # Combine all sections\n",
    "            final_report = {\n",
    "                **exec_market,\n",
    "                **analysis_findings,\n",
    "                **rec_appendix\n",
    "            }\n",
    "\n",
    "            # Validate sections\n",
    "            required_keys = {\n",
    "                \"executive_summary\", \"market_overview\", \n",
    "                \"key_findings\", \"detailed_analysis\",\n",
    "                \"recommendations\", \"appendix\"\n",
    "            }\n",
    "            \n",
    "            missing_keys = required_keys - set(final_report.keys())\n",
    "            if missing_keys:\n",
    "                raise ValueError(f\"Missing sections in report: {missing_keys}\")\n",
    "\n",
    "            return final_report\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=self.__class__.__name__,\n",
    "                action=\"execute\",\n",
    "                error=e,\n",
    "                context={\"data\": data}\n",
    "            )\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetitiveAnalysisWorkflow:\n",
    "    def __init__(self, config: Config, research_id: str):\n",
    "        self.config = config\n",
    "        self.logger = ResearchLogger(research_id)\n",
    "\n",
    "        self.market_scout = MarketIntelligenceScout(config, self.logger)\n",
    "        self.product_analyst = DigitalProductAnalyst(config, self.logger)\n",
    "        self.marketing_decoder = MarketingMessageDecoder(config, self.logger)\n",
    "        self.technical_comparator = TechnicalFeatureComparator(config, self.logger)\n",
    "        self.pricing_specialist = PricingStrategySpecialist(config, self.logger)\n",
    "        self.competitive_analyst = CompetitiveStrategyAnalyst(config, self.logger)\n",
    "        self.report_specialist = CompetitiveIntelligenceReportSpecialist(config, self.logger)\n",
    "        self.web_scraper = WebScraper(config, self.logger)\n",
    "\n",
    "    async def run_parallel_analysis(self, competitors_data: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Run parallel analysis tasks\"\"\"\n",
    "        tasks = [\n",
    "            self.product_analyst.execute(competitors_data),\n",
    "            self.marketing_decoder.execute(competitors_data),\n",
    "            self.technical_comparator.execute(competitors_data),\n",
    "            self.pricing_specialist.execute(competitors_data)\n",
    "        ]\n",
    "        \n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        return {\n",
    "            \"product_analysis\": results[0],\n",
    "            \"marketing_analysis\": results[1],\n",
    "            \"technical_analysis\": results[2],\n",
    "            \"pricing_analysis\": results[3]\n",
    "        }\n",
    "\n",
    "    async def execute_workflow(self, industry: str, target_company: str = None) -> Dict:\n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"Workflow\",\n",
    "                action=\"start_workflow\",\n",
    "                input_data={\"industry\": industry, \"target_company\": target_company},\n",
    "                output_data=None\n",
    "            )\n",
    "            \n",
    "            # Step 1: Retrieve target company information (if necessar;)\n",
    "            target_company_data = None\n",
    "            if target_company:\n",
    "                target_company_data = await self.web_scraper.analyze_company(target_company)\n",
    "\n",
    "            # Step 2: Identify competitors\n",
    "            competitors_data = await self.market_scout.execute({\n",
    "                \"industry\": industry,\n",
    "                \"target_company\": target_company,\n",
    "                \"target_company_data\": target_company_data\n",
    "            })\n",
    "\n",
    "            # Step 3: Gather website information for all competitors\n",
    "            website_data = await self.web_scraper.analyze_competitors(competitors_data)\n",
    "\n",
    "            # Step 4: Run parallel analysis with website data\n",
    "            parallel_results = await self.run_parallel_analysis({\n",
    "                \"competitors_data\": competitors_data,\n",
    "                \"website_data\": website_data,\n",
    "                \"target_company_data\": target_company_data\n",
    "            })\n",
    "\n",
    "            # Step 3: Strategic analysis\n",
    "            strategic_analysis = await self.competitive_analyst.execute({\n",
    "                \"competitors_data\": competitors_data,\n",
    "                \"parallel_results\": parallel_results\n",
    "            })\n",
    "\n",
    "            # Step 4: Generate report\n",
    "            final_report = await self.report_specialist.execute({\n",
    "                \"strategic_analysis\": strategic_analysis,\n",
    "                \"raw_data\": {\n",
    "                    \"competitors\": competitors_data,\n",
    "                    \"analysis\": parallel_results\n",
    "                }\n",
    "            })\n",
    "\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"Workflow\",\n",
    "                action=\"complete_workflow\",\n",
    "                input_data={\"industry\": industry, \"target_company\": target_company},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            return final_report\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"Workflow\",\n",
    "                action=\"execute_workflow\",\n",
    "                error=e,\n",
    "                context={\n",
    "                    \"industry\": industry,\n",
    "                    \"target_company\": target_company\n",
    "                }\n",
    "            )\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_workflow(industry, target_company=None):\n",
    "    config = Config()\n",
    "    research_id = f\"research_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    workflow = CompetitiveAnalysisWorkflow(config, research_id)\n",
    "    \n",
    "    result = await workflow.execute_workflow(\n",
    "        industry=industry,\n",
    "        target_company=target_company\n",
    "    )\n",
    "    \n",
    "    # Print research summary\n",
    "    print(\"\\nResearch Summary:\")\n",
    "    print(json.dumps(workflow.logger.get_research_summary(), indent=2))\n",
    "    \n",
    "    print(\"\\nFinal Result:\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry = \"Software Testing and Quality Assurance\"\n",
    "target_company = \"Nogrunt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Agent: Workflow | Action: start_workflow\n",
      "INFO:__main__:Agent: WebScraper | Action: start_company_analysis\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_website\n",
      "INFO:__main__:Agent: WebScraper | Action: found_website_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: start_concurrent_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: MarketIntelligenceScout | Action: start_execution\n",
      "INFO:root:AFC is enabled with max remote calls: 10.\n",
      "INFO:__main__:Agent: MarketIntelligenceScout | Action: loading_response_into_json\n",
      "INFO:__main__:Agent: MarketIntelligenceScout | Action: complete_execution\n",
      "INFO:__main__:Agent: WebScraper | Action: start_competitors_analysis\n",
      "INFO:__main__:Agent: WebScraper | Action: start_company_analysis\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_website\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Agent: WebScraper | Action: found_website_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: start_concurrent_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_company_analysis\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_website\n",
      "INFO:__main__:Agent: WebScraper | Action: found_website_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: start_concurrent_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_company_analysis\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_website\n",
      "INFO:__main__:Agent: WebScraper | Action: found_website_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: start_concurrent_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_company_analysis\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_website\n",
      "INFO:__main__:Agent: WebScraper | Action: found_website_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: start_concurrent_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_company_analysis\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_website\n",
      "INFO:__main__:Agent: WebScraper | Action: found_website_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: start_concurrent_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "ERROR:__main__:Error in WebScraper during page_javascript_error: Cannot read properties of undefined (reading 'TenantFeatures')\n",
      "ERROR:__main__:Context: {\n",
      "  \"url\": \"https://saucelabs.com/\"\n",
      "}\n",
      "ERROR:__main__:Error in WebScraper during page_javascript_error: Cannot read properties of undefined (reading 'TenantFeatures')\n",
      "ERROR:__main__:Context: {\n",
      "  \"url\": \"https://saucelabs.com/\"\n",
      "}\n",
      "ERROR:__main__:Error in WebScraper during page_javascript_error: Cannot read properties of undefined (reading 'TenantFeatures')\n",
      "ERROR:__main__:Context: {\n",
      "  \"url\": \"https://saucelabs.com/\"\n",
      "}\n",
      "ERROR:__main__:Error in WebScraper during page_javascript_error: Cannot read properties of undefined (reading 'TenantFeatures')\n",
      "ERROR:__main__:Context: {\n",
      "  \"url\": \"https://saucelabs.com/company/about-us\"\n",
      "}\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_company_analysis\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_website\n",
      "INFO:__main__:Agent: WebScraper | Action: found_website_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: start_concurrent_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "ERROR:__main__:Error in WebScraper during page_javascript_error: Mismatched anonymous define() module: function(e){return t(e,window)}\n",
      "http://requirejs.org/docs/errors.html#mismatch\n",
      "ERROR:__main__:Context: {\n",
      "  \"url\": \"https://www.geeksforgeeks.org/selenium-basics-components-features-uses-and-limitations/\"\n",
      "}\n",
      "ERROR:__main__:Error in WebScraper during page_javascript_error: Cannot read properties of null (reading '__k')\n",
      "ERROR:__main__:Context: {\n",
      "  \"url\": \"https://www.selenium.dev/\"\n",
      "}\n",
      "ERROR:__main__:Error in WebScraper during page_javascript_error: quizData is not defined\n",
      "ERROR:__main__:Context: {\n",
      "  \"url\": \"https://www.geeksforgeeks.org/selenium-basics-components-features-uses-and-limitations/\"\n",
      "}\n",
      "ERROR:__main__:Error in WebScraper during page_javascript_error: Failed to load script \"https://traveler.content.now.optum.com/traveler.umd.js\"\n",
      "ERROR:__main__:Context: {\n",
      "  \"url\": \"https://www.healthline.com/nutrition/best-selenium-supplement\"\n",
      "}\n",
      "ERROR:__main__:Error in WebScraper during page_javascript_error: Failed to read a named property 'addEventListener' from 'Window': Blocked a frame with origin \"https://e425d26c79ae86b376a46affbb7417db.safeframe.googlesyndication.com\" from accessing a cross-origin frame.\n",
      "ERROR:__main__:Context: {\n",
      "  \"url\": \"https://www.geeksforgeeks.org/selenium-basics-components-features-uses-and-limitations/\"\n",
      "}\n",
      "ERROR:__main__:Error in WebScraper during page_javascript_error: Failed to read a named property 'addEventListener' from 'Window': Blocked a frame with origin \"https://e425d26c79ae86b376a46affbb7417db.safeframe.googlesyndication.com\" from accessing a cross-origin frame.\n",
      "ERROR:__main__:Context: {\n",
      "  \"url\": \"https://www.geeksforgeeks.org/selenium-basics-components-features-uses-and-limitations/\"\n",
      "}\n",
      "ERROR:__main__:Error in WebScraper during page_javascript_error: Failed to read a named property 'addEventListener' from 'Window': Blocked a frame with origin \"https://e425d26c79ae86b376a46affbb7417db.safeframe.googlesyndication.com\" from accessing a cross-origin frame.\n",
      "ERROR:__main__:Context: {\n",
      "  \"url\": \"https://www.geeksforgeeks.org/selenium-basics-components-features-uses-and-limitations/\"\n",
      "}\n",
      "ERROR:__main__:Error in WebScraper during page_javascript_error: Failed to read a named property 'addEventListener' from 'Window': Blocked a frame with origin \"https://e425d26c79ae86b376a46affbb7417db.safeframe.googlesyndication.com\" from accessing a cross-origin frame.\n",
      "ERROR:__main__:Context: {\n",
      "  \"url\": \"https://www.geeksforgeeks.org/selenium-basics-components-features-uses-and-limitations/\"\n",
      "}\n",
      "ERROR:__main__:Error in WebScraper during page_javascript_error: Failed to read a named property 'addEventListener' from 'Window': Blocked a frame with origin \"https://e425d26c79ae86b376a46affbb7417db.safeframe.googlesyndication.com\" from accessing a cross-origin frame.\n",
      "ERROR:__main__:Context: {\n",
      "  \"url\": \"https://www.geeksforgeeks.org/selenium-basics-components-features-uses-and-limitations/\"\n",
      "}\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_competitors_analysis\n",
      "INFO:__main__:Agent: DigitalProductAnalyst | Action: start_execution\n",
      "INFO:root:AFC is enabled with max remote calls: 10.\n",
      "INFO:__main__:Agent: DigitalProductAnalyst | Action: loading_response_into_json\n",
      "INFO:__main__:Agent: DigitalProductAnalyst | Action: complete_execution\n",
      "INFO:__main__:Agent: MarketingMessageDecoder | Action: start_execution\n",
      "INFO:root:AFC is enabled with max remote calls: 10.\n"
     ]
    }
   ],
   "source": [
    "result = await test_workflow(industry, target_company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_to_markdown(data, output_file=\"output.md\"):\n",
    "    # Convert JSON to Markdown\n",
    "    def json_to_markdown(data, level=0):\n",
    "        indent = \"  \" * level\n",
    "        lines = []\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                # Use headers for levels 0-6, bold for deeper nesting\n",
    "                if level <= 6:\n",
    "                    # Create header with appropriate number of #\n",
    "                    lines.append(f\"{indent}{'#' * (level + 1)} {key.replace('_', ' ').title()}\")\n",
    "                else:\n",
    "                    lines.append(f\"{indent}**{key.replace('_', ' ').title()}:**\")\n",
    "                lines.extend(json_to_markdown(value, level + 1))\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                lines.extend(json_to_markdown(item, level + 1))\n",
    "        else:\n",
    "            lines.append(f\"{indent}- {data}\")\n",
    "        return lines\n",
    "\n",
    "    markdown_output = \"\\n\".join(json_to_markdown(data))\n",
    "\n",
    "    # Save to file\n",
    "    try:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(markdown_output)\n",
    "    except Exception as e:\n",
    "        return f\"Error: Could not save to file: {e}\"\n",
    "\n",
    "    return markdown_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "report_name = f\"comp_analysis_report_{target_company}_{industry}_{current_timestamp}.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json_to_markdown(result, report_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
