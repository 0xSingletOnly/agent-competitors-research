{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competitors research agent\n",
    "Steps for competitors research:\n",
    "1. Find list of competitors  \n",
    "2. Go to their website  \n",
    "3. Collect information including:  \n",
    "\t- standout features\n",
    "\t- product and pricing tiers\n",
    "\t- unique service proposition\n",
    "\t- marketing messages  \n",
    "4. Analyze competitors  \n",
    "\t- identify common patterns  \n",
    "\t- spot potential gaps  \n",
    "\t- compare pricing strategies  \n",
    "\t- compare messaging themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "import asyncio\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchLogger:\n",
    "    def __init__(self, research_id: str):\n",
    "        self.research_id = research_id\n",
    "        self.log_file = f\"research_logs/{research_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "        self.json_log_file = f\"research_logs/{research_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_data.json\"\n",
    "        \n",
    "        # Create logs directory if it doesn't exist\n",
    "        os.makedirs(\"research_logs\", exist_ok=True)\n",
    "        \n",
    "        # Set up file handler\n",
    "        self.file_handler = logging.FileHandler(self.log_file)\n",
    "        self.file_handler.setFormatter(\n",
    "            logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        )\n",
    "        \n",
    "        # Add file handler to root logger\n",
    "        logging.getLogger('').addHandler(self.file_handler)\n",
    "        \n",
    "        self.research_data = {\n",
    "            \"research_id\": research_id,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"steps\": []\n",
    "        }\n",
    "\n",
    "    def log_step(self, agent_name: str, action: str, input_data: Any, output_data: Any):\n",
    "        \"\"\"Log a research step with both input and output data\"\"\"\n",
    "        step_data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"agent\": agent_name,\n",
    "            \"action\": action,\n",
    "            \"input\": input_data,\n",
    "            \"output\": output_data\n",
    "        }\n",
    "        \n",
    "        self.research_data[\"steps\"].append(step_data)\n",
    "        \n",
    "        # Log to file\n",
    "        logger.info(f\"Agent: {agent_name} | Action: {action}\")\n",
    "        logger.debug(f\"Input: {json.dumps(input_data, indent=2)}\")\n",
    "        logger.debug(f\"Output: {json.dumps(output_data, indent=2)}\")\n",
    "        \n",
    "        # Save updated research data to JSON file\n",
    "        self._save_research_data()\n",
    "\n",
    "    def log_error(self, agent_name: str, action: str, error: Exception, context: Dict = None):\n",
    "        \"\"\"Log error information\"\"\"\n",
    "        error_data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"agent\": agent_name,\n",
    "            \"action\": action,\n",
    "            \"error\": str(error),\n",
    "            \"error_type\": type(error).__name__,\n",
    "            \"context\": context\n",
    "        }\n",
    "        \n",
    "        self.research_data[\"errors\"] = self.research_data.get(\"errors\", [])\n",
    "        self.research_data[\"errors\"].append(error_data)\n",
    "        \n",
    "        logger.error(f\"Error in {agent_name} during {action}: {str(error)}\")\n",
    "        if context:\n",
    "            logger.error(f\"Context: {json.dumps(context, indent=2)}\")\n",
    "        \n",
    "        self._save_research_data()\n",
    "\n",
    "    def _save_research_data(self):\n",
    "        \"\"\"Save the complete research data to JSON file\"\"\"\n",
    "        with open(self.json_log_file, 'w') as f:\n",
    "            json.dump(self.research_data, f, indent=2)\n",
    "\n",
    "    def get_research_summary(self) -> Dict:\n",
    "        \"\"\"Generate a summary of the research process\"\"\"\n",
    "        return {\n",
    "            \"research_id\": self.research_id,\n",
    "            \"total_steps\": len(self.research_data[\"steps\"]),\n",
    "            \"errors\": len(self.research_data.get(\"errors\", [])),\n",
    "            \"agents_involved\": list(set(step[\"agent\"] for step in self.research_data[\"steps\"])),\n",
    "            \"duration\": (datetime.now() - datetime.fromisoformat(self.research_data[\"timestamp\"])).total_seconds()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimiter:\n",
    "    def __init__(self, calls_per_minute: int):\n",
    "        self.calls_per_minute = calls_per_minute\n",
    "        self.calls = []\n",
    "        \n",
    "    async def wait_if_needed(self):\n",
    "        now = datetime.now()\n",
    "        self.calls = [call for call in self.calls \n",
    "                     if (now - call).total_seconds() < 60]\n",
    "        \n",
    "        if len(self.calls) >= self.calls_per_minute:\n",
    "            sleep_time = 60 - (now - self.calls[0]).total_seconds()\n",
    "            await asyncio.sleep(sleep_time)\n",
    "            \n",
    "        self.calls.append(now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.gemini_client = genai.Client(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "        self.rate_limiter = RateLimiter(calls_per_minute=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper:\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.subscription_key = os.getenv('BING_API_KEY')\n",
    "        self.endpoint = 'https://api.bing.microsoft.com/v7.0/search'\n",
    "\n",
    "    async def get_website_url(self, company_name: str) -> str:\n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"search_company_website\",\n",
    "                input_data={\"company_name\": company_name},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            headers = {'Ocp-Apim-Subscription-Key': self.subscription_key}\n",
    "            params = {\n",
    "                'q': f\"{company_name} official website\",\n",
    "                'count': 1\n",
    "            }\n",
    "            \n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(self.endpoint, headers=headers, params=params) as response:\n",
    "                    data = await response.json()\n",
    "                    website_url = data['webPages']['value'][0]['url']\n",
    "                    \n",
    "                    self.logger.log_step(\n",
    "                        agent_name=\"WebScraper\",\n",
    "                        action=\"found_website_url\",\n",
    "                        input_data={\"company_name\": company_name},\n",
    "                        output_data={\"website_url\": website_url}\n",
    "                    )\n",
    "                    \n",
    "                    return website_url\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"get_website_url\",\n",
    "                error=e,\n",
    "                context={\"company_name\": company_name}\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    async def extract_page_content(self, url: str) -> str:\n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"start_page_extraction\",\n",
    "                input_data={\"url\": url},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(url) as response:\n",
    "                    content = await response.text()\n",
    "                    \n",
    "                    # Use BeautifulSoup to parse and clean the content\n",
    "                    soup = BeautifulSoup(content, 'html.parser')\n",
    "                    \n",
    "                    # Remove script and style elements\n",
    "                    for script in soup([\"script\", \"style\"]):\n",
    "                        script.decompose()\n",
    "                    \n",
    "                    # Get text and clean it\n",
    "                    text = soup.get_text()\n",
    "                    lines = (line.strip() for line in text.splitlines())\n",
    "                    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "                    text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "                    \n",
    "                    self.logger.log_step(\n",
    "                        agent_name=\"WebScraper\",\n",
    "                        action=\"complete_page_extraction\",\n",
    "                        input_data={\"url\": url},\n",
    "                        output_data={\"content_length\": len(text)}\n",
    "                    )\n",
    "                    \n",
    "                    return text\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"extract_page_content\",\n",
    "                error=e,\n",
    "                context={\"url\": url}\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    async def analyze_company(self, company_name: str) -> Dict:\n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"start_company_analysis\",\n",
    "                input_data={\"company_name\": company_name},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            website_url = await self.get_website_url(company_name)\n",
    "            \n",
    "            # Define common page paths to check\n",
    "            pages_to_analyze = [\n",
    "                {\"path\": \"\", \"type\": \"home\"},\n",
    "                {\"path\": \"pricing\", \"type\": \"pricing\"},\n",
    "                {\"path\": \"products\", \"type\": \"products\"},\n",
    "                {\"path\": \"about\", \"type\": \"about\"},\n",
    "                {\"path\": \"features\", \"type\": \"features\"},\n",
    "                {\"path\": \"solutions\", \"type\": \"solutions\"}\n",
    "            ]\n",
    "            \n",
    "            company_data = {\n",
    "                \"name\": company_name,\n",
    "                \"website\": website_url,\n",
    "                \"pages\": {}\n",
    "            }\n",
    "\n",
    "            for page in pages_to_analyze:\n",
    "                try:\n",
    "                    page_url = f\"{website_url.rstrip('/')}/{page['path']}\"\n",
    "                    \n",
    "                    self.logger.log_step(\n",
    "                        agent_name=\"WebScraper\",\n",
    "                        action=\"analyze_page\",\n",
    "                        input_data={\n",
    "                            \"company_name\": company_name,\n",
    "                            \"page_type\": page[\"type\"],\n",
    "                            \"url\": page_url\n",
    "                        },\n",
    "                        output_data=None\n",
    "                    )\n",
    "                    \n",
    "                    content = await self.extract_page_content(page_url)\n",
    "                    company_data[\"pages\"][page[\"type\"]] = {\n",
    "                        \"url\": page_url,\n",
    "                        \"content\": content\n",
    "                    }\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.log_error(\n",
    "                        agent_name=\"WebScraper\",\n",
    "                        action=\"analyze_page\",\n",
    "                        error=e,\n",
    "                        context={\n",
    "                            \"company_name\": company_name,\n",
    "                            \"page_type\": page[\"type\"],\n",
    "                            \"url\": page_url\n",
    "                        }\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"complete_company_analysis\",\n",
    "                input_data={\"company_name\": company_name},\n",
    "                output_data={\n",
    "                    \"website\": website_url,\n",
    "                    \"pages_analyzed\": list(company_data[\"pages\"].keys())\n",
    "                }\n",
    "            )\n",
    "\n",
    "            return company_data\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"analyze_company\",\n",
    "                error=e,\n",
    "                context={\"company_name\": company_name}\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    async def analyze_competitors(self, competitors_data: Dict) -> Dict:\n",
    "        print(type(competitors_data))\n",
    "        if isinstance(competitors_data, str):\n",
    "            competitors_data = json.loads(competitors_data)\n",
    "            \n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"start_competitors_analysis\",\n",
    "                input_data={\"competitors\": [comp[\"name\"] for comp in competitors_data[\"competitors\"]]},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            results = {}\n",
    "            for competitor in competitors_data[\"competitors\"]:\n",
    "                try:\n",
    "                    results[competitor[\"name\"]] = await self.analyze_company(competitor[\"name\"])\n",
    "                except Exception as e:\n",
    "                    self.logger.log_error(\n",
    "                        agent_name=\"WebScraper\",\n",
    "                        action=\"analyze_competitor\",\n",
    "                        error=e,\n",
    "                        context={\"competitor\": competitor}\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"complete_competitors_analysis\",\n",
    "                input_data={\"competitors\": [comp[\"name\"] for comp in competitors_data[\"competitors\"]]},\n",
    "                output_data={\n",
    "                    \"competitors_analyzed\": list(results.keys()),\n",
    "                    \"total_competitors\": len(competitors_data[\"competitors\"]),\n",
    "                    \"successful_analyses\": len(results)\n",
    "                }\n",
    "            )\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"analyze_competitors\",\n",
    "                error=e,\n",
    "                context={\"competitors_data\": competitors_data}\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    async def extract_structured_data(self, content: str, data_type: str) -> Dict:\n",
    "        \"\"\"Extract specific types of data from page content\"\"\"\n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"start_structured_data_extraction\",\n",
    "                input_data={\"data_type\": data_type},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            # Use Gemini to extract structured data\n",
    "            prompt = f\"\"\"\n",
    "            Extract the following type of information: {data_type}\n",
    "            From the following content:\n",
    "            {content[:1000]}  # Limit content length for API\n",
    "            \n",
    "            Return the information in JSON format.\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.config.gemini_client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash-exp\",\n",
    "                contents=prompt\n",
    "            )\n",
    "            \n",
    "            structured_data = json.loads(response.text)\n",
    "            \n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"complete_structured_data_extraction\",\n",
    "                input_data={\"data_type\": data_type},\n",
    "                output_data=structured_data\n",
    "            )\n",
    "            \n",
    "            return structured_data\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"extract_structured_data\",\n",
    "                error=e,\n",
    "                context={\"data_type\": data_type}\n",
    "            )\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.system_prompt = \"\"\n",
    "        self.role = \"\"\n",
    "        self.goal = \"\"\n",
    "        self.backstory = \"\"\n",
    "\n",
    "    async def execute(self, input_data: Any) -> Any:\n",
    "        await self.config.rate_limiter.wait_if_needed()\n",
    "        \n",
    "        try:\n",
    "            # Log the start of execution\n",
    "            self.logger.log_step(\n",
    "                agent_name=self.__class__.__name__,\n",
    "                action=\"start_execution\",\n",
    "                input_data=input_data,\n",
    "                output_data=None\n",
    "            )\n",
    "            \n",
    "            prompt = f\"\"\"\n",
    "            Role: {self.role}\n",
    "            Goal: {self.goal}\n",
    "            Backstory: {self.backstory}\n",
    "            System Instructions: {self.system_prompt}\n",
    "            \n",
    "            Input Data:\n",
    "            {json.dumps(input_data, indent=2)}\n",
    "            \n",
    "            Please provide your analysis based on the above information.\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.config.gemini_client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash-exp\",\n",
    "                contents=prompt\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.text.replace('```json', '').replace('```', '').strip())\n",
    "            \n",
    "            # Log the successful execution\n",
    "            self.logger.log_step(\n",
    "                agent_name=self.__class__.__name__,\n",
    "                action=\"complete_execution\",\n",
    "                input_data=input_data,\n",
    "                output_data=result\n",
    "            )\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Log the error\n",
    "            self.logger.log_error(\n",
    "                agent_name=self.__class__.__name__,\n",
    "                action=\"execute\",\n",
    "                error=e,\n",
    "                context={\"input_data\": input_data}\n",
    "            )\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketIntelligenceScout(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Expert market researcher specializing in competitor identification\"\n",
    "        self.goal = \"Identify and categorize the most relevant competitors\"\n",
    "        self.backstory = \"Former market research director with 15 years of experience\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        1. Search for top competitors in the given market\n",
    "        2. Return results in JSON format with the following structure:\n",
    "        {\n",
    "            \"competitors\": [\n",
    "                {\n",
    "                    \"name\": \"\",\n",
    "                    \"website\": \"\",\n",
    "                    \"industry\": \"\",\n",
    "                    \"threat_level\": \"\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitalProductAnalyst(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Product analysis specialist\"\n",
    "        self.goal = \"Analyze product features and capabilities\"\n",
    "        self.backstory = \"Previously a product manager at major tech companies\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Analyze each competitor's product features and capabilities.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"competitor_name\": {\n",
    "                \"key_features\": [],\n",
    "                \"unique_capabilities\": [],\n",
    "                \"user_experience\": \"\",\n",
    "                \"product_maturity\": \"\"\n",
    "            }\n",
    "        }\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketingMessageDecoder(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Marketing communications analyst\"\n",
    "        self.goal = \"Decode and analyze competitors' marketing strategies\"\n",
    "        self.backstory = \"Former copywriter turned marketing strategist\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Analyze marketing messages and positioning.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"competitor_name\": {\n",
    "                \"value_propositions\": [],\n",
    "                \"messaging_tone\": \"\",\n",
    "                \"target_audience\": \"\",\n",
    "                \"unique_selling_points\": []\n",
    "            }\n",
    "        }\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechnicalFeatureComparator(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Technical analyst specializing in feature comparison\"\n",
    "        self.goal = \"Provide detailed technical comparison of competitor products\"\n",
    "        self.backstory = \"Senior solutions architect with cross-industry experience\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Compare technical features across competitors.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"competitor_name\": {\n",
    "                \"tech_stack\": [],\n",
    "                \"api_capabilities\": [],\n",
    "                \"scalability_features\": [],\n",
    "                \"technical_advantages\": [],\n",
    "                \"technical_limitations\": []\n",
    "            }\n",
    "        }\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PricingStrategySpecialist(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Pricing analysis expert\"\n",
    "        self.goal = \"Analyze and compare pricing models and strategies\"\n",
    "        self.backstory = \"Former pricing consultant in SaaS industry\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Analyze pricing strategies and models.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"competitor_name\": {\n",
    "                \"pricing_tiers\": [],\n",
    "                \"pricing_model\": \"\",\n",
    "                \"discount_strategies\": [],\n",
    "                \"pricing_positioning\": \"\"\n",
    "            }\n",
    "        }\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetitiveStrategyAnalyst(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Strategic analyst specializing in competitive analysis\"\n",
    "        self.goal = \"Synthesize competitive intelligence into strategic insights\"\n",
    "        self.backstory = \"Strategy consultant from major consulting firms\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Synthesize all competitive data into strategic insights.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"market_patterns\": [],\n",
    "            \"competitive_advantages\": {},\n",
    "            \"market_gaps\": [],\n",
    "            \"strategic_recommendations\": [],\n",
    "            \"threat_assessment\": {}\n",
    "        }\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetitiveIntelligenceReportSpecialist(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Report creation specialist\"\n",
    "        self.goal = \"Create clear, actionable reports from competitive analysis\"\n",
    "        self.backstory = \"Communications expert in data visualization\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Create a comprehensive report from all analyses.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"executive_summary\": \"\",\n",
    "            \"key_findings\": [],\n",
    "            \"detailed_analysis\": {},\n",
    "            \"recommendations\": [],\n",
    "            \"market_overview\": \"\",\n",
    "            \"appendix\": {}\n",
    "        }\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetitiveAnalysisWorkflow:\n",
    "    def __init__(self, config: Config, research_id: str):\n",
    "        self.config = config\n",
    "        self.logger = ResearchLogger(research_id)\n",
    "\n",
    "        self.market_scout = MarketIntelligenceScout(config, self.logger)\n",
    "        self.product_analyst = DigitalProductAnalyst(config, self.logger)\n",
    "        self.marketing_decoder = MarketingMessageDecoder(config, self.logger)\n",
    "        self.technical_comparator = TechnicalFeatureComparator(config, self.logger)\n",
    "        self.pricing_specialist = PricingStrategySpecialist(config, self.logger)\n",
    "        self.competitive_analyst = CompetitiveStrategyAnalyst(config, self.logger)\n",
    "        self.report_specialist = CompetitiveIntelligenceReportSpecialist(config, self.logger)\n",
    "        self.web_scraper = WebScraper(config, self.logger)\n",
    "\n",
    "    async def run_parallel_analysis(self, competitors_data: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Run parallel analysis tasks\"\"\"\n",
    "        tasks = [\n",
    "            self.product_analyst.execute(competitors_data),\n",
    "            self.marketing_decoder.execute(competitors_data),\n",
    "            self.technical_comparator.execute(competitors_data),\n",
    "            self.pricing_specialist.execute(competitors_data)\n",
    "        ]\n",
    "        \n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        return {\n",
    "            \"product_analysis\": results[0],\n",
    "            \"marketing_analysis\": results[1],\n",
    "            \"technical_analysis\": results[2],\n",
    "            \"pricing_analysis\": results[3]\n",
    "        }\n",
    "\n",
    "    async def execute_workflow(self, industry: str, target_company: str = None) -> Dict:\n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"Workflow\",\n",
    "                action=\"start_workflow\",\n",
    "                input_data={\"industry\": industry, \"target_company\": target_company},\n",
    "                output_data=None\n",
    "            )\n",
    "            \n",
    "            # Step 1: Retrieve target company information (if necessar;)\n",
    "            target_company_data = None\n",
    "            if target_company:\n",
    "                target_company_data = await self.web_scraper.analyze_company(target_company)\n",
    "\n",
    "            # Step 2: Identify competitors\n",
    "            competitors_data = await self.market_scout.execute({\n",
    "                \"industry\": industry,\n",
    "                \"target_company\": target_company,\n",
    "                \"target_company_data\": target_company_data\n",
    "            })\n",
    "\n",
    "            # Step 3: Gather website information for all competitors\n",
    "            website_data = await self.web_scraper.analyze_competitors(competitors_data)\n",
    "\n",
    "            # Step 4: Run parallel analysis with website data\n",
    "            parallel_results = await self.run_parallel_analysis({\n",
    "                \"competitors_data\": competitors_data,\n",
    "                \"website_data\": website_data,\n",
    "                \"target_company_data\": target_company_data\n",
    "            })\n",
    "\n",
    "            # Step 3: Strategic analysis\n",
    "            strategic_analysis = await self.competitive_analyst.execute({\n",
    "                \"competitors_data\": competitors_data,\n",
    "                \"parallel_results\": parallel_results\n",
    "            })\n",
    "\n",
    "            # Step 4: Generate report\n",
    "            final_report = await self.report_specialist.execute({\n",
    "                \"strategic_analysis\": strategic_analysis,\n",
    "                \"raw_data\": {\n",
    "                    \"competitors\": competitors_data,\n",
    "                    \"analysis\": parallel_results\n",
    "                }\n",
    "            })\n",
    "\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"Workflow\",\n",
    "                action=\"complete_workflow\",\n",
    "                input_data={\"industry\": industry, \"target_company\": target_company},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            return final_report\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"Workflow\",\n",
    "                action=\"execute_workflow\",\n",
    "                error=e,\n",
    "                context={\n",
    "                    \"industry\": industry,\n",
    "                    \"target_company\": target_company\n",
    "                }\n",
    "            )\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_workflow():\n",
    "    config = Config()\n",
    "    research_id = f\"research_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    workflow = CompetitiveAnalysisWorkflow(config, research_id)\n",
    "    \n",
    "    industry = \"AI development platforms\"\n",
    "    target_company = \"OpenAI\"  # Optional, can be None\n",
    "    \n",
    "    result = await workflow.execute_workflow(\n",
    "        industry=industry,\n",
    "        target_company=target_company\n",
    "    )\n",
    "    \n",
    "    # Print research summary\n",
    "    print(\"\\nResearch Summary:\")\n",
    "    print(json.dumps(workflow.logger.get_research_summary(), indent=2))\n",
    "    \n",
    "    print(\"\\nFinal Result:\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Agent: Workflow | Action: start_workflow\n",
      "INFO:__main__:Agent: WebScraper | Action: start_company_analysis\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_website\n",
      "INFO:__main__:Agent: WebScraper | Action: found_website_url\n",
      "INFO:__main__:Agent: WebScraper | Action: analyze_page\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: analyze_page\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: analyze_page\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: analyze_page\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: analyze_page\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: analyze_page\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_company_analysis\n",
      "INFO:__main__:Agent: MarketIntelligenceScout | Action: start_execution\n",
      "INFO:root:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/var/folders/mk/spx5m7bs31zcl288wyr4sdfm0000gn/T/ipykernel_33881/3431605583.py\u001b[0m(39)\u001b[0;36mexecute\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     37 \u001b[0;31m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     38 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 39 \u001b[0;31m            \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     40 \u001b[0;31m            \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     41 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "*** AttributeError: 'GenerateContentResponse' object has no attribute 'txt'. Did you mean: 'text'?\n",
      "'```json\\n{\\n    \"competitors\": [\\n        {\\n            \"name\": \"Google AI\",\\n            \"website\": \"https://ai.google/\",\\n            \"industry\": \"AI development platforms\",\\n             \"threat_level\": \"High\"\\n        },\\n        {\\n            \"name\": \"Microsoft Azure AI\",\\n            \"website\": \"https://azure.microsoft.com/en-us/products/ai/\",\\n            \"industry\": \"AI development platforms\",\\n             \"threat_level\": \"High\"\\n        },\\n        {\\n            \"name\": \"Amazon SageMaker\",\\n             \"website\": \"https://aws.amazon.com/sagemaker/\",\\n            \"industry\": \"AI development platforms\",\\n             \"threat_level\": \"High\"\\n        },\\n         {\\n            \"name\": \"Hugging Face\",\\n            \"website\": \"https://huggingface.co/\",\\n            \"industry\": \"AI development platforms\",\\n             \"threat_level\": \"Medium\"\\n        },\\n         {\\n            \"name\": \"IBM Watson\",\\n            \"website\": \"https://www.ibm.com/watson\",\\n            \"industry\": \"AI development platforms\",\\n            \"threat_level\": \"Medium\"\\n        }\\n    ]\\n}\\n```\\n'\n",
      "'{\\n    \"competitors\": [\\n        {\\n            \"name\": \"Google AI\",\\n            \"website\": \"https://ai.google/\",\\n            \"industry\": \"AI development platforms\",\\n             \"threat_level\": \"High\"\\n        },\\n        {\\n            \"name\": \"Microsoft Azure AI\",\\n            \"website\": \"https://azure.microsoft.com/en-us/products/ai/\",\\n            \"industry\": \"AI development platforms\",\\n             \"threat_level\": \"High\"\\n        },\\n        {\\n            \"name\": \"Amazon SageMaker\",\\n             \"website\": \"https://aws.amazon.com/sagemaker/\",\\n            \"industry\": \"AI development platforms\",\\n             \"threat_level\": \"High\"\\n        },\\n         {\\n            \"name\": \"Hugging Face\",\\n            \"website\": \"https://huggingface.co/\",\\n            \"industry\": \"AI development platforms\",\\n             \"threat_level\": \"Medium\"\\n        },\\n         {\\n            \"name\": \"IBM Watson\",\\n            \"website\": \"https://www.ibm.com/watson\",\\n            \"industry\": \"AI development platforms\",\\n            \"threat_level\": \"Medium\"\\n        }\\n    ]\\n}'\n",
      "'{\\n    \"competitors\": [\\n        {\\n            \"name\": \"Google AI\",\\n            \"website\": \"https://ai.google/\",\\n            \"industry\": \"AI development platforms\",\\n             \"threat_level\": \"High\"\\n        },\\n        {\\n            \"name\": \"Microsoft Azure AI\",\\n            \"website\": \"https://azure.microsoft.com/en-us/products/ai/\",\\n            \"industry\": \"AI development platforms\",\\n             \"threat_level\": \"High\"\\n        },\\n        {\\n            \"name\": \"Amazon SageMaker\",\\n             \"website\": \"https://aws.amazon.com/sagemaker/\",\\n            \"industry\": \"AI development platforms\",\\n             \"threat_level\": \"High\"\\n        },\\n         {\\n            \"name\": \"Hugging Face\",\\n            \"website\": \"https://huggingface.co/\",\\n            \"industry\": \"AI development platforms\",\\n             \"threat_level\": \"Medium\"\\n        },\\n         {\\n            \"name\": \"IBM Watson\",\\n            \"website\": \"https://www.ibm.com/watson\",\\n            \"industry\": \"AI development platforms\",\\n            \"threat_level\": \"Medium\"\\n        }\\n    ]\\n}'\n",
      "'{\\n    \"competitors\": [\\n        {\\n            \"name\": \"Google AI\",\\n            \"website\": \"https://ai.google/\",\\n            \"industry\": \"AI development platforms\",\\n             \"threat_level\": \"High\"\\n        },\\n        {\\n            \"name\": \"Microsoft Azure AI\",\\n            \"website\": \"https://azure.microsoft.com/en-us/products/ai/\",\\n            \"industry\": \"AI development platforms\",\\n             \"threat_level\": \"High\"\\n        },\\n        {\\n            \"name\": \"Amazon SageMaker\",\\n             \"website\": \"https://aws.amazon.com/sagemaker/\",\\n            \"industry\": \"AI development platforms\",\\n             \"threat_level\": \"High\"\\n        },\\n         {\\n            \"name\": \"Hugging Face\",\\n            \"website\": \"https://huggingface.co/\",\\n            \"industry\": \"AI development platforms\",\\n             \"threat_level\": \"Medium\"\\n        },\\n         {\\n            \"name\": \"IBM Watson\",\\n            \"website\": \"https://www.ibm.com/watson\",\\n            \"industry\": \"AI development platforms\",\\n            \"threat_level\": \"Medium\"\\n        }\\n    ]\\n}'\n",
      "*** NameError: name 'reponse' is not defined\n",
      "'{\\n    \"competitors\": [\\n        {\\n            \"name\": \"Google AI\",\\n            \"website\": \"https://ai.google/\",\\n            \"industry\": \"AI development platforms\",\\n             \"threat_level\": \"High\"\\n        },\\n        {\\n            \"name\": \"Microsoft Azure AI\",\\n            \"website\": \"https://azure.microsoft.com/en-us/products/ai/\",\\n            \"industry\": \"AI development platforms\",\\n             \"threat_level\": \"High\"\\n        },\\n        {\\n            \"name\": \"Amazon SageMaker\",\\n             \"website\": \"https://aws.amazon.com/sagemaker/\",\\n            \"industry\": \"AI development platforms\",\\n             \"threat_level\": \"High\"\\n        },\\n         {\\n            \"name\": \"Hugging Face\",\\n            \"website\": \"https://huggingface.co/\",\\n            \"industry\": \"AI development platforms\",\\n             \"threat_level\": \"Medium\"\\n        },\\n         {\\n            \"name\": \"IBM Watson\",\\n            \"website\": \"https://www.ibm.com/watson\",\\n            \"industry\": \"AI development platforms\",\\n            \"threat_level\": \"Medium\"\\n        }\\n    ]\\n}'\n"
     ]
    }
   ],
   "source": [
    "result = await test_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_to_markdown(json_str, output_file=\"output.md\"):\n",
    "    \"\"\"\n",
    "    Converts a JSON string to a Markdown formatted string and saves it to a file.\n",
    "    Handles incomplete JSON strings due to potential truncation.\n",
    "\n",
    "    Args:\n",
    "        json_str: A string containing potentially incomplete JSON data.\n",
    "        output_file: The name of the file to save the markdown output to. Defaults to \"output.md\".\n",
    "\n",
    "    Returns:\n",
    "        A string containing the Markdown representation of the JSON data, or an error message.\n",
    "        Also saves the markdown output to the specified file if successful.\n",
    "    \"\"\"\n",
    "    # Remove the ```json\\n and \\n from the string and strip whitespace\n",
    "    json_str = json_str.replace(\"```json\\n\", \"\").strip()\n",
    "\n",
    "    # Check if the string is valid JSON and attempt to repair minimal issues\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        # Repair: Add closing braces or truncate to last valid index\n",
    "        braces_balance = json_str.count('{') - json_str.count('}')\n",
    "        brackets_balance = json_str.count('[') - json_str.count(']')\n",
    "        if braces_balance > 0:\n",
    "            json_str += '}' * braces_balance\n",
    "        elif brackets_balance > 0:\n",
    "            json_str += ']' * brackets_balance\n",
    "        else:\n",
    "            last_valid_index = max(json_str.rfind('}'), json_str.rfind(']'))\n",
    "            json_str = json_str[:last_valid_index + 1]\n",
    "        \n",
    "        # Re-validate JSON\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            return f\"Error: Invalid JSON string after repair: {e}\"\n",
    "\n",
    "    # Convert JSON to Markdown\n",
    "    def json_to_markdown(data, level=0):\n",
    "        indent = \"  \" * level\n",
    "        lines = []\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                lines.append(f\"{indent}**{key.replace('_', ' ').title()}:**\")\n",
    "                lines.extend(json_to_markdown(value, level + 1))\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                lines.extend(json_to_markdown(item, level + 1))\n",
    "        else:\n",
    "            lines.append(f\"{indent}- {data}\")\n",
    "        return lines\n",
    "\n",
    "    markdown_output = \"\\n\".join(json_to_markdown(data))\n",
    "\n",
    "    # Save to file\n",
    "    try:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(markdown_output)\n",
    "    except Exception as e:\n",
    "        return f\"Error: Could not save to file: {e}\"\n",
    "\n",
    "    return markdown_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**Executive Summary:**\\n  - The AI/ML market is characterized by a mix of cloud-based platforms, open-source frameworks, and specialized solutions. Key players include Google, Amazon, Microsoft, and IBM, each offering comprehensive AI/ML services with varying strengths. Open-source options like TensorFlow and PyTorch are critical, while AutoML solutions from DataRobot and H2O.ai cater to different user needs. Significant market gaps exist around MLOps simplification, XAI, model monitoring, cost transparency, and hardware accelerator utilization. This report provides a detailed analysis of the competitive landscape, highlighting key findings, recommendations, and market opportunities.\\n**Key Findings:**\\n    - Cloud-based platforms from Google, Amazon, and Microsoft dominate the market, providing extensive AI/ML services.\\n    - Open-source frameworks like TensorFlow and PyTorch are fundamental for much of AI/ML development, but are not fully encompassing solutions.\\n    - Automated Machine Learning (AutoML) is a growing trend, lowering the barrier to entry for AI/ML.\\n    - A clear distinction exists between code-centric and visual, user-friendly platforms.\\n    - MLOps simplification, Explainable AI (XAI), and cost transparency remain significant market gaps.\\n    - Each competitor has unique strengths, such as Google's TensorFlow integration, Amazon's breadth of services, Microsoft's enterprise focus, and IBM's governance capabilities.\\n    - Pricing models vary, with most cloud platforms offering consumption-based pricing while AutoML solutions are typically subscription based.\\n**Detailed Analysis:**\\n  **Market Overview:**\\n    **Market Patterns:**\\n        - The market is dominated by cloud-based AI/ML platforms (Google, Amazon, Microsoft, IBM) offering comprehensive suites of services.\\n        - There's a strong presence of open-source frameworks (TensorFlow, PyTorch) that underpin much of the AI/ML development.\\n        - Automated Machine Learning (AutoML) is a growing trend, aiming to lower the barrier of entry to AI/ML (DataRobot, H2O.ai, cloud platforms).\\n        - A clear segmentation exists between code-centric (TensorFlow, PyTorch) and visual, user-friendly platforms (RapidMiner, Alteryx).\\n        - There is a growing niche market of cloud based ML/AI development environment solutions (Paperspace)\\n        - Cloud-based platforms are increasingly offering managed services and pre-trained models to accelerate development.\\n        - Enterprise-focused solutions emphasize governance, collaboration, and integration with existing systems (IBM, Azure).\\n    **Market Gaps:**\\n        - Simplified MLOps solutions that are accessible to smaller teams and non-technical users.\\n        - More robust explainable AI (XAI) and interpretability features across the platforms, especially those that are AutoML focused.\\n        - Better tools for managing and monitoring model drift and degradation in real-world deployments.\\n        - Solutions that bridge the gap between enterprise governance and open-source flexibility.\\n        - Improved cost management features that can provide better transparency and predict future costs.\\n        - Easier and simpler tooling for setting up and utilizing hardware accelerators (GPUs, TPUs) across the ML lifecycle.\\n  **Competitor Analysis:**\\n    **Google Cloud Ai Platform:**\\n      **Overview:**\\n        - Google Cloud AI Platform offers a comprehensive suite of AI/ML services deeply integrated with the Google ecosystem. It leverages TensorFlow's power and provides robust MLOps capabilities with Kubeflow. Its focus is on scalability and advanced infrastructure.\\n      **Strengths:**\\n          - Strong integration with Google's ecosystem (BigQuery, TPUs).\\n          - Advanced MLOps capabilities with Kubeflow.\\n          - Leading in TensorFlow optimization.\\n          - Seamless integration with other Google Cloud services\\n          - Strong support for TensorFlow ecosystem\\n          - Advanced MLOps features with Kubeflow\\n          - Competitive pricing for certain workloads\\n          - Good support for serverless AI\\n      **Weaknesses:**\\n          - Can be complex to configure and manage for beginners\\n          - Potential vendor lock-in with Google Cloud ecosystem\\n          - Cost can escalate quickly with high resource usage\\n          - May not offer the same level of flexibility as open-source tools in all aspects\\n      **Target Audience:**\\n        - Data scientists, ML engineers, businesses building AI solutions\\n      **Unique Selling Points:**\\n          - Tight integration with the Google ecosystem\\n          - Large pre-trained model library\\n          - Strong focus on cloud scalability and infrastructure\\n    **Amazon Sagemaker:**\\n      **Overview:**\\n        - Amazon SageMaker is a mature platform with a broad array of pre-built algorithms and strong integration within the AWS ecosystem. It is known for its comprehensive ML service suite and large user community.\\n      **Strengths:**\\n          - Mature platform with a wide array of pre-built algorithms.\\n          - Tight integration within the AWS ecosystem.\\n          - Strong community and support.\\n          - Mature and feature-rich platform\\n          - Tight integration with the broader AWS ecosystem\\n          - Wide range of pre-built algorithms\\n          - Strong MLOps support with SageMaker Pipelines\\n          - Flexible deployment options\\n      **Weaknesses:**\\n          - Can be complex to manage due to numerous features\\n          - Potential vendor lock-in within the AWS ecosystem\\n          - Cost can become high with complex setups\\n          - Steep learning curve for new users\\n      **Target Audience:**\\n        - Data scientists, ML engineers, businesses seeking scalable AI\\n      **Unique Selling Points:**\\n          - Breadth of service offerings\\n          - Mature and well-supported platform\\n          - Deep integration with AWS ecosystem\\n    **Microsoft Azure Machine Learning:**\\n      **Overview:**\\n        - Microsoft Azure Machine Learning emphasizes enterprise AI and governance, offering deep integration with the Microsoft ecosystem and tools like Power BI. Its strengths are in user-friendliness and hybrid cloud capabilities.\\n      **Strengths:**\\n          - Deep integration with the Microsoft ecosystem and tools like Power BI.\\n          - Strong hybrid cloud capabilities.\\n          - User-friendly interface for the Azure ecosystem.\\n          - Strong integration with other Microsoft products and services\\n          - User-friendly interface (Azure ML Studio)\\n          - Good MLOps support with Azure ML pipelines\\n          - Competitive pricing compared to other cloud providers\\n          - Strong support for both GUI and code-based development\\n      **Weaknesses:**\\n          - Potential vendor lock-in with the Microsoft ecosystem\\n          - Can be challenging to manage complex configurations\\n          - Some components can be complex to fully utilize\\n          - Less mature than AWS SageMaker in some aspects\\n      **Target Audience:**\\n        - Data scientists, businesses using Microsoft technologies, enterprises\\n      **Unique Selling Points:**\\n          - Integration with Microsoft ecosystem\\n          - Hybrid cloud capabilities\\n          - Strong enterprise features\\n    **Ibm Watson Studio:**\\n      **Overview:**\\n        - IBM Watson Studio focuses on enterprise AI and governance, with hybrid deployment options and strong integration with other IBM Watson services. It provides good support for team collaboration and model management.\\n      **Strengths:**\\n          - Strong focus on enterprise AI and governance.\\n          - Hybrid deployment options.\\n          - Integration with other IBM Watson services.\\n          - Strong support for enterprise data science teams\\n          - Visual modeling tools for non-programmers\\n          - Integration with Watson APIs for AI capabilities\\n          - Good capabilities for data preparation and data wrangling\\n          - Hybrid cloud deployment options\\n      **Weaknesses:**\\n          - Potential vendor lock-in within IBM ecosystem\\n          - Can be more expensive than some competitors\\n          - Complex to navigate all of its features\\n          - Less popular and may have smaller community support than competitors\\n      **Target Audience:**\\n        - Data scientists, data science teams, enterprises looking for comprehensive AI solutions\\n      **Unique Selling Points:**\\n          - Strong focus on enterprise collaboration\\n          - Automated AI features\\n          - Wide range of integrations and IBM ecosystem benefits\\n    **Datarobot:**\\n      **Overview:**\\n        - DataRobot is known for its highly automated model building and deployment process. It focuses on business users and aims for faster time to value using comprehensive AutoML features.\\n      **Strengths:**\\n          - Highly automated model building and deployment.\\n          - Focus on business users and faster time to value.\\n          - Strong AutoML capabilities, speeding up model development\\n          - User-friendly interface for non-technical users\\n          - Automated model evaluation and selection\\n          - Good features for model monitoring and governance\\n      **Weaknesses:**\\n          - Less control over modeling process compared to coding platforms\\n          - Limited flexibility for custom models and workflows\\n          - Cost can be very high\\n          - Can act as a black box without full visibility into algorithms\\n      **Target Audience:**\\n        - Business analysts, citizen data scientists, companies needing rapid ML deployment\\n      **Unique Selling Points:**\\n          - Strong emphasis on automation and ease of use\\n          - Fast time-to-value\\n          - Business-centric outcomes\\n    **H2O Ai:**\\n      **Overview:**\\n        - H2O.ai is an open-source platform with a strong community. It offers Driverless AI for automated machine learning, emphasizing performance, scalability, and explainability of models.\\n      **Strengths:**\\n          - Open-source platform with a strong community.\\n          - Driverless AI for automated machine learning.\\n          - Strong performance, scalability, and focus on explainability.\\n          - High performance and speed\\n          - Strong support for various algorithms and frameworks\\n          - Flexible deployment options, including on-premise and cloud\\n          - Good capabilities for AutoML using Driverless AI\\n      **Weaknesses:**\\n          - Complexity can be high for new users\\n          - Driverless AI can be expensive\\n          - Requires in-depth knowledge to effectively leverage the platform\\n          - Less accessible for non-coders compared to visual platforms\\n      **Target Audience:**\\n        - Data Scientists, ML engineers, Enterprise AI teams\\n      **Unique Selling Points:**\\n          - Strong open source community\\n          - Enterprise level scalability\\n          - Highly performant ML capabilities\\n    **Tensorflow:**\\n      **Overview:**\\n        - TensorFlow is a dominant open-source library with a vast community. It's optimized for various hardware accelerators and is versatile for all ML use cases, known for it's cutting-edge research focus.\\n      **Strengths:**\\n          - Dominant open-source library with a vast community.\\n          - Optimized for various hardware accelerators.\\n          - Versatile and extensible framework for all ML use cases\\n          - Large community support and resources\\n          - Flexibility for advanced deep learning applications\\n          - Strong support for research and development\\n          - Good support for cloud and edge deployments\\n      **Weaknesses:**\\n          - Steep learning curve, particularly for beginners\\n          - Can be verbose and complex to write model definitions\\n          - Lower level interface that may not be beginner friendly\\n          - May require in depth knowledge to utilize performance optimizations\\n      **Target Audience:**\\n        - ML researchers, data scientists, deep learning engineers\\n      **Unique Selling Points:**\\n          - Extensive open source community support\\n          - Cutting edge research and development\\n          - Versatile and extensible framework\\n    **Pytorch:**\\n      **Overview:**\\n        - PyTorch is an open-source framework that is intuitive and flexible for research and experimentation. It is rapidly growing in popularity and is known for its ease of use and strong research adoption.\\n      **Strengths:**\\n          - Intuitive API and flexibility for research and experimentation.\\n          - Dynamic computation graph and ease of use.\\n          - Strong research adoption.\\n          - Flexibility and dynamic computation graphs\\n          - Ease of use and intuitive Python API\\n          - Strong community support and active development\\n          - Good for research and academic work\\n      **Weaknesses:**\\n          - Slightly less mature than TensorFlow in some areas\\n          - Less strong for production deployments than TensorFlow in some areas\\n          - Can be harder to debug than more mature libraries\\n          - May require a deeper understanding of model internals\\n      **Target Audience:**\\n        - ML researchers, deep learning engineers, developers seeking flexibility\\n      **Unique Selling Points:**\\n          - Ease of use and intuitive API\\n          - Dynamic graph computation\\n          - Strong emphasis on research\\n    **Rapidminer:**\\n      **Overview:**\\n        - RapidMiner provides a user-friendly visual workflow-based interface and is good for educational purposes. It's a comprehensive end-to-end data science platform suitable for all levels of users.\\n      **Strengths:**\\n          - User friendly visual workflow based interface.\\n          - Good for educational purposes and all levels of users.\\n          - End-to-end data science platform.\\n          - User friendly visual interface\\n          - Low code/no code solution\\n          - Large library of algorithms and operations\\n          - Good for end to end data science projects\\n      **Weaknesses:**\\n          - Limited control and flexibility compared to code based solutions\\n          - Can be expensive\\n          - Vendor lock-in\\n          - Less advanced deep learning capabilities compared to libraries such as PyTorch or TensorFlow\\n      **Target Audience:**\\n        - Data Scientists, Business Analysts, Data Engineers\\n      **Unique Selling Points:**\\n          - Visual workflow design interface\\n          - Comprehensive data science functionality\\n          - Supports entire data science lifecycle\\n    **Alteryx:**\\n      **Overview:**\\n        - Alteryx is known for its strong data blending and preparation capabilities, offering a user-friendly visual interface for non-technical users, and is good for automated data pipelines.\\n      **Strengths:**\\n          - Strong data blending and preparation capabilities.\\n          - User friendly visual and drag and drop interface for non technical users.\\n          - Good for building automated data pipelines and data analytics.\\n          - User friendly interface for data analytics\\n          - Good support for data preparation\\n          - Easy to automate data workflows with pre-built tools\\n          - Good for non coders\\n      **Weaknesses:**\\n          - Can be expensive\\n          - Limited deep learning and AI capabilities\\n          - Vendor lock-in\\n          - Less flexible than coding platforms\\n      **Target Audience:**\\n        - Business Analysts, Data Analysts, data-driven businesses\\n      **Unique Selling Points:**\\n          - Self-service analytics for business users\\n          - Data blending and preparation capabilities\\n          - Ease of use with drag and drop interface\\n    **Paperspace:**\\n      **Overview:**\\n        - Paperspace focuses on developer experience and simple cloud-based ML/AI environment setup. It is optimized for developer productivity with simple hardware management for high performance ML applications.\\n      **Strengths:**\\n          - Focus on developer experience and simple cloud based ML/AI environment setup.\\n          - Simplified setup and management of hardware resources\\n          - Strong GPU offerings for high performance ML applications.\\n          - Ease of use in creating ML/AI development environments\\n          - Choice of operating system\\n          - Good integration with different libraries\\n          - Good for fast prototyping and iterative development\\n      **Weaknesses:**\\n          - Can be more expensive than self managed infrastructure\\n          - Relatively less mature than the established cloud platforms\\n          - May not be the best for large scale model deployments\\n          - Less mature MLOps tooling compared to others\\n      **Target Audience:**\\n        - ML engineers, Data Scientists, AI developers\\n      **Unique Selling Points:**\\n          - GPU accelerated cloud computing resources\\n          - Developer friendly collaborative environment\\n          - Simplified setup and deployment for ML projects\\n**Recommendations:**\\n    - Invest in simplifying MLOps processes for non-technical users and smaller teams.\\n    - Develop more robust explainable AI (XAI) features to enhance model transparency and trust.\\n    - Provide better tools for managing and monitoring model performance in real-world deployments.\\n    - Focus on solutions that bridge the gap between enterprise governance requirements and open-source flexibility.\\n    - Offer more transparent cost management features and cost prediction tools.\\n    - Simplify tooling for setting up and utilizing hardware accelerators like GPUs and TPUs.\\n    - Enhance collaborative features to improve team-based data science projects.\\n    - Focus on user experience to provide a frictionless experience.\\n**Market Overview:**\\n  - The AI/ML market is dynamic, with a mix of established cloud providers, open-source tools, and specialized solutions. The market is growing rapidly, emphasizing the need for platforms that are accessible, transparent, and efficient. Addressing the gaps around MLOps, XAI, model monitoring, and cost management will be critical for success.\\n**Appendix:**\\n  **Threat Assessment:**\\n    **High:**\\n        - Google Cloud AI Platform:  A major player with the cloud ecosystem and open source support.\\n        - Amazon SageMaker:  Mature platform and broad range of services and integrations.\\n        - Microsoft Azure Machine Learning:  Enterprise focus and strong cloud based services and enterprise integrations.\\n        - IBM Watson Studio: Strong enterprise capabilities and hybrid cloud offerings.\\n    **Medium:**\\n        - DataRobot: Very strong in automation but relatively inflexible and expensive.\\n        - H2O.ai: Open source with strong community, performance and scalable platform, also has driverless AI.\\n        - TensorFlow (Google): Dominant open-source ML/DL framework.\\n        - PyTorch (Facebook): Strong and rising open-source framework, popular in research.\\n        - RapidMiner: Visual interface, comprehensive platform but does not have focus on the most advanced research.\\n        - Alteryx: Strong in data prep and user friendly workflow platform, less focused on advanced ML\\n        - Paperspace: Strong for development and ML environments.\\n  **Pricing Analysis Summary:**\\n    - Cloud platforms typically use consumption-based pricing, while AutoML solutions may use subscription based pricing. Discounts are common based on volume, term and usage.\\n  **Tech Stack Summary:**\\n    - Most competitors offer Python support and integrations with popular frameworks such as TensorFlow and PyTorch.\\n  **Marketing Summary:**\\n    - Cloud providers tend to position themselves based on their cloud ecosystem benefits. Open source platforms tend to lean on community and research. AutoML solutions often market ease of use and faster time to value. Visual based workflow products highlight ease of use.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_json_to_markdown(result, report_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
