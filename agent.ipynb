{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competitors research agent\n",
    "Steps for competitors research:\n",
    "1. Find list of competitors  \n",
    "2. Go to their website  \n",
    "3. Collect information including:  \n",
    "\t- standout features\n",
    "\t- product and pricing tiers\n",
    "\t- unique service proposition\n",
    "\t- marketing messages  \n",
    "4. Analyze competitors  \n",
    "\t- identify common patterns  \n",
    "\t- spot potential gaps  \n",
    "\t- compare pricing strategies  \n",
    "\t- compare messaging themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "import asyncio\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "\n",
    "from playwright.async_api import async_playwright\n",
    "from typing import Any, Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchLogger:\n",
    "    def __init__(self, research_id: str):\n",
    "        self.research_id = research_id\n",
    "        self.log_file = f\"research_logs/{research_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "        self.json_log_file = f\"research_logs/{research_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_data.json\"\n",
    "        \n",
    "        # Create logs directory if it doesn't exist\n",
    "        os.makedirs(\"research_logs\", exist_ok=True)\n",
    "        \n",
    "        # Set up file handler\n",
    "        self.file_handler = logging.FileHandler(self.log_file)\n",
    "        self.file_handler.setFormatter(\n",
    "            logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        )\n",
    "        \n",
    "        # Add file handler to root logger\n",
    "        logging.getLogger('').addHandler(self.file_handler)\n",
    "        \n",
    "        self.research_data = {\n",
    "            \"research_id\": research_id,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"steps\": []\n",
    "        }\n",
    "\n",
    "    def log_step(self, agent_name: str, action: str, input_data: Any, output_data: Any):\n",
    "        \"\"\"Log a research step with both input and output data\"\"\"\n",
    "        step_data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"agent\": agent_name,\n",
    "            \"action\": action,\n",
    "            \"input\": input_data,\n",
    "            \"output\": output_data\n",
    "        }\n",
    "        \n",
    "        self.research_data[\"steps\"].append(step_data)\n",
    "        \n",
    "        # Log to file\n",
    "        logger.info(f\"Agent: {agent_name} | Action: {action}\")\n",
    "        try:\n",
    "            logger.debug(f\"Input: {json.dumps(input_data, indent=2)}\")\n",
    "            logger.debug(f\"Output: {json.dumps(output_data, indent=2)}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error logging input/output data: {str(e)}\")\n",
    "            logger.debug(f\"Input: {input_data}\")\n",
    "            logger.debug(f\"Output: {output_data}\")\n",
    "        \n",
    "        # Save updated research data to JSON file\n",
    "        self._save_research_data()\n",
    "\n",
    "    def log_error(self, agent_name: str, action: str, error: Exception, context: Dict = None):\n",
    "        \"\"\"Log error information\"\"\"\n",
    "        error_data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"agent\": agent_name,\n",
    "            \"action\": action,\n",
    "            \"error\": str(error),\n",
    "            \"error_type\": type(error).__name__,\n",
    "            \"context\": context\n",
    "        }\n",
    "        \n",
    "        self.research_data[\"errors\"] = self.research_data.get(\"errors\", [])\n",
    "        self.research_data[\"errors\"].append(error_data)\n",
    "        \n",
    "        logger.error(f\"Error in {agent_name} during {action}: {str(error)}\")\n",
    "        if context:\n",
    "            logger.error(f\"Context: {json.dumps(context, indent=2)}\")\n",
    "        \n",
    "        self._save_research_data()\n",
    "\n",
    "    def _save_research_data(self):\n",
    "        \"\"\"Save the complete research data to JSON file\"\"\"\n",
    "        with open(self.json_log_file, 'w') as f:\n",
    "            json.dump(self.research_data, f, indent=2)\n",
    "\n",
    "    def get_research_summary(self) -> Dict:\n",
    "        \"\"\"Generate a summary of the research process\"\"\"\n",
    "        return {\n",
    "            \"research_id\": self.research_id,\n",
    "            \"total_steps\": len(self.research_data[\"steps\"]),\n",
    "            \"errors\": len(self.research_data.get(\"errors\", [])),\n",
    "            \"agents_involved\": list(set(step[\"agent\"] for step in self.research_data[\"steps\"])),\n",
    "            \"duration\": (datetime.now() - datetime.fromisoformat(self.research_data[\"timestamp\"])).total_seconds()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimiter:\n",
    "    def __init__(self, calls_per_minute: int):\n",
    "        self.calls_per_minute = calls_per_minute\n",
    "        self.calls = []\n",
    "        \n",
    "    async def wait_if_needed(self):\n",
    "        now = datetime.now()\n",
    "        self.calls = [call for call in self.calls \n",
    "                     if (now - call).total_seconds() < 60]\n",
    "        \n",
    "        if len(self.calls) >= self.calls_per_minute:\n",
    "            sleep_time = 60 - (now - self.calls[0]).total_seconds()\n",
    "            await asyncio.sleep(sleep_time)\n",
    "            \n",
    "        self.calls.append(now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.gemini_client = genai.Client(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "        self.rate_limiter = RateLimiter(calls_per_minute=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper:\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.subscription_key = os.getenv('BING_API_KEY')\n",
    "        self.endpoint = 'https://api.bing.microsoft.com/v7.0/search'\n",
    "        self.playwright = None\n",
    "        self.browser = None\n",
    "\n",
    "    async def initialize(self):\n",
    "        self.playwright = await async_playwright().start()\n",
    "        self.browser = await self.playwright.chromium.launch(\n",
    "            headless=True,\n",
    "            args=[\n",
    "            '--disable-blink-features=AutomationControlled',\n",
    "            '--no-sandbox',\n",
    "            '--disable-setuid-sandbox',\n",
    "            '--disable-dev-shm-usage',\n",
    "            '--disable-accelerated-2d-canvas',\n",
    "            '--disable-gpu'\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    async def cleanup(self):\n",
    "        \"\"\"Cleanup Playwright resources\"\"\"\n",
    "        if self.browser:\n",
    "            await self.browser.close()\n",
    "        if self.playwright:\n",
    "            await self.playwright.stop()\n",
    "\n",
    "    async def get_website_url(self, company_name: str) -> str:\n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"search_company_website\",\n",
    "                input_data={\"company_name\": company_name},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            headers = {'Ocp-Apim-Subscription-Key': self.subscription_key}\n",
    "            params = {\n",
    "                'q': f\"{company_name} official website\",\n",
    "                'count': 1\n",
    "            }\n",
    "            \n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(self.endpoint, headers=headers, params=params) as response:\n",
    "                    data = await response.json()\n",
    "                    website_url = data['webPages']['value'][0]['url']\n",
    "                    \n",
    "                    self.logger.log_step(\n",
    "                        agent_name=\"WebScraper\",\n",
    "                        action=\"found_website_url\",\n",
    "                        input_data={\"company_name\": company_name},\n",
    "                        output_data={\"website_url\": website_url}\n",
    "                    )\n",
    "                    \n",
    "                    return website_url\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"get_website_url\",\n",
    "                error=e,\n",
    "                context={\"company_name\": company_name}\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    async def _scroll_page(self, page):\n",
    "        \"\"\"Helper method to scroll the page and ensure content is loaded\"\"\"\n",
    "        try:\n",
    "            await page.evaluate(\"\"\"\n",
    "                async () => {\n",
    "                    await new Promise((resolve) => {\n",
    "                        let totalHeight = 0;\n",
    "                        const distance = 100;\n",
    "                        const timer = setInterval(() => {\n",
    "                            const scrollHeight = document.body.scrollHeight;\n",
    "                            window.scrollBy(0, distance);\n",
    "                            totalHeight += distance;\n",
    "                            \n",
    "                            if(totalHeight >= scrollHeight){\n",
    "                                clearInterval(timer);\n",
    "                                resolve();\n",
    "                            }\n",
    "                        }, 100);\n",
    "                    });\n",
    "                }\n",
    "            \"\"\")\n",
    "        except Exception:\n",
    "            # If scrolling fails, continue anyway\n",
    "            pass\n",
    "    async def extract_page_content(self, url: str) -> str:\n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"start_page_extraction\",\n",
    "                input_data={\"url\": url},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            if not self.browser:\n",
    "                await self.initialize()\n",
    "\n",
    "            page = await self.browser.new_page()\n",
    "            await page.set_extra_http_headers({\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            })\n",
    "            await page.set_viewport_size({\"width\": 1920, \"height\": 1080})\n",
    "\n",
    "            await page.goto(\n",
    "                url,\n",
    "                wait_until='domcontentloaded',\n",
    "                timeout=10000\n",
    "            )\n",
    "            # Wait for the main content to be available\n",
    "            try:\n",
    "                # Wait for any of these common selectors that might indicate the main content\n",
    "                await page.wait_for_selector('main, #content, .content, article, body', \n",
    "                                        timeout=5000,\n",
    "                                        state='visible')\n",
    "            except Exception:\n",
    "                # If we can't find specific elements, we'll still continue\n",
    "                pass\n",
    "        \n",
    "            # Check for Cloudfare or other protection\n",
    "            if await page.get_by_text(\"Just a moment\").count() > 0:\n",
    "                self.logger.log_step(\n",
    "                    agent_name=\"WebScraper\",\n",
    "                    action=\"detected_protection\",\n",
    "                    input_data={\"url\": url},\n",
    "                    output_data=None\n",
    "                )\n",
    "\n",
    "                await page.wait_for_timeout(5000)  # Wait longer for Cloudflare\n",
    "                await page.reload(wait_until='domcontentloaded', timeout=15000)\n",
    "                await page.wait_for_timeout(2000)\n",
    "\n",
    "            # Scroll to load any lazy-loaded content\n",
    "            await self._scroll_page(page)\n",
    "            \n",
    "            # Extract page content\n",
    "            content = await page.content()\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "\n",
    "            text = soup.get_text()\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"complete_page_extraction\",\n",
    "                input_data={\"url\": url},\n",
    "                output_data={\n",
    "                    \"content_length\": len(text),\n",
    "                    \"content_preview\": text[:200]\n",
    "                }\n",
    "            )\n",
    "\n",
    "            return text\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"extract_page_content\",\n",
    "                error=e,\n",
    "                context={\"url\": url}\n",
    "            )\n",
    "            raise\n",
    "    \n",
    "    async def process_pages_concurrently(self, urls: List[str], max_concurrent: int = 5) -> Dict[str, str]:\n",
    "        \"\"\"Process multiple pages concurrently with rate limiting\"\"\"\n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"start_concurrent_extraction\",\n",
    "                input_data={\"urls\": urls, \"max_concurrent\": max_concurrent},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            if not self.browser:\n",
    "                await self.initialize()\n",
    "\n",
    "            async def process_single_url(url: str) -> Tuple[str, str]:\n",
    "                try:\n",
    "                    content = await self.extract_page_content(url)\n",
    "                    return url, content\n",
    "                except Exception as e:\n",
    "                    self.logger.log_error(\n",
    "                        agent_name=\"WebScraper\",\n",
    "                        action=\"process_single_url\",\n",
    "                        error=e,\n",
    "                        context={\"url\": url}\n",
    "                    )\n",
    "                    return url, \"\"\n",
    "\n",
    "            # Process URLs in chunks to limit concurrent operations\n",
    "            results = {}\n",
    "            for i in range(0, len(urls), max_concurrent):\n",
    "                chunk = urls[i:i + max_concurrent]\n",
    "                chunk_tasks = [process_single_url(url) for url in chunk]\n",
    "                chunk_results = await asyncio.gather(*chunk_tasks, return_exceptions=True)\n",
    "                \n",
    "                for url, content in chunk_results:\n",
    "                    if content:  # Only store successful results\n",
    "                        results[url] = content\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"process_pages_concurrently\",\n",
    "                error=e,\n",
    "                context={\"urls\": urls}\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    async def search_company_page(self, company_name: str, page_type: str) -> str:\n",
    "        \"\"\"Search for a specific type of page for a company\"\"\"\n",
    "        try:\n",
    "            search_terms = {\n",
    "                'pricing': ['pricing', 'plans', 'packages'],\n",
    "                'features': ['features', 'product features'],\n",
    "                'products': ['products', 'solutions'],\n",
    "                'about': ['about', 'company information'],\n",
    "            }\n",
    "            \n",
    "            search_term = search_terms.get(page_type, [page_type])[0]\n",
    "            query = f\"{company_name} {search_term}\"\n",
    "            \n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"search_company_page\",\n",
    "                input_data={\"company_name\": company_name, \"page_type\": page_type},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            headers = {'Ocp-Apim-Subscription-Key': self.subscription_key}\n",
    "            params = {\n",
    "                'q': query,\n",
    "                'count': 1\n",
    "            }\n",
    "            \n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(self.endpoint, headers=headers, params=params) as response:\n",
    "                    data = await response.json()\n",
    "                    if 'webPages' in data and data['webPages']['value']:\n",
    "                        page_url = data['webPages']['value'][0]['url']\n",
    "                        \n",
    "                        self.logger.log_step(\n",
    "                            agent_name=\"WebScraper\",\n",
    "                            action=\"found_page_url\",\n",
    "                            input_data={\"query\": query},\n",
    "                            output_data={\"page_url\": page_url}\n",
    "                        )\n",
    "                        \n",
    "                        return page_url\n",
    "                    return None\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"search_company_page\",\n",
    "                error=e,\n",
    "                context={\"company_name\": company_name, \"page_type\": page_type}\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    async def analyze_company(self, company_name: str) -> Dict:\n",
    "      try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"start_company_analysis\",\n",
    "                input_data={\"company_name\": company_name},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            website_url = await self.get_website_url(company_name)\n",
    "            page_types = ['pricing', 'features', 'products', 'about']\n",
    "\n",
    "            urls_to_process = [website_url]\n",
    "            page_urls = {}\n",
    "\n",
    "            for page_type in page_types:\n",
    "                try:\n",
    "                    page_url = await self.search_company_page(company_name, page_type)\n",
    "                    if page_url:\n",
    "                        urls_to_process.append(page_url)\n",
    "                        page_urls[page_type] = page_url\n",
    "                except Exception as e:\n",
    "                    self.logger.log_error(\n",
    "                        agent_name=\"WebScraper\",\n",
    "                        action=\"get_page_url\",\n",
    "                        error=e,\n",
    "                        context={\"company_name\": company_name, \"page_type\": page_type}\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "            all_content = await self.process_pages_concurrently(urls_to_process)\n",
    "\n",
    "            company_data = {\n",
    "                \"name\": company_name,\n",
    "                \"website\": website_url,\n",
    "                \"pages\": {\n",
    "                    \"home\": {\n",
    "                        \"url\": website_url,\n",
    "                        \"content\": all_content.get(website_url, \"\")\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            for page_type, url in page_urls.items():\n",
    "                company_data[\"pages\"][page_type] = {\n",
    "                    \"url\": url,\n",
    "                    \"content\": all_content.get(url, \"\")\n",
    "                }\n",
    "\n",
    "            return company_data\n",
    "\n",
    "      except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"analyze_company\",\n",
    "                error=e,\n",
    "                context={\"company_name\": company_name}\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    async def analyze_competitors(self, competitors_data: Dict) -> Dict:\n",
    "        print(type(competitors_data))\n",
    "        if isinstance(competitors_data, str):\n",
    "            competitors_data = json.loads(competitors_data)\n",
    "            \n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"start_competitors_analysis\",\n",
    "                input_data={\"competitors\": [comp[\"name\"] for comp in competitors_data[\"competitors\"]]},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            results = {}\n",
    "            for competitor in competitors_data[\"competitors\"]:\n",
    "                try:\n",
    "                    results[competitor[\"name\"]] = await self.analyze_company(competitor[\"name\"])\n",
    "                except Exception as e:\n",
    "                    self.logger.log_error(\n",
    "                        agent_name=\"WebScraper\",\n",
    "                        action=\"analyze_competitor\",\n",
    "                        error=e,\n",
    "                        context={\"competitor\": competitor}\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"complete_competitors_analysis\",\n",
    "                input_data={\"competitors\": [comp[\"name\"] for comp in competitors_data[\"competitors\"]]},\n",
    "                output_data={\n",
    "                    \"competitors_analyzed\": list(results.keys()),\n",
    "                    \"total_competitors\": len(competitors_data[\"competitors\"]),\n",
    "                    \"successful_analyses\": len(results)\n",
    "                }\n",
    "            )\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"analyze_competitors\",\n",
    "                error=e,\n",
    "                context={\"competitors_data\": competitors_data}\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    async def extract_structured_data(self, content: str, data_type: str) -> Dict:\n",
    "        \"\"\"Extract specific types of data from page content\"\"\"\n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"start_structured_data_extraction\",\n",
    "                input_data={\"data_type\": data_type},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            # Use Gemini to extract structured data\n",
    "            prompt = f\"\"\"\n",
    "            Extract the following type of information: {data_type}\n",
    "            From the following content:\n",
    "            # TODO: Remove content limit?\n",
    "            {content[:1000]}  # Limit content length for API\n",
    "            \n",
    "            Return the information in JSON format.\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.config.gemini_client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash-exp\",\n",
    "                contents=prompt,\n",
    "                config=genai.types.GenerateContentConfig(\n",
    "                  temperature= 0.1,\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            structured_data = json.loads(response.text)\n",
    "            \n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"complete_structured_data_extraction\",\n",
    "                input_data={\"data_type\": data_type},\n",
    "                output_data=structured_data\n",
    "            )\n",
    "            \n",
    "            return structured_data\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"extract_structured_data\",\n",
    "                error=e,\n",
    "                context={\"data_type\": data_type}\n",
    "            )\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.system_prompt = \"\"\n",
    "        self.role = \"\"\n",
    "        self.goal = \"\"\n",
    "        self.backstory = \"\"\n",
    "        self.temperature = 1.0\n",
    "\n",
    "    async def execute(self, input_data: Any) -> Any:\n",
    "        await self.config.rate_limiter.wait_if_needed()\n",
    "        \n",
    "        try:\n",
    "            # Log the start of execution\n",
    "            self.logger.log_step(\n",
    "                agent_name=self.__class__.__name__,\n",
    "                action=\"start_execution\",\n",
    "                input_data=input_data,\n",
    "                output_data=None\n",
    "            )\n",
    "            \n",
    "            prompt = f\"\"\"\n",
    "            Role: {self.role}\n",
    "            Goal: {self.goal}\n",
    "            Backstory: {self.backstory}\n",
    "            System Instructions: {self.system_prompt}\n",
    "            \n",
    "            Input Data:\n",
    "            {json.dumps(input_data, indent=2)}\n",
    "            \n",
    "            Please provide your analysis based on the above information.\n",
    "            \"\"\"\n",
    "\n",
    "            response = self.config.gemini_client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash-exp\",\n",
    "                contents=prompt,\n",
    "                config=genai.types.GenerateContentConfig(\n",
    "                  temperature=self.temperature,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            self.logger.log_step(\n",
    "                agent_name=self.__class__.__name__,\n",
    "                action=\"loading_response_into_json\",\n",
    "                input_data=input_data,\n",
    "                output_data=response.text\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "              result = json.loads(response.text.replace('```json', '').replace('```', '').strip())\n",
    "            except Exception as e:\n",
    "              result = response.text\n",
    "              self.logger.log_error(\n",
    "                  agent_name=self.__class__.__name__,\n",
    "                  action=\"loading_response_into_json\",\n",
    "                  error=e,\n",
    "                  context={\"response\": result}\n",
    "            )\n",
    "\n",
    "            # Log the successful execution\n",
    "            self.logger.log_step(\n",
    "                agent_name=self.__class__.__name__,\n",
    "                action=\"complete_execution\",\n",
    "                input_data=input_data,\n",
    "                output_data=result\n",
    "            )\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Log the error\n",
    "            self.logger.log_error(\n",
    "                agent_name=self.__class__.__name__,\n",
    "                action=\"execute\",\n",
    "                error=e,\n",
    "                context={\"input_data\": input_data}\n",
    "            )\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketIntelligenceScout(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Expert market researcher specializing in competitor identification\"\n",
    "        self.goal = \"Identify and categorize the most relevant competitors\"\n",
    "        self.backstory = \"Former market research director with 15 years of experience\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        1. Search for top competitors in the given market\n",
    "        2. Return results in JSON format with the following structure:\n",
    "        {\n",
    "            \"competitors\": [\n",
    "                {\n",
    "                    \"name\": \"\",\n",
    "                    \"website\": \"\",\n",
    "                    \"industry\": \"\",\n",
    "                    \"threat_level\": \"\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitalProductAnalyst(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Product analysis specialist\"\n",
    "        self.goal = \"Analyze product features and capabilities\"\n",
    "        self.backstory = \"Previously a product manager at major tech companies\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Analyze each competitor's product features and capabilities.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"competitor_name\": {\n",
    "                \"key_features\": [],\n",
    "                \"unique_capabilities\": [],\n",
    "                \"user_experience\": \"\",\n",
    "                \"product_maturity\": \"\"\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketingMessageDecoder(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Marketing communications analyst\"\n",
    "        self.goal = \"Decode and analyze competitors' marketing strategies\"\n",
    "        self.backstory = \"Former copywriter turned marketing strategist\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Analyze marketing messages and positioning.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"competitor_name\": {\n",
    "                \"value_propositions\": [],\n",
    "                \"messaging_tone\": \"\",\n",
    "                \"target_audience\": \"\",\n",
    "                \"unique_selling_points\": []\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechnicalFeatureComparator(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Technical analyst specializing in feature comparison\"\n",
    "        self.goal = \"Provide detailed technical comparison of competitor products\"\n",
    "        self.backstory = \"Senior solutions architect with cross-industry experience\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Compare technical features across competitors.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"competitor_name\": {\n",
    "                \"tech_stack\": [],\n",
    "                \"api_capabilities\": [],\n",
    "                \"scalability_features\": [],\n",
    "                \"technical_advantages\": [],\n",
    "                \"technical_limitations\": []\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PricingStrategySpecialist(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Pricing analysis expert\"\n",
    "        self.goal = \"Analyze and compare pricing models and strategies\"\n",
    "        self.backstory = \"Former pricing consultant in SaaS industry\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Analyze pricing strategies and models.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"competitor_name\": {\n",
    "                \"pricing_tiers\": [],\n",
    "                \"pricing_model\": \"\",\n",
    "                \"discount_strategies\": [],\n",
    "                \"pricing_positioning\": \"\"\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetitiveStrategyAnalyst(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Strategic analyst specializing in competitive analysis\"\n",
    "        self.goal = \"Synthesize competitive intelligence into strategic insights\"\n",
    "        self.backstory = \"Strategy consultant from major consulting firms\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Synthesize all competitive data into strategic insights.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"market_patterns\": [],\n",
    "            \"competitive_advantages\": {},\n",
    "            \"market_gaps\": [],\n",
    "            \"strategic_recommendations\": [],\n",
    "            \"threat_assessment\": {}\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.temperature = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetitiveIntelligenceReportSpecialist(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Report creation specialist\"\n",
    "        self.goal = \"Create clear, actionable reports from competitive analysis\"\n",
    "        self.backstory = \"Communications expert in data visualization\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Create a comprehensive report from all analyses.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"executive_summary\": \"\",\n",
    "            \"key_findings\": [],\n",
    "            \"detailed_analysis\": {},\n",
    "            \"recommendations\": [],\n",
    "            \"market_overview\": \"\",\n",
    "            \"appendix\": {}\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.temperature = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetitiveAnalysisWorkflow:\n",
    "    def __init__(self, config: Config, research_id: str):\n",
    "        self.config = config\n",
    "        self.logger = ResearchLogger(research_id)\n",
    "\n",
    "        self.market_scout = MarketIntelligenceScout(config, self.logger)\n",
    "        self.product_analyst = DigitalProductAnalyst(config, self.logger)\n",
    "        self.marketing_decoder = MarketingMessageDecoder(config, self.logger)\n",
    "        self.technical_comparator = TechnicalFeatureComparator(config, self.logger)\n",
    "        self.pricing_specialist = PricingStrategySpecialist(config, self.logger)\n",
    "        self.competitive_analyst = CompetitiveStrategyAnalyst(config, self.logger)\n",
    "        self.report_specialist = CompetitiveIntelligenceReportSpecialist(config, self.logger)\n",
    "        self.web_scraper = WebScraper(config, self.logger)\n",
    "\n",
    "    async def run_parallel_analysis(self, competitors_data: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Run parallel analysis tasks\"\"\"\n",
    "        tasks = [\n",
    "            self.product_analyst.execute(competitors_data),\n",
    "            self.marketing_decoder.execute(competitors_data),\n",
    "            self.technical_comparator.execute(competitors_data),\n",
    "            self.pricing_specialist.execute(competitors_data)\n",
    "        ]\n",
    "        \n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        return {\n",
    "            \"product_analysis\": results[0],\n",
    "            \"marketing_analysis\": results[1],\n",
    "            \"technical_analysis\": results[2],\n",
    "            \"pricing_analysis\": results[3]\n",
    "        }\n",
    "\n",
    "    async def execute_workflow(self, industry: str, target_company: str = None) -> Dict:\n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"Workflow\",\n",
    "                action=\"start_workflow\",\n",
    "                input_data={\"industry\": industry, \"target_company\": target_company},\n",
    "                output_data=None\n",
    "            )\n",
    "            \n",
    "            # Step 1: Retrieve target company information (if necessar;)\n",
    "            target_company_data = None\n",
    "            if target_company:\n",
    "                target_company_data = await self.web_scraper.analyze_company(target_company)\n",
    "\n",
    "            # Step 2: Identify competitors\n",
    "            competitors_data = await self.market_scout.execute({\n",
    "                \"industry\": industry,\n",
    "                \"target_company\": target_company,\n",
    "                \"target_company_data\": target_company_data\n",
    "            })\n",
    "\n",
    "            # Step 3: Gather website information for all competitors\n",
    "            website_data = await self.web_scraper.analyze_competitors(competitors_data)\n",
    "\n",
    "            # # Step 4: Run parallel analysis with website data\n",
    "            # parallel_results = await self.run_parallel_analysis({\n",
    "            #     \"competitors_data\": competitors_data,\n",
    "            #     \"website_data\": website_data,\n",
    "            #     \"target_company_data\": target_company_data\n",
    "            # })\n",
    "\n",
    "            # # Step 3: Strategic analysis\n",
    "            # strategic_analysis = await self.competitive_analyst.execute({\n",
    "            #     \"competitors_data\": competitors_data,\n",
    "            #     \"parallel_results\": parallel_results\n",
    "            # })\n",
    "\n",
    "            # # Step 4: Generate report\n",
    "            # final_report = await self.report_specialist.execute({\n",
    "            #     \"strategic_analysis\": strategic_analysis,\n",
    "            #     \"raw_data\": {\n",
    "            #         \"competitors\": competitors_data,\n",
    "            #         \"analysis\": parallel_results\n",
    "            #     }\n",
    "            # })\n",
    "\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"Workflow\",\n",
    "                action=\"complete_workflow\",\n",
    "                input_data={\"industry\": industry, \"target_company\": target_company},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            return final_report\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"Workflow\",\n",
    "                action=\"execute_workflow\",\n",
    "                error=e,\n",
    "                context={\n",
    "                    \"industry\": industry,\n",
    "                    \"target_company\": target_company\n",
    "                }\n",
    "            )\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_workflow():\n",
    "    config = Config()\n",
    "    research_id = f\"research_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    workflow = CompetitiveAnalysisWorkflow(config, research_id)\n",
    "    \n",
    "    industry = \"AI development platforms\"\n",
    "    target_company = \"OpenAI\"  # Optional, can be None\n",
    "    \n",
    "    result = await workflow.execute_workflow(\n",
    "        industry=industry,\n",
    "        target_company=target_company\n",
    "    )\n",
    "    \n",
    "    # Print research summary\n",
    "    print(\"\\nResearch Summary:\")\n",
    "    print(json.dumps(workflow.logger.get_research_summary(), indent=2))\n",
    "    \n",
    "    print(\"\\nFinal Result:\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Agent: Workflow | Action: start_workflow\n",
      "INFO:__main__:Agent: WebScraper | Action: start_company_analysis\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_website\n",
      "INFO:__main__:Agent: WebScraper | Action: found_website_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: start_concurrent_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: MarketIntelligenceScout | Action: start_execution\n",
      "INFO:root:AFC is enabled with max remote calls: 10.\n",
      "INFO:__main__:Agent: MarketIntelligenceScout | Action: loading_response_into_json\n",
      "INFO:__main__:Agent: MarketIntelligenceScout | Action: complete_execution\n",
      "INFO:__main__:Agent: WebScraper | Action: start_competitors_analysis\n",
      "INFO:__main__:Agent: WebScraper | Action: start_company_analysis\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_website\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Agent: WebScraper | Action: found_website_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: start_concurrent_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_company_analysis\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_website\n",
      "INFO:__main__:Agent: WebScraper | Action: found_website_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: start_concurrent_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m test_workflow()\n",
      "Cell \u001b[0;32mIn[33], line 10\u001b[0m, in \u001b[0;36mtest_workflow\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m industry \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI development platforms\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m target_company \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Optional, can be None\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m workflow\u001b[38;5;241m.\u001b[39mexecute_workflow(\n\u001b[1;32m     11\u001b[0m     industry\u001b[38;5;241m=\u001b[39mindustry,\n\u001b[1;32m     12\u001b[0m     target_company\u001b[38;5;241m=\u001b[39mtarget_company\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Print research summary\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResearch Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[32], line 55\u001b[0m, in \u001b[0;36mCompetitiveAnalysisWorkflow.execute_workflow\u001b[0;34m(self, industry, target_company)\u001b[0m\n\u001b[1;32m     48\u001b[0m competitors_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmarket_scout\u001b[38;5;241m.\u001b[39mexecute({\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindustry\u001b[39m\u001b[38;5;124m\"\u001b[39m: industry,\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_company\u001b[39m\u001b[38;5;124m\"\u001b[39m: target_company,\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_company_data\u001b[39m\u001b[38;5;124m\"\u001b[39m: target_company_data\n\u001b[1;32m     52\u001b[0m })\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Step 3: Gather website information for all competitors\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m website_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweb_scraper\u001b[38;5;241m.\u001b[39manalyze_competitors(competitors_data)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# # Step 4: Run parallel analysis with website data\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# parallel_results = await self.run_parallel_analysis({\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#     \"competitors_data\": competitors_data,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m#     }\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# })\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_step(\n\u001b[1;32m     80\u001b[0m     agent_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorkflow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     81\u001b[0m     action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete_workflow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     82\u001b[0m     input_data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindustry\u001b[39m\u001b[38;5;124m\"\u001b[39m: industry, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_company\u001b[39m\u001b[38;5;124m\"\u001b[39m: target_company},\n\u001b[1;32m     83\u001b[0m     output_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     84\u001b[0m )\n",
      "Cell \u001b[0;32mIn[23], line 349\u001b[0m, in \u001b[0;36mWebScraper.analyze_competitors\u001b[0;34m(self, competitors_data)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m competitor \u001b[38;5;129;01min\u001b[39;00m competitors_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompetitors\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m         results[competitor[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyze_company(competitor[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_error(\n\u001b[1;32m    352\u001b[0m             agent_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWebScraper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    353\u001b[0m             action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalyze_competitor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    354\u001b[0m             error\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m    355\u001b[0m             context\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompetitor\u001b[39m\u001b[38;5;124m\"\u001b[39m: competitor}\n\u001b[1;32m    356\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[23], line 303\u001b[0m, in \u001b[0;36mWebScraper.analyze_company\u001b[0;34m(self, company_name)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_error(\n\u001b[1;32m    296\u001b[0m             agent_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWebScraper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    297\u001b[0m             action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_page_url\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    298\u001b[0m             error\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m    299\u001b[0m             context\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompany_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: company_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: page_type}\n\u001b[1;32m    300\u001b[0m         )\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m all_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_pages_concurrently(urls_to_process)\n\u001b[1;32m    305\u001b[0m company_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: company_name,\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwebsite\u001b[39m\u001b[38;5;124m\"\u001b[39m: website_url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     }\n\u001b[1;32m    314\u001b[0m }\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page_type, url \u001b[38;5;129;01min\u001b[39;00m page_urls\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[23], line 205\u001b[0m, in \u001b[0;36mWebScraper.process_pages_concurrently\u001b[0;34m(self, urls, max_concurrent)\u001b[0m\n\u001b[1;32m    203\u001b[0m chunk \u001b[38;5;241m=\u001b[39m urls[i:i \u001b[38;5;241m+\u001b[39m max_concurrent]\n\u001b[1;32m    204\u001b[0m chunk_tasks \u001b[38;5;241m=\u001b[39m [process_single_url(url) \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m chunk]\n\u001b[0;32m--> 205\u001b[0m chunk_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mchunk_tasks, return_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url, content \u001b[38;5;129;01min\u001b[39;00m chunk_results:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content:  \u001b[38;5;66;03m# Only store successful results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 189\u001b[0m, in \u001b[0;36mWebScraper.process_pages_concurrently.<locals>.process_single_url\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_single_url\u001b[39m(url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m         content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_page_content(url)\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m url, content\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[23], line 140\u001b[0m, in \u001b[0;36mWebScraper.extract_page_content\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mwait_for_timeout(\u001b[38;5;241m2000\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Scroll to load any lazy-loaded content\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scroll_page(page)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Extract page content\u001b[39;00m\n\u001b[1;32m    143\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mcontent()\n",
      "Cell \u001b[0;32mIn[23], line 72\u001b[0m, in \u001b[0;36mWebScraper._scroll_page\u001b[0;34m(self, page)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to scroll the page and ensure content is loaded\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mevaluate(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124m        async () => \u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124m            await new Promise((resolve) => \u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124m                let totalHeight = 0;\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124m                const distance = 100;\u001b[39m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124m                const timer = setInterval(() => \u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124m                    const scrollHeight = document.body.scrollHeight;\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124m                    window.scrollBy(0, distance);\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124m                    totalHeight += distance;\u001b[39m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124m                    \u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124m                    if(totalHeight >= scrollHeight)\u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124m                        clearInterval(timer);\u001b[39m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124m                        resolve();\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124m                    }\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124m                }, 100);\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124m            });\u001b[39m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124m        }\u001b[39m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# If scrolling fails, continue anyway\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/agent/lib/python3.13/site-packages/playwright/async_api/_generated.py:8495\u001b[0m, in \u001b[0;36mPage.evaluate\u001b[0;34m(self, expression, arg)\u001b[0m\n\u001b[1;32m   8442\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\n\u001b[1;32m   8443\u001b[0m     \u001b[38;5;28mself\u001b[39m, expression: \u001b[38;5;28mstr\u001b[39m, arg: typing\u001b[38;5;241m.\u001b[39mOptional[typing\u001b[38;5;241m.\u001b[39mAny] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   8444\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m   8445\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Page.evaluate\u001b[39;00m\n\u001b[1;32m   8446\u001b[0m \n\u001b[1;32m   8447\u001b[0m \u001b[38;5;124;03m    Returns the value of the `expression` invocation.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8491\u001b[0m \u001b[38;5;124;03m    Any\u001b[39;00m\n\u001b[1;32m   8492\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   8494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mfrom_maybe_impl(\n\u001b[0;32m-> 8495\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_obj\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[1;32m   8496\u001b[0m             expression\u001b[38;5;241m=\u001b[39mexpression, arg\u001b[38;5;241m=\u001b[39mmapping\u001b[38;5;241m.\u001b[39mto_impl(arg)\n\u001b[1;32m   8497\u001b[0m         )\n\u001b[1;32m   8498\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/agent/lib/python3.13/site-packages/playwright/_impl/_page.py:467\u001b[0m, in \u001b[0;36mPage.evaluate\u001b[0;34m(self, expression, arg)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, expression: \u001b[38;5;28mstr\u001b[39m, arg: Serializable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_main_frame\u001b[38;5;241m.\u001b[39mevaluate(expression, arg)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/agent/lib/python3.13/site-packages/playwright/_impl/_frame.py:278\u001b[0m, in \u001b[0;36mFrame.evaluate\u001b[0;34m(self, expression, arg)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, expression: \u001b[38;5;28mstr\u001b[39m, arg: Serializable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_result(\n\u001b[0;32m--> 278\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m    279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluateExpression\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    280\u001b[0m             \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    281\u001b[0m                 expression\u001b[38;5;241m=\u001b[39mexpression,\n\u001b[1;32m    282\u001b[0m                 arg\u001b[38;5;241m=\u001b[39mserialize_argument(arg),\n\u001b[1;32m    283\u001b[0m             ),\n\u001b[1;32m    284\u001b[0m         )\n\u001b[1;32m    285\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/agent/lib/python3.13/site-packages/playwright/_impl/_connection.py:61\u001b[0m, in \u001b[0;36mChannel.send\u001b[0;34m(self, method, params)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, params: Dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mwrap_api_call(\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_send(method, params, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_internal_type,\n\u001b[1;32m     64\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/agent/lib/python3.13/site-packages/playwright/_impl/_connection.py:526\u001b[0m, in \u001b[0;36mConnection.wrap_api_call\u001b[0;34m(self, cb, is_internal)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_zone\u001b[38;5;241m.\u001b[39mset(parsed_st)\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m cb()\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m rewrite_error(error, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparsed_st[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapiName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/agent/lib/python3.13/site-packages/playwright/_impl/_connection.py:92\u001b[0m, in \u001b[0;36mChannel._inner_send\u001b[0;34m(self, method, params, return_as_dict)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m     89\u001b[0m callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_send_message_to_server(\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object, method, _filter_none(params)\n\u001b[1;32m     91\u001b[0m )\n\u001b[0;32m---> 92\u001b[0m done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m     93\u001b[0m     {\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mon_error_future,\n\u001b[1;32m     95\u001b[0m         callback\u001b[38;5;241m.\u001b[39mfuture,\n\u001b[1;32m     96\u001b[0m     },\n\u001b[1;32m     97\u001b[0m     return_when\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39mFIRST_COMPLETED,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callback\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    100\u001b[0m     callback\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/agent/lib/python3.13/asyncio/tasks.py:451\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing coroutines is forbidden, use tasks explicitly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    450\u001b[0m loop \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _wait(fs, timeout, return_when, loop)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/agent/lib/python3.13/asyncio/tasks.py:537\u001b[0m, in \u001b[0;36m_wait\u001b[0;34m(fs, timeout, return_when, loop)\u001b[0m\n\u001b[1;32m    534\u001b[0m     f\u001b[38;5;241m.\u001b[39madd_done_callback(_on_completion)\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m waiter\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = await test_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_to_markdown(data, output_file=\"output.md\"):\n",
    "    # Convert JSON to Markdown\n",
    "    def json_to_markdown(data, level=0):\n",
    "        indent = \"  \" * level\n",
    "        lines = []\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                lines.append(f\"{indent}**{key.replace('_', ' ').title()}:**\")\n",
    "                lines.extend(json_to_markdown(value, level + 1))\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                lines.extend(json_to_markdown(item, level + 1))\n",
    "        else:\n",
    "            lines.append(f\"{indent}- {data}\")\n",
    "        return lines\n",
    "\n",
    "    markdown_output = \"\\n\".join(json_to_markdown(data))\n",
    "\n",
    "    # Save to file\n",
    "    try:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(markdown_output)\n",
    "    except Exception as e:\n",
    "        return f\"Error: Could not save to file: {e}\"\n",
    "\n",
    "    return markdown_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "report_name = f\"competitive_analysis_report_{current_timestamp}.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**Executive Summary:**\\n  - This report provides a comprehensive analysis of the competitive landscape in the AI development platform market, focusing on key players, their strengths, weaknesses, and market trends. The market is dominated by major cloud providers (Google, Microsoft, Amazon), but also features innovative companies like Hugging Face, Cohere, and AI21 Labs. The report identifies key market gaps, strategic recommendations, and a threat assessment to inform strategic decision-making.\\n**Key Findings:**\\n    - The AI development platform market is dominated by major cloud providers (Google, Microsoft, Amazon) offering comprehensive suites of AI services and infrastructure.\\n    - There is a significant trend towards open-source and community-driven platforms like Hugging Face, which foster collaboration and innovation.\\n    - Several players (Cohere, AI21 Labs) are carving out niches by focusing on specific strengths such as enterprise-grade security, multilingual models, and advanced retrieval capabilities.\\n    - The market is segmented between platforms targeting individual developers/researchers and those focused on enterprise clients.\\n    - Multimodality, large context windows, and RAG are becoming increasingly important features for AI models.\\n    - The market is shifting towards more flexible deployment options, including SaaS, cloud, VPC, and on-premises deployments.\\n    - Responsible AI and data privacy are growing concerns.\\n    - There is a need for more simplified and accessible AI development tools for non-technical users.\\n    - There is a need for more cost-effective solutions for small and medium-sized businesses.\\n**Detailed Analysis:**\\n  **Market Overview:**\\n    **Market Patterns:**\\n        - The AI development platform market is dominated by major cloud providers (Google, Microsoft, Amazon) offering comprehensive suites of AI services and infrastructure.\\n        - A significant trend is the rise of open-source and community-driven platforms like Hugging Face, which are fostering collaboration and innovation in the AI space.\\n        - Several players (Cohere, AI21 Labs) are carving out niches by focusing on specific strengths such as enterprise-grade security, multilingual models, and advanced retrieval capabilities.\\n        - There's a clear segmentation between platforms targeting individual developers and researchers (Hugging Face) and those focused on enterprise clients (Microsoft, Amazon, Cohere, AI21 Labs, IBM).\\n        - Multimodality, large context windows, and RAG are becoming increasingly important features for AI models.\\n        - The market is shifting towards more flexible deployment options, including SaaS, cloud, VPC, and on-premises deployments.\\n        - Responsible AI and data privacy are growing concerns, with companies like Microsoft and IBM emphasizing these aspects.\\n    **Market Gaps:**\\n        - A need for more simplified and accessible AI development tools for non-technical users.\\n        - A gap in the market for AI solutions that are both powerful and easy to integrate into existing workflows.\\n        - A need for more transparent and explainable AI models, particularly in sensitive applications.\\n        - A potential gap in the market for AI solutions that are specifically tailored to niche industries.\\n        - There is a need for more cost-effective solutions for small and medium-sized businesses.\\n        - A need for more robust data privacy and security solutions tailored to specific regulatory requirements.\\n  **Competitor Analysis:**\\n    **Google Ai:**\\n      **Competitive Advantages:**\\n          - Gemini models with multimodality and large context windows.\\n          - Integration with Google Search for grounding.\\n          - Free tier and flexible pay-as-you-go pricing.\\n          - Access to Google's infrastructure (TPUs, GPUs).\\n          - Free usage of Google AI Studio.\\n      **Technical Analysis:**\\n        **Tech Stack:**\\n            - Gemini models\\n            - TensorFlow\\n            - TPUs\\n            - Keras\\n            - Colab\\n            - Android\\n            - Chrome APIs\\n            - Vertex AI\\n            - Google Cloud\\n            - Dialogflow\\n            - TPUs\\n            - GPUs\\n            - CPUs\\n            - Kubernetes Engine\\n        **Api Capabilities:**\\n            - Gemini API\\n            - Text Embedding API\\n            - Natural Language API\\n            - Speech-to-Text API\\n            - Text-to-Speech API\\n            - Translation API\\n            - Vision API\\n            - Document AI API\\n            - Dialogflow API\\n        **Scalability Features:**\\n            - Pay-as-you-go pricing\\n            - Context caching\\n            - Vertex AI Platform\\n            - Google Kubernetes Engine\\n            - TPUs, GPUs, and CPUs for various workloads\\n            - Support for 15,000 nodes in a single cluster\\n        **Technical Advantages:**\\n            - Multimodal models (Gemini)\\n            - Large context windows (up to 2M tokens)\\n            - Context caching for efficiency\\n            - Search grounding capabilities\\n            - Wide range of pre-trained models\\n            - Strong integration with Google Cloud ecosystem\\n            - Free tier for testing\\n            - Support for various frameworks (Keras, TensorFlow)\\n            - Access to Google's infrastructure (TPUs, GPUs)\\n            - Code assistance tools (Gemini Code Assist)\\n        **Technical Limitations:**\\n            - Pricing can be complex with various models and features\\n            - Rate limits on free tier\\n            - Some features may have limited availability\\n            - Potential vendor lock-in with Google Cloud\\n            - Some models may not be available for fine-tuning\\n      **Marketing Analysis:**\\n        **Value Propositions:**\\n            - Fast and free to get started\\n            - Quickly integrate AI models with a Gemini API key\\n            - Generous free tier with flexible pay-as-you-go plans to help you scale\\n            - Access to our latest AI models\\n            - Experience Google DeepMind's Gemini models, built for multimodality to seamlessly understand text, code, images, audio, and video\\n            - Unlock breakthrough capabilities with 2M token context window, context caching, and search grounding features\\n        **Messaging Tone:**\\n          - Professional, technical, and encouraging\\n        **Target Audience:**\\n          - Developers, researchers, and businesses looking to integrate AI into their applications and workflows\\n        **Unique Selling Points:**\\n            - Gemini models with multimodality\\n            - Large context window (up to 2 million tokens)\\n            - Free tier and flexible pay-as-you-go pricing\\n            - Integration with Google Search for grounding\\n            - Free usage of Google AI Studio\\n      **Pricing Analysis:**\\n        **Pricing Tiers:**\\n            - Free tier with rate limits\\n            - Pay-as-you-go for Gemini 1.5 Flash\\n            - Pay-as-you-go for Gemini 1.5 Flash-8B\\n            - Pay-as-you-go for Gemini 1.5 Pro\\n            - Pay-as-you-go for Gemini 1.0 Pro\\n            - Free tier for Text Embedding 004\\n        **Pricing Model:**\\n          - Usage-based, pay-as-you-go, with free tier options for testing\\n        **Discount Strategies:**\\n        **Pricing Positioning:**\\n          - Competitive, with a focus on accessibility and scalability. Offers a free tier for experimentation and a range of models at different price points to cater to various needs.\\n    **Microsoft Azure Ai:**\\n      **Competitive Advantages:**\\n          - Integration with OpenAI models through Azure OpenAI Service.\\n          - Strong enterprise focus with security and compliance.\\n          - Comprehensive platform for the full generative AI lifecycle.\\n          - Integration with Microsoft's developer tools (GitHub, Visual Studio).\\n          - Model flexibility with a curated selection of models.\\n      **Technical Analysis:**\\n        **Tech Stack:**\\n            - Azure Machine Learning\\n            - Azure AI Services\\n            - Azure OpenAI Service\\n            - Azure AI Search\\n            - Azure Kubernetes Service (AKS)\\n            - Azure Container Apps\\n            - Azure Cosmos DB\\n            - Azure SQL Database\\n            - GitHub Enterprise\\n            - Visual Studio\\n        **Api Capabilities:**\\n            - Azure OpenAI Service API\\n            - Azure AI Search API\\n            - Azure AI Content Safety API\\n            - Azure AI Document Intelligence API\\n            - Azure AI Speech API\\n            - Azure AI Language API\\n            - Azure AI Translator API\\n            - Azure AI Vision API\\n        **Scalability Features:**\\n            - Azure AI Foundry\\n            - Azure Kubernetes Service (AKS)\\n            - Azure Container Apps\\n            - Pay-as-you-go pricing\\n            - 99.9% SLA\\n            - Support for various deployment strategies\\n        **Technical Advantages:**\\n            - Integration with OpenAI models\\n            - Model flexibility with Azure AI Model Catalog\\n            - Strong enterprise focus with security and compliance\\n            - Built-in safety features\\n            - Integration with GitHub and Visual Studio\\n            - Support for various frameworks and languages\\n            - Comprehensive AI lifecycle support\\n            - Large selection of foundation models\\n            - Customization options (fine-tuning)\\n            - RAG capabilities with Azure AI Search\\n        **Technical Limitations:**\\n            - Pricing can be complex\\n            - Potential vendor lock-in with Azure\\n            - Some services may require specific Azure infrastructure\\n            - Reliance on Microsoft ecosystem\\n      **Marketing Analysis:**\\n        **Value Propositions:**\\n            - Model flexibility with the Azure AI model catalog\\n            - Enterprise-grade generative AI with Azure OpenAI Service\\n            - Data integration with Azure AI Search for RAG\\n            - Built-in safety features against various AI threats\\n            - Integration with GitHub and Visual Studio\\n            - Seamless customization of models\\n            - Trustworthy AI with Microsoft's security, safety, and privacy commitments\\n        **Messaging Tone:**\\n          - Professional, enterprise-focused, and security-conscious\\n        **Target Audience:**\\n          - Enterprises, developers, and organizations seeking secure and scalable AI solutions\\n        **Unique Selling Points:**\\n            - Azure AI Foundry as an all-in-one toolkit\\n            - Azure OpenAI Service for secure, scalable access to OpenAI models\\n            - Azure AI Search for state-of-the-art RAG\\n            - Comprehensive protection against AI threats\\n            - Integration with Microsoft ecosystem (GitHub, Visual Studio)\\n            - Responsible AI commitments\\n      **Pricing Analysis:**\\n        **Pricing Tiers:**\\n        **Pricing Model:**\\n          - Pay-as-you-go, with a focus on enterprise scalability and flexibility\\n        **Discount Strategies:**\\n        **Pricing Positioning:**\\n          - Enterprise-focused, emphasizing model flexibility, customization, and trustworthy AI. Offers a wide range of services and models with a focus on security and scalability.\\n    **Amazon Machine Learning:**\\n      **Competitive Advantages:**\\n          - Broadest set of ML services and infrastructure.\\n          - Strong focus on scalability and performance.\\n          - Integration with AWS ecosystem.\\n          - Support for various AI frameworks (TensorFlow, PyTorch, MXNet).\\n          - Purpose-built infrastructure for distributed training (SageMaker HyperPod).\\n      **Technical Analysis:**\\n        **Tech Stack:**\\n            - Amazon SageMaker\\n            - Amazon EC2\\n            - Amazon S3\\n            - Amazon Aurora\\n            - Amazon DynamoDB\\n            - AWS Lambda\\n            - AWS Deep Learning AMIs\\n            - AWS Deep Learning Containers\\n            - TensorFlow\\n            - PyTorch\\n            - Apache MXNet\\n            - Hugging Face\\n            - AWS Inferentia\\n            - AWS Panorama\\n        **Api Capabilities:**\\n            - Amazon SageMaker API\\n            - Amazon Comprehend API\\n            - Amazon Polly API\\n            - Amazon Rekognition API\\n            - Amazon Textract API\\n            - Amazon Translate API\\n            - Amazon Transcribe API\\n            - Amazon Lex API\\n            - Amazon Bedrock API\\n        **Scalability Features:**\\n            - Amazon SageMaker\\n            - Amazon EC2 Auto Scaling\\n            - AWS Lambda\\n            - AWS Batch\\n            - AWS Parallel Computing Service\\n            - Pay-as-you-go pricing\\n            - Support for various instance types (Trn1, P5, Inf2, G5)\\n        **Technical Advantages:**\\n            - Broadest set of ML services and infrastructure\\n            - Support for various AI frameworks\\n            - Purpose-built infrastructure for ML workloads\\n            - Large selection of pre-trained models\\n            - Strong integration with AWS ecosystem\\n            - Scalable compute resources (EC2)\\n            - Cost-effective training options (Trn1 instances)\\n            - Tools for responsible AI development\\n            - Community and training resources\\n        **Technical Limitations:**\\n            - Complexity of AWS ecosystem\\n            - Pricing can be complex with various services\\n            - Potential vendor lock-in with AWS\\n            - Requires expertise in AWS services\\n      **Marketing Analysis:**\\n        **Value Propositions:**\\n            - Comprehensive set of ML services, infrastructure, and deployment resources\\n            - Build, train, and deploy machine learning and foundation models at scale with Amazon SageMaker\\n            - Broadest AI framework support\\n            - Purpose-built infrastructure for each step of the ML lifecycle\\n            - Responsible AI development with tools like Guardrails for Amazon Bedrock\\n        **Messaging Tone:**\\n          - Technical, comprehensive, and scalable\\n        **Target Audience:**\\n          - Developers, data scientists, and businesses seeking to build and deploy ML models at scale\\n        **Unique Selling Points:**\\n            - Amazon SageMaker for end-to-end ML lifecycle\\n            - Broadest AI framework support\\n            - Variety of EC2 instances optimized for ML workloads\\n            - Commitment to responsible AI\\n            - Large ecosystem of services and tools\\n      **Pricing Analysis:**\\n        **Pricing Tiers:**\\n        **Pricing Model:**\\n          - Pay-as-you-go, with a focus on scalability and a wide range of services\\n        **Discount Strategies:**\\n        **Pricing Positioning:**\\n          - Comprehensive, with a focus on providing a wide range of ML services and infrastructure. Offers a variety of tools and services for different stages of the ML lifecycle.\\n    **Hugging Face:**\\n      **Competitive Advantages:**\\n          - Largest open-source community for ML models and datasets.\\n          - Strong focus on open-source tools and libraries.\\n          - Platform for collaboration and sharing of ML resources.\\n          - Support for various modalities (text, image, video, audio, 3D).\\n          - Browser-based ML with Transformers.js.\\n      **Technical Analysis:**\\n        **Tech Stack:**\\n            - Transformers library\\n            - Diffusers\\n            - Safetensors\\n            - Hub Python Library\\n            - Tokenizers\\n            - PEFT\\n            - Transformers.js\\n            - timm\\n            - TRL\\n            - Datasets\\n            - Text Generation Inference\\n            - Accelerate\\n            - PyTorch\\n            - TensorFlow\\n            - JAX\\n        **Api Capabilities:**\\n            - Inference API\\n            - Text Generation Inference API\\n        **Scalability Features:**\\n            - Inference Endpoints\\n            - Spaces hardware upgrades\\n            - Persistent storage for Spaces\\n            - Support for various hardware accelerators (GPUs, TPUs)\\n        **Technical Advantages:**\\n            - Large community and open-source focus\\n            - Extensive collection of pre-trained models and datasets\\n            - Support for various frameworks (PyTorch, TensorFlow, JAX)\\n            - Tools for model training, fine-tuning, and deployment\\n            - Spaces for sharing and showcasing applications\\n            - Free tier for public models and datasets\\n            - Support for various modalities (text, image, audio, video)\\n            - Focus on accessibility and collaboration\\n        **Technical Limitations:**\\n            - Compute resources can be expensive\\n            - Enterprise features require paid subscriptions\\n            - May require technical expertise to use effectively\\n            - Limited control over infrastructure\\n      **Marketing Analysis:**\\n        **Value Propositions:**\\n            - The platform where the machine learning community collaborates on models, datasets, and applications\\n            - Host and collaborate on unlimited public models, datasets and applications\\n            - Move faster with the HF Open source stack\\n            - Explore all modalities: Text, image, video, audio or even 3D\\n            - Build your portfolio and share your work with the world\\n            - Paid Compute and Enterprise solutions for scaling ML\\n        **Messaging Tone:**\\n          - Community-driven, open-source, and collaborative\\n        **Target Audience:**\\n          - Machine learning researchers, developers, and enthusiasts\\n        **Unique Selling Points:**\\n            - Large community and open-source focus\\n            - Extensive library of models, datasets, and applications\\n            - Support for various modalities\\n            - Free tier for collaboration\\n            - Paid compute and enterprise solutions for scaling\\n      **Pricing Analysis:**\\n        **Pricing Tiers:**\\n            - Free tier for public models, datasets and applications\\n            - Pro account for advanced features\\n            - Enterprise Hub for enterprise features\\n            - Pay-per-hour for Spaces hardware upgrades\\n            - Pay-per-hour for Inference Endpoints\\n        **Pricing Model:**\\n          - Freemium model with paid options for compute, enterprise features, and dedicated resources\\n        **Discount Strategies:**\\n        **Pricing Positioning:**\\n          - Community-focused, with a free tier for collaboration and open-source contributions. Offers paid options for compute and enterprise features, catering to both individual developers and organizations.\\n    **Cohere:**\\n      **Competitive Advantages:**\\n          - Strong focus on enterprise-grade security and data protection.\\n          - Private deployment options for data sovereignty.\\n          - Cutting-edge multilingual models.\\n          - Advanced retrieval capabilities.\\n          - AI workspace tailored for enterprises.\\n      **Technical Analysis:**\\n        **Tech Stack:**\\n            - Command models\\n            - Embed models\\n            - Rerank models\\n            - Transformer architecture\\n            - Supercomputers\\n        **Api Capabilities:**\\n            - Command API\\n            - Embed API\\n            - Rerank API\\n        **Scalability Features:**\\n            - Scalable language models\\n            - Private deployment options\\n            - Support for cloud providers (AWS, Azure, OCI, GCP)\\n            - VPC and on-premises deployment options\\n        **Technical Advantages:**\\n            - Focus on enterprise-grade security and data protection\\n            - Multilingual models\\n            - Advanced retrieval capabilities\\n            - Built-in RAG\\n            - Customizable AI solutions\\n            - Flexible deployment options\\n            - Low-code solutions\\n            - Dedicated support for enterprises\\n        **Technical Limitations:**\\n            - Pricing can be complex\\n            - May require enterprise-level contracts\\n            - Less open-source focus compared to Hugging Face\\n            - Limited information on specific model architectures\\n      **Marketing Analysis:**\\n        **Value Propositions:**\\n            - Cutting-edge multilingual models, advanced retrieval, and an AI workspace\\n            - Designed for the modern enterprise\\n            - High-performance, scalable language models (Command)\\n            - Leading multimodal search and retrieval model (Embed)\\n            - Powerful model for semantic search boost (Rerank)\\n            - Built-in retrieval-augmented generation (RAG)\\n            - Enterprise-grade security, advanced access controls, and private deployment options\\n        **Messaging Tone:**\\n          - Professional, enterprise-focused, and innovative\\n        **Target Audience:**\\n          - Enterprises and developers seeking secure and customizable AI solutions\\n        **Unique Selling Points:**\\n            - All-in-one platform for private and secure AI\\n            - Cutting-edge multilingual models\\n            - Advanced retrieval capabilities\\n            - Built-in RAG\\n            - Private deployment options (SaaS, cloud, VPC, on-premises)\\n            - Focus on enterprise security and data protection\\n      **Pricing Analysis:**\\n        **Pricing Tiers:**\\n            - Pay-as-you-go for Command R+\\n            - Pay-as-you-go for Command R\\n            - Pay-as-you-go for Fine-tuned Command R\\n            - Pay-as-you-go for Command R7B\\n            - Pay-per-search for Rerank\\n            - Pay-per-token for Embed\\n        **Pricing Model:**\\n          - Usage-based, pay-as-you-go, with a focus on enterprise-grade models and security\\n        **Discount Strategies:**\\n        **Pricing Positioning:**\\n          - Enterprise-focused, emphasizing security, customization, and advanced retrieval capabilities. Offers a range of models with different price points and deployment options.\\n    **Ai21 Labs:**\\n      **Competitive Advantages:**\\n          - Production-grade Mamba-based model (Jamba).\\n          - Built-in RAG engine for grounded conversational AI.\\n          - Focus on high-quality, out-of-the-box value.\\n          - Dedicated integration team for deployment.\\n          - Task-specific models for reading and writing.\\n      **Technical Analysis:**\\n        **Tech Stack:**\\n            - Jamba models\\n            - Jurassic-1\\n            - Jurassic-2\\n            - Mamba SSM\\n            - Transformer architecture\\n        **Api Capabilities:**\\n            - AI21 Studio API\\n            - Wordtune API\\n            - Contextual Answers API\\n            - Embeddings API\\n        **Scalability Features:**\\n            - Pay-as-you-go pricing\\n            - Flexible deployment options (SaaS, Private VPC, On-Prem)\\n            - Built-in RAG engine\\n        **Technical Advantages:**\\n            - High-quality and performant models\\n            - Out-of-the-box RAG capabilities\\n            - Enterprise-grade security\\n            - Dedicated integration team\\n            - Task-specific models for various NLP tasks\\n            - Transparent pricing\\n            - Efficient tokenization\\n        **Technical Limitations:**\\n            - Limited information on specific model architectures\\n            - May require custom implementation for complex use cases\\n            - Less open-source focus compared to Hugging Face\\n      **Marketing Analysis:**\\n        **Value Propositions:**\\n            - Accurate, reliable, and scalable generative AI tailored to specific needs\\n            - Human-centered AI built for production\\n            - Groundbreaking foundation models like Jamba\\n            - Conversational AI grounded in enterprise data\\n            - Turnkey tools and rigorous security protocols\\n            - Built-in RAG engine\\n            - Flexible deployment options\\n        **Messaging Tone:**\\n          - Professional, reliable, and innovative\\n        **Target Audience:**\\n          - Enterprises seeking practical and scalable AI solutions\\n        **Unique Selling Points:**\\n            - Jamba model with Mamba architecture\\n            - Built-in RAG engine\\n            - Enterprise-grade security\\n            - Dedicated integration team\\n            - Flexible deployment options\\n            - Focus on production-ready AI\\n      **Pricing Analysis:**\\n        **Pricing Tiers:**\\n            - Free trial with credits\\n            - Pay-as-you-go for Foundation Models\\n            - Pay-as-you-go for Task-Specific Models\\n            - Custom plan for scaling and dedicated support\\n        **Pricing Model:**\\n          - Usage-based, pay-as-you-go, with a free trial and custom plans for enterprise\\n        **Discount Strategies:**\\n        **Pricing Positioning:**\\n          - Focus on reliable, accurate, and scalable generative AI for enterprise workflows. Offers a free trial and a range of models and task-specific APIs.\\n    **Ibm Watson:**\\n      **Competitive Advantages:**\\n          - Long history of AI research and development.\\n          - Comprehensive platform for AI development and deployment.\\n          - Integration with IBM Cloud and other cloud platforms.\\n          - Focus on responsible, transparent, and explainable AI.\\n          - Wide range of AI applications across various industries.\\n      **Technical Analysis:**\\n        **Tech Stack:**\\n            - DeepQA software\\n            - Apache UIMA\\n            - POWER7 processors\\n            - IBM Cloud Pak for Data\\n            - SPSS Modeler\\n            - Watson Assistant\\n            - Watsonx.ai\\n            - Watsonx.data\\n            - Watsonx.governance\\n        **Api Capabilities:**\\n            - Watson Studio APIs\\n            - Watson Assistant APIs\\n            - Watson Discovery APIs\\n        **Scalability Features:**\\n            - IBM Cloud Pak for Data\\n            - Various deployment options (public, private, on-premises)\\n            - Support for large-scale data processing\\n            - Committed-term licenses and pay-as-you-go options\\n        **Technical Advantages:**\\n            - Long history and experience in AI\\n            - Strong focus on enterprise solutions\\n            - Support for various deployment options\\n            - Integration with IBM Cloud ecosystem\\n            - Tools for data preparation, model development, and deployment\\n            - AI governance and transparency features\\n            - Support for open-source frameworks\\n        **Technical Limitations:**\\n            - Complexity of IBM ecosystem\\n            - Pricing can be complex\\n            - Potential vendor lock-in with IBM\\n            - May require expertise in IBM products\\n      **Marketing Analysis:**\\n        **Value Propositions:**\\n            - Advanced enterprise AI with 70 years of research\\n            - Watsonx portfolio for training, tuning, and distributing models\\n            - Generative AI and machine learning capabilities\\n            - Responsible, transparent, and explainable data and AI workflows\\n            - AI-powered virtual agents without coding\\n            - AI-generated code recommendations\\n        **Messaging Tone:**\\n          - Authoritative, experienced, and enterprise-focused\\n        **Target Audience:**\\n          - Enterprises and organizations seeking to leverage AI for various business applications\\n        **Unique Selling Points:**\\n            - Long history of AI research and development\\n            - Watsonx portfolio for end-to-end AI lifecycle\\n            - Focus on responsible AI and governance\\n            - AI-powered virtual agents and code assistants\\n            - Integration with IBM Cloud Pak for Data\\n      **Pricing Analysis:**\\n        **Pricing Tiers:**\\n            - Committed-term license for IBM Watson Studio on IBM Cloud Pak for Data\\n            - Pay-as-you-go for IBM Watson Studio on IBM Cloud Pak for Data as a Service\\n        **Pricing Model:**\\n          - Subscription-based, with options for committed-term licenses or pay-as-you-go\\n        **Discount Strategies:**\\n        **Pricing Positioning:**\\n          - Enterprise-focused, emphasizing data science capabilities and AI lifecycle management. Offers multiple deployment options and a range of tools for building and deploying AI models.\\n  **Threat Assessment:**\\n    **High:**\\n        - Google AI\\n        - Microsoft Azure AI\\n        - Amazon Machine Learning\\n    **Medium:**\\n        - Hugging Face\\n        - Cohere\\n        - AI21 Labs\\n        - IBM Watson\\n    **Analysis:**\\n      - The major cloud providers (Google, Microsoft, Amazon) pose the highest threat due to their extensive resources, established infrastructure, and comprehensive AI offerings. While Hugging Face, Cohere, AI21 Labs, and IBM Watson are significant players, they currently have a smaller market share and are focusing on specific niches. However, their innovative approaches and strong focus on particular strengths could pose a greater threat in the future. The open-source nature of Hugging Face poses a unique threat as it is community driven and can rapidly evolve. Cohere and AI21 Labs are focusing on enterprise solutions which can be a significant threat to the major cloud providers.\\n**Recommendations:**\\n    - Focus on developing AI solutions that are easy to use and integrate, catering to a broader audience beyond technical experts.\\n    - Invest in building transparent and explainable AI models to address growing concerns about bias and fairness.\\n    - Explore opportunities to develop AI solutions tailored to specific industry verticals, leveraging domain expertise.\\n    - Offer more flexible pricing models and cost-effective solutions to attract small and medium-sized businesses.\\n    - Prioritize data privacy and security in AI development, and offer solutions that meet specific regulatory requirements.\\n    - Continue to invest in cutting-edge research and development to stay ahead of the competition in areas like multimodality, large context windows, and RAG.\\n    - Build strong partnerships and collaborations with other players in the ecosystem to expand reach and capabilities.\\n    - Continuously monitor the market and adapt to emerging trends and technologies to maintain a competitive edge.\\n**Market Overview:**\\n  - The AI development platform market is a rapidly evolving space, characterized by a mix of established tech giants and innovative startups. Major cloud providers like Google, Microsoft, and Amazon dominate with their comprehensive suites of AI services and extensive infrastructure. However, open-source platforms like Hugging Face are significantly impacting the market by fostering collaboration and innovation within the AI community. Companies like Cohere and AI21 Labs are carving out niches by focusing on specific strengths such as enterprise-grade security, multilingual models, and advanced retrieval capabilities. The market is also seeing a clear segmentation between platforms targeting individual developers and researchers and those focused on enterprise clients. Key trends include the increasing importance of multimodality, large context windows, and RAG, as well as a shift towards more flexible deployment options and a growing emphasis on responsible AI and data privacy.\\n**Appendix:**\\n  **Competitor List:**\\n      **Name:**\\n        - Google AI\\n      **Website:**\\n        - https://ai.google/\\n      **Industry:**\\n        - AI development platforms\\n      **Threat Level:**\\n        - High\\n      **Name:**\\n        - Microsoft Azure AI\\n      **Website:**\\n        - https://azure.microsoft.com/en-us/products/ai-services\\n      **Industry:**\\n        - AI development platforms\\n      **Threat Level:**\\n        - High\\n      **Name:**\\n        - Amazon Machine Learning\\n      **Website:**\\n        - https://aws.amazon.com/machine-learning/\\n      **Industry:**\\n        - AI development platforms\\n      **Threat Level:**\\n        - High\\n      **Name:**\\n        - Hugging Face\\n      **Website:**\\n        - https://huggingface.co/\\n      **Industry:**\\n        - AI development platforms\\n      **Threat Level:**\\n        - Medium\\n      **Name:**\\n        - Cohere\\n      **Website:**\\n        - https://cohere.com/\\n      **Industry:**\\n        - AI development platforms\\n      **Threat Level:**\\n        - Medium\\n      **Name:**\\n        - AI21 Labs\\n      **Website:**\\n        - https://www.ai21.com/\\n      **Industry:**\\n        - AI development platforms\\n      **Threat Level:**\\n        - Medium\\n      **Name:**\\n        - IBM Watson\\n      **Website:**\\n        - https://www.ibm.com/watson\\n      **Industry:**\\n        - AI development platforms\\n      **Threat Level:**\\n        - Medium\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_json_to_markdown(result, report_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
