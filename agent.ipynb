{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competitors research agent\n",
    "Steps for competitors research:\n",
    "1. Find list of competitors  \n",
    "2. Go to their website  \n",
    "3. Collect information including:  \n",
    "\t- standout features\n",
    "\t- product and pricing tiers\n",
    "\t- unique service proposition\n",
    "\t- marketing messages  \n",
    "4. Analyze competitors  \n",
    "\t- identify common patterns  \n",
    "\t- spot potential gaps  \n",
    "\t- compare pricing strategies  \n",
    "\t- compare messaging themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "import asyncio\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "\n",
    "from playwright.async_api import async_playwright\n",
    "from typing import Any, Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchLogger:\n",
    "    def __init__(self, research_id: str):\n",
    "        self.research_id = research_id\n",
    "        self.log_file = f\"research_logs/{research_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "        self.json_log_file = f\"research_logs/{research_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_data.json\"\n",
    "        \n",
    "        # Create logs directory if it doesn't exist\n",
    "        os.makedirs(\"research_logs\", exist_ok=True)\n",
    "        \n",
    "        # Set up file handler\n",
    "        self.file_handler = logging.FileHandler(self.log_file)\n",
    "        self.file_handler.setFormatter(\n",
    "            logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        )\n",
    "        \n",
    "        # Add file handler to root logger\n",
    "        logging.getLogger('').addHandler(self.file_handler)\n",
    "        \n",
    "        self.research_data = {\n",
    "            \"research_id\": research_id,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"steps\": []\n",
    "        }\n",
    "\n",
    "    def log_step(self, agent_name: str, action: str, input_data: Any, output_data: Any):\n",
    "        \"\"\"Log a research step with both input and output data\"\"\"\n",
    "        step_data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"agent\": agent_name,\n",
    "            \"action\": action,\n",
    "            \"input\": input_data,\n",
    "            \"output\": output_data\n",
    "        }\n",
    "        \n",
    "        self.research_data[\"steps\"].append(step_data)\n",
    "        \n",
    "        # Log to file\n",
    "        logger.info(f\"Agent: {agent_name} | Action: {action}\")\n",
    "        try:\n",
    "            logger.debug(f\"Input: {json.dumps(input_data, indent=2)}\")\n",
    "            logger.debug(f\"Output: {json.dumps(output_data, indent=2)}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error logging input/output data: {str(e)}\")\n",
    "            logger.debug(f\"Input: {input_data}\")\n",
    "            logger.debug(f\"Output: {output_data}\")\n",
    "        \n",
    "        # Save updated research data to JSON file\n",
    "        self._save_research_data()\n",
    "\n",
    "    def log_error(self, agent_name: str, action: str, error: Exception, context: Dict = None):\n",
    "        \"\"\"Log error information\"\"\"\n",
    "        error_data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"agent\": agent_name,\n",
    "            \"action\": action,\n",
    "            \"error\": str(error),\n",
    "            \"error_type\": type(error).__name__,\n",
    "            \"context\": context\n",
    "        }\n",
    "        \n",
    "        self.research_data[\"errors\"] = self.research_data.get(\"errors\", [])\n",
    "        self.research_data[\"errors\"].append(error_data)\n",
    "        \n",
    "        logger.error(f\"Error in {agent_name} during {action}: {str(error)}\")\n",
    "        if context:\n",
    "            logger.error(f\"Context: {json.dumps(context, indent=2)}\")\n",
    "        \n",
    "        self._save_research_data()\n",
    "\n",
    "    def _save_research_data(self):\n",
    "        \"\"\"Save the complete research data to JSON file\"\"\"\n",
    "        with open(self.json_log_file, 'w') as f:\n",
    "            json.dump(self.research_data, f, indent=2)\n",
    "\n",
    "    def get_research_summary(self) -> Dict:\n",
    "        \"\"\"Generate a summary of the research process\"\"\"\n",
    "        return {\n",
    "            \"research_id\": self.research_id,\n",
    "            \"total_steps\": len(self.research_data[\"steps\"]),\n",
    "            \"errors\": len(self.research_data.get(\"errors\", [])),\n",
    "            \"agents_involved\": list(set(step[\"agent\"] for step in self.research_data[\"steps\"])),\n",
    "            \"duration\": (datetime.now() - datetime.fromisoformat(self.research_data[\"timestamp\"])).total_seconds()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimiter:\n",
    "    def __init__(self, calls_per_minute: int):\n",
    "        self.calls_per_minute = calls_per_minute\n",
    "        self.calls = []\n",
    "        \n",
    "    async def wait_if_needed(self):\n",
    "        now = datetime.now()\n",
    "        self.calls = [call for call in self.calls \n",
    "                     if (now - call).total_seconds() < 60]\n",
    "        \n",
    "        if len(self.calls) >= self.calls_per_minute:\n",
    "            sleep_time = 60 - (now - self.calls[0]).total_seconds()\n",
    "            await asyncio.sleep(sleep_time)\n",
    "            \n",
    "        self.calls.append(now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.gemini_client = genai.Client(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "        self.rate_limiter = RateLimiter(calls_per_minute=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper:\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.subscription_key = os.getenv('BING_API_KEY')\n",
    "        self.endpoint = 'https://api.bing.microsoft.com/v7.0/search'\n",
    "        self.playwright = None\n",
    "        self.browser = None\n",
    "\n",
    "    async def initialize(self):\n",
    "        self.playwright = await async_playwright().start()\n",
    "        self.browser = await self.playwright.chromium.launch()\n",
    "        \n",
    "    async def cleanup(self):\n",
    "        \"\"\"Cleanup Playwright resources\"\"\"\n",
    "        if self.browser:\n",
    "            await self.browser.close()\n",
    "        if self.playwright:\n",
    "            await self.playwright.stop()\n",
    "\n",
    "    async def get_website_url(self, company_name: str) -> str:\n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"search_company_website\",\n",
    "                input_data={\"company_name\": company_name},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            headers = {'Ocp-Apim-Subscription-Key': self.subscription_key}\n",
    "            params = {\n",
    "                'q': f\"{company_name} official website\",\n",
    "                'count': 1\n",
    "            }\n",
    "            \n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(self.endpoint, headers=headers, params=params) as response:\n",
    "                    data = await response.json()\n",
    "                    website_url = data['webPages']['value'][0]['url']\n",
    "                    \n",
    "                    self.logger.log_step(\n",
    "                        agent_name=\"WebScraper\",\n",
    "                        action=\"found_website_url\",\n",
    "                        input_data={\"company_name\": company_name},\n",
    "                        output_data={\"website_url\": website_url}\n",
    "                    )\n",
    "                    \n",
    "                    return website_url\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"get_website_url\",\n",
    "                error=e,\n",
    "                context={\"company_name\": company_name}\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    async def extract_page_content(self, url: str) -> str:\n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"start_page_extraction\",\n",
    "                input_data={\"url\": url},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            if not self.browser:\n",
    "                await self.initialize()\n",
    "\n",
    "            page = await self.browser.new_page()\n",
    "\n",
    "            try:\n",
    "                await page.goto(\n",
    "                    url,\n",
    "                    wait_until='domcontentloaded',\n",
    "                    timeout=10000\n",
    "                )\n",
    "            except TimeoutError:\n",
    "                self.logger.log_step(\n",
    "                    agent_name=\"WebScraper\",\n",
    "                    action=\"page_load_timeout\",\n",
    "                    error=\"Page load timed out\",\n",
    "                    context={\"url\": url}\n",
    "                )\n",
    "                raise\n",
    "        \n",
    "            content = await page.content()\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "\n",
    "            text = soup.get_text()\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"complete_page_extraction\",\n",
    "                input_data={\"url\": url},\n",
    "                output_data={\n",
    "                    \"content_length\": len(text),\n",
    "                    \"content_preview\": text[:50]\n",
    "                }\n",
    "            )\n",
    "\n",
    "            return text\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"extract_page_content\",\n",
    "                error=e,\n",
    "                context={\"url\": url}\n",
    "            )\n",
    "            raise\n",
    "    \n",
    "    async def process_pages_concurrently(self, urls: List[str], max_concurrent: int = 5) -> Dict[str, str]:\n",
    "        \"\"\"Process multiple pages concurrently with rate limiting\"\"\"\n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"start_concurrent_extraction\",\n",
    "                input_data={\"urls\": urls, \"max_concurrent\": max_concurrent},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            if not self.browser:\n",
    "                await self.initialize()\n",
    "\n",
    "            async def process_single_url(url: str) -> Tuple[str, str]:\n",
    "                try:\n",
    "                    content = await self.extract_page_content(url)\n",
    "                    return url, content\n",
    "                except Exception as e:\n",
    "                    self.logger.log_error(\n",
    "                        agent_name=\"WebScraper\",\n",
    "                        action=\"process_single_url\",\n",
    "                        error=e,\n",
    "                        context={\"url\": url}\n",
    "                    )\n",
    "                    return url, \"\"\n",
    "\n",
    "            # Process URLs in chunks to limit concurrent operations\n",
    "            results = {}\n",
    "            for i in range(0, len(urls), max_concurrent):\n",
    "                chunk = urls[i:i + max_concurrent]\n",
    "                chunk_tasks = [process_single_url(url) for url in chunk]\n",
    "                chunk_results = await asyncio.gather(*chunk_tasks, return_exceptions=True)\n",
    "                \n",
    "                for url, content in chunk_results:\n",
    "                    if content:  # Only store successful results\n",
    "                        results[url] = content\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"process_pages_concurrently\",\n",
    "                error=e,\n",
    "                context={\"urls\": urls}\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    async def search_company_page(self, company_name: str, page_type: str) -> str:\n",
    "        \"\"\"Search for a specific type of page for a company\"\"\"\n",
    "        try:\n",
    "            search_terms = {\n",
    "                'pricing': ['pricing', 'plans', 'packages'],\n",
    "                'features': ['features', 'product features'],\n",
    "                'products': ['products', 'solutions'],\n",
    "                'about': ['about', 'company information'],\n",
    "            }\n",
    "            \n",
    "            search_term = search_terms.get(page_type, [page_type])[0]\n",
    "            query = f\"{company_name} {search_term}\"\n",
    "            \n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"search_company_page\",\n",
    "                input_data={\"company_name\": company_name, \"page_type\": page_type},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            headers = {'Ocp-Apim-Subscription-Key': self.subscription_key}\n",
    "            params = {\n",
    "                'q': query,\n",
    "                'count': 1\n",
    "            }\n",
    "            \n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(self.endpoint, headers=headers, params=params) as response:\n",
    "                    data = await response.json()\n",
    "                    if 'webPages' in data and data['webPages']['value']:\n",
    "                        page_url = data['webPages']['value'][0]['url']\n",
    "                        \n",
    "                        self.logger.log_step(\n",
    "                            agent_name=\"WebScraper\",\n",
    "                            action=\"found_page_url\",\n",
    "                            input_data={\"query\": query},\n",
    "                            output_data={\"page_url\": page_url}\n",
    "                        )\n",
    "                        \n",
    "                        return page_url\n",
    "                    return None\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"search_company_page\",\n",
    "                error=e,\n",
    "                context={\"company_name\": company_name, \"page_type\": page_type}\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    async def analyze_company(self, company_name: str) -> Dict:\n",
    "      try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"start_company_analysis\",\n",
    "                input_data={\"company_name\": company_name},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            website_url = await self.get_website_url(company_name)\n",
    "            page_types = ['pricing', 'features', 'products', 'about']\n",
    "\n",
    "            urls_to_process = [website_url]\n",
    "            page_urls = {}\n",
    "\n",
    "            for page_type in page_types:\n",
    "                try:\n",
    "                    page_url = await self.search_company_page(company_name, page_type)\n",
    "                    if page_url:\n",
    "                        urls_to_process.append(page_url)\n",
    "                        page_urls[page_type] = page_url\n",
    "                except Exception as e:\n",
    "                    self.logger.log_error(\n",
    "                        agent_name=\"WebScraper\",\n",
    "                        action=\"get_page_url\",\n",
    "                        error=e,\n",
    "                        context={\"company_name\": company_name, \"page_type\": page_type}\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "            all_content = await self.process_pages_concurrently(urls_to_process)\n",
    "\n",
    "            company_data = {\n",
    "                \"name\": company_name,\n",
    "                \"website\": website_url,\n",
    "                \"pages\": {\n",
    "                    \"home\": {\n",
    "                        \"url\": website_url,\n",
    "                        \"content\": all_content.get(website_url, \"\")\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            for page_type, url in page_urls.items():\n",
    "                company_data[\"pages\"][page_type] = {\n",
    "                    \"url\": url,\n",
    "                    \"content\": all_content.get(url, \"\")\n",
    "                }\n",
    "\n",
    "            return company_data\n",
    "\n",
    "      except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"analyze_company\",\n",
    "                error=e,\n",
    "                context={\"company_name\": company_name}\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    async def analyze_competitors(self, competitors_data: Dict) -> Dict:\n",
    "        print(type(competitors_data))\n",
    "        if isinstance(competitors_data, str):\n",
    "            competitors_data = json.loads(competitors_data)\n",
    "            \n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"start_competitors_analysis\",\n",
    "                input_data={\"competitors\": [comp[\"name\"] for comp in competitors_data[\"competitors\"]]},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            results = {}\n",
    "            for competitor in competitors_data[\"competitors\"]:\n",
    "                try:\n",
    "                    results[competitor[\"name\"]] = await self.analyze_company(competitor[\"name\"])\n",
    "                except Exception as e:\n",
    "                    self.logger.log_error(\n",
    "                        agent_name=\"WebScraper\",\n",
    "                        action=\"analyze_competitor\",\n",
    "                        error=e,\n",
    "                        context={\"competitor\": competitor}\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"complete_competitors_analysis\",\n",
    "                input_data={\"competitors\": [comp[\"name\"] for comp in competitors_data[\"competitors\"]]},\n",
    "                output_data={\n",
    "                    \"competitors_analyzed\": list(results.keys()),\n",
    "                    \"total_competitors\": len(competitors_data[\"competitors\"]),\n",
    "                    \"successful_analyses\": len(results)\n",
    "                }\n",
    "            )\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"analyze_competitors\",\n",
    "                error=e,\n",
    "                context={\"competitors_data\": competitors_data}\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    async def extract_structured_data(self, content: str, data_type: str) -> Dict:\n",
    "        \"\"\"Extract specific types of data from page content\"\"\"\n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"start_structured_data_extraction\",\n",
    "                input_data={\"data_type\": data_type},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            # Use Gemini to extract structured data\n",
    "            prompt = f\"\"\"\n",
    "            Extract the following type of information: {data_type}\n",
    "            From the following content:\n",
    "            # TODO: Remove content limit?\n",
    "            {content[:1000]}  # Limit content length for API\n",
    "            \n",
    "            Return the information in JSON format.\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.config.gemini_client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash-exp\",\n",
    "                contents=prompt,\n",
    "                config=genai.types.GenerateContentConfig(\n",
    "                  temperature= 0.1,\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            structured_data = json.loads(response.text)\n",
    "            \n",
    "            self.logger.log_step(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"complete_structured_data_extraction\",\n",
    "                input_data={\"data_type\": data_type},\n",
    "                output_data=structured_data\n",
    "            )\n",
    "            \n",
    "            return structured_data\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"WebScraper\",\n",
    "                action=\"extract_structured_data\",\n",
    "                error=e,\n",
    "                context={\"data_type\": data_type}\n",
    "            )\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.system_prompt = \"\"\n",
    "        self.role = \"\"\n",
    "        self.goal = \"\"\n",
    "        self.backstory = \"\"\n",
    "        self.temperature = 1.0\n",
    "\n",
    "    async def execute(self, input_data: Any) -> Any:\n",
    "        await self.config.rate_limiter.wait_if_needed()\n",
    "        \n",
    "        try:\n",
    "            # Log the start of execution\n",
    "            self.logger.log_step(\n",
    "                agent_name=self.__class__.__name__,\n",
    "                action=\"start_execution\",\n",
    "                input_data=input_data,\n",
    "                output_data=None\n",
    "            )\n",
    "            \n",
    "            prompt = f\"\"\"\n",
    "            Role: {self.role}\n",
    "            Goal: {self.goal}\n",
    "            Backstory: {self.backstory}\n",
    "            System Instructions: {self.system_prompt}\n",
    "            \n",
    "            Input Data:\n",
    "            {json.dumps(input_data, indent=2)}\n",
    "            \n",
    "            Please provide your analysis based on the above information.\n",
    "            \"\"\"\n",
    "\n",
    "            response = self.config.gemini_client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash-exp\",\n",
    "                contents=prompt,\n",
    "                config=genai.types.GenerateContentConfig(\n",
    "                  temperature=self.temperature,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            self.logger.log_step(\n",
    "                agent_name=self.__class__.__name__,\n",
    "                action=\"loading_response_into_json\",\n",
    "                input_data=input_data,\n",
    "                output_data=response.text\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "              result = json.loads(response.text.replace('```json', '').replace('```', '').strip())\n",
    "            except Exception as e:\n",
    "              result = response.text\n",
    "              self.logger.log_error(\n",
    "                  agent_name=self.__class__.__name__,\n",
    "                  action=\"loading_response_into_json\",\n",
    "                  error=e,\n",
    "                  context={\"response\": result}\n",
    "            )\n",
    "\n",
    "            # Log the successful execution\n",
    "            self.logger.log_step(\n",
    "                agent_name=self.__class__.__name__,\n",
    "                action=\"complete_execution\",\n",
    "                input_data=input_data,\n",
    "                output_data=result\n",
    "            )\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Log the error\n",
    "            self.logger.log_error(\n",
    "                agent_name=self.__class__.__name__,\n",
    "                action=\"execute\",\n",
    "                error=e,\n",
    "                context={\"input_data\": input_data}\n",
    "            )\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketIntelligenceScout(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Expert market researcher specializing in competitor identification\"\n",
    "        self.goal = \"Identify and categorize the most relevant competitors\"\n",
    "        self.backstory = \"Former market research director with 15 years of experience\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        1. Search for top competitors in the given market\n",
    "        2. Return results in JSON format with the following structure:\n",
    "        {\n",
    "            \"competitors\": [\n",
    "                {\n",
    "                    \"name\": \"\",\n",
    "                    \"website\": \"\",\n",
    "                    \"industry\": \"\",\n",
    "                    \"threat_level\": \"\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitalProductAnalyst(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Product analysis specialist\"\n",
    "        self.goal = \"Analyze product features and capabilities\"\n",
    "        self.backstory = \"Previously a product manager at major tech companies\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Analyze each competitor's product features and capabilities.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"competitor_name\": {\n",
    "                \"key_features\": [],\n",
    "                \"unique_capabilities\": [],\n",
    "                \"user_experience\": \"\",\n",
    "                \"product_maturity\": \"\"\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketingMessageDecoder(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Marketing communications analyst\"\n",
    "        self.goal = \"Decode and analyze competitors' marketing strategies\"\n",
    "        self.backstory = \"Former copywriter turned marketing strategist\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Analyze marketing messages and positioning.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"competitor_name\": {\n",
    "                \"value_propositions\": [],\n",
    "                \"messaging_tone\": \"\",\n",
    "                \"target_audience\": \"\",\n",
    "                \"unique_selling_points\": []\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechnicalFeatureComparator(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Technical analyst specializing in feature comparison\"\n",
    "        self.goal = \"Provide detailed technical comparison of competitor products\"\n",
    "        self.backstory = \"Senior solutions architect with cross-industry experience\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Compare technical features across competitors.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"competitor_name\": {\n",
    "                \"tech_stack\": [],\n",
    "                \"api_capabilities\": [],\n",
    "                \"scalability_features\": [],\n",
    "                \"technical_advantages\": [],\n",
    "                \"technical_limitations\": []\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PricingStrategySpecialist(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Pricing analysis expert\"\n",
    "        self.goal = \"Analyze and compare pricing models and strategies\"\n",
    "        self.backstory = \"Former pricing consultant in SaaS industry\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Analyze pricing strategies and models.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"competitor_name\": {\n",
    "                \"pricing_tiers\": [],\n",
    "                \"pricing_model\": \"\",\n",
    "                \"discount_strategies\": [],\n",
    "                \"pricing_positioning\": \"\"\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetitiveStrategyAnalyst(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Strategic analyst specializing in competitive analysis\"\n",
    "        self.goal = \"Synthesize competitive intelligence into strategic insights\"\n",
    "        self.backstory = \"Strategy consultant from major consulting firms\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Synthesize all competitive data into strategic insights.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"market_patterns\": [],\n",
    "            \"competitive_advantages\": {},\n",
    "            \"market_gaps\": [],\n",
    "            \"strategic_recommendations\": [],\n",
    "            \"threat_assessment\": {}\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.temperature = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetitiveIntelligenceReportSpecialist(BaseAgent):\n",
    "    def __init__(self, config: Config, logger: ResearchLogger):\n",
    "        super().__init__(config, logger)\n",
    "        self.role = \"Report creation specialist\"\n",
    "        self.goal = \"Create clear, actionable reports from competitive analysis\"\n",
    "        self.backstory = \"Communications expert in data visualization\"\n",
    "        self.system_prompt = \"\"\"\n",
    "        Create a comprehensive report from all analyses.\n",
    "        Return results in JSON format:\n",
    "        {\n",
    "            \"executive_summary\": \"\",\n",
    "            \"key_findings\": [],\n",
    "            \"detailed_analysis\": {},\n",
    "            \"recommendations\": [],\n",
    "            \"market_overview\": \"\",\n",
    "            \"appendix\": {}\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.temperature = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetitiveAnalysisWorkflow:\n",
    "    def __init__(self, config: Config, research_id: str):\n",
    "        self.config = config\n",
    "        self.logger = ResearchLogger(research_id)\n",
    "\n",
    "        self.market_scout = MarketIntelligenceScout(config, self.logger)\n",
    "        self.product_analyst = DigitalProductAnalyst(config, self.logger)\n",
    "        self.marketing_decoder = MarketingMessageDecoder(config, self.logger)\n",
    "        self.technical_comparator = TechnicalFeatureComparator(config, self.logger)\n",
    "        self.pricing_specialist = PricingStrategySpecialist(config, self.logger)\n",
    "        self.competitive_analyst = CompetitiveStrategyAnalyst(config, self.logger)\n",
    "        self.report_specialist = CompetitiveIntelligenceReportSpecialist(config, self.logger)\n",
    "        self.web_scraper = WebScraper(config, self.logger)\n",
    "\n",
    "    async def run_parallel_analysis(self, competitors_data: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Run parallel analysis tasks\"\"\"\n",
    "        tasks = [\n",
    "            self.product_analyst.execute(competitors_data),\n",
    "            self.marketing_decoder.execute(competitors_data),\n",
    "            self.technical_comparator.execute(competitors_data),\n",
    "            self.pricing_specialist.execute(competitors_data)\n",
    "        ]\n",
    "        \n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        return {\n",
    "            \"product_analysis\": results[0],\n",
    "            \"marketing_analysis\": results[1],\n",
    "            \"technical_analysis\": results[2],\n",
    "            \"pricing_analysis\": results[3]\n",
    "        }\n",
    "\n",
    "    async def execute_workflow(self, industry: str, target_company: str = None) -> Dict:\n",
    "        try:\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"Workflow\",\n",
    "                action=\"start_workflow\",\n",
    "                input_data={\"industry\": industry, \"target_company\": target_company},\n",
    "                output_data=None\n",
    "            )\n",
    "            \n",
    "            # Step 1: Retrieve target company information (if necessar;)\n",
    "            target_company_data = None\n",
    "            if target_company:\n",
    "                target_company_data = await self.web_scraper.analyze_company(target_company)\n",
    "\n",
    "            # Step 2: Identify competitors\n",
    "            competitors_data = await self.market_scout.execute({\n",
    "                \"industry\": industry,\n",
    "                \"target_company\": target_company,\n",
    "                \"target_company_data\": target_company_data\n",
    "            })\n",
    "\n",
    "            # Step 3: Gather website information for all competitors\n",
    "            website_data = await self.web_scraper.analyze_competitors(competitors_data)\n",
    "\n",
    "            # Step 4: Run parallel analysis with website data\n",
    "            parallel_results = await self.run_parallel_analysis({\n",
    "                \"competitors_data\": competitors_data,\n",
    "                \"website_data\": website_data,\n",
    "                \"target_company_data\": target_company_data\n",
    "            })\n",
    "\n",
    "            # Step 3: Strategic analysis\n",
    "            strategic_analysis = await self.competitive_analyst.execute({\n",
    "                \"competitors_data\": competitors_data,\n",
    "                \"parallel_results\": parallel_results\n",
    "            })\n",
    "\n",
    "            # Step 4: Generate report\n",
    "            final_report = await self.report_specialist.execute({\n",
    "                \"strategic_analysis\": strategic_analysis,\n",
    "                \"raw_data\": {\n",
    "                    \"competitors\": competitors_data,\n",
    "                    \"analysis\": parallel_results\n",
    "                }\n",
    "            })\n",
    "\n",
    "            self.logger.log_step(\n",
    "                agent_name=\"Workflow\",\n",
    "                action=\"complete_workflow\",\n",
    "                input_data={\"industry\": industry, \"target_company\": target_company},\n",
    "                output_data=None\n",
    "            )\n",
    "\n",
    "            return final_report\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_error(\n",
    "                agent_name=\"Workflow\",\n",
    "                action=\"execute_workflow\",\n",
    "                error=e,\n",
    "                context={\n",
    "                    \"industry\": industry,\n",
    "                    \"target_company\": target_company\n",
    "                }\n",
    "            )\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_workflow():\n",
    "    config = Config()\n",
    "    research_id = f\"research_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    workflow = CompetitiveAnalysisWorkflow(config, research_id)\n",
    "    \n",
    "    industry = \"AI development platforms\"\n",
    "    target_company = \"OpenAI\"  # Optional, can be None\n",
    "    \n",
    "    result = await workflow.execute_workflow(\n",
    "        industry=industry,\n",
    "        target_company=target_company\n",
    "    )\n",
    "    \n",
    "    # Print research summary\n",
    "    print(\"\\nResearch Summary:\")\n",
    "    print(json.dumps(workflow.logger.get_research_summary(), indent=2))\n",
    "    \n",
    "    print(\"\\nFinal Result:\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Agent: Workflow | Action: start_workflow\n",
      "INFO:__main__:Agent: WebScraper | Action: start_company_analysis\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_website\n",
      "INFO:__main__:Agent: WebScraper | Action: found_website_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: start_concurrent_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: MarketIntelligenceScout | Action: start_execution\n",
      "INFO:root:AFC is enabled with max remote calls: 10.\n",
      "INFO:__main__:Agent: MarketIntelligenceScout | Action: loading_response_into_json\n",
      "INFO:__main__:Agent: MarketIntelligenceScout | Action: complete_execution\n",
      "INFO:__main__:Agent: WebScraper | Action: start_competitors_analysis\n",
      "INFO:__main__:Agent: WebScraper | Action: start_company_analysis\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_website\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Agent: WebScraper | Action: found_website_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: start_concurrent_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "ERROR:__main__:Error in WebScraper during extract_page_content: Page.goto: Target page, context or browser has been closed\n",
      "ERROR:__main__:Context: {\n",
      "  \"url\": \"https://cloud.google.com/products/ai\"\n",
      "}\n",
      "ERROR:__main__:Error in WebScraper during process_single_url: Page.goto: Target page, context or browser has been closed\n",
      "ERROR:__main__:Context: {\n",
      "  \"url\": \"https://cloud.google.com/products/ai\"\n",
      "}\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_company_analysis\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_website\n",
      "INFO:__main__:Agent: WebScraper | Action: found_website_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: start_concurrent_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: complete_page_extraction\n",
      "INFO:__main__:Agent: WebScraper | Action: start_company_analysis\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_website\n",
      "INFO:__main__:Agent: WebScraper | Action: found_website_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n",
      "INFO:__main__:Agent: WebScraper | Action: found_page_url\n",
      "INFO:__main__:Agent: WebScraper | Action: search_company_page\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m test_workflow()\n",
      "Cell \u001b[0;32mIn[34], line 10\u001b[0m, in \u001b[0;36mtest_workflow\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m industry \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI development platforms\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m target_company \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Optional, can be None\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m workflow\u001b[38;5;241m.\u001b[39mexecute_workflow(\n\u001b[1;32m     11\u001b[0m     industry\u001b[38;5;241m=\u001b[39mindustry,\n\u001b[1;32m     12\u001b[0m     target_company\u001b[38;5;241m=\u001b[39mtarget_company\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Print research summary\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResearch Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[33], line 55\u001b[0m, in \u001b[0;36mCompetitiveAnalysisWorkflow.execute_workflow\u001b[0;34m(self, industry, target_company)\u001b[0m\n\u001b[1;32m     48\u001b[0m competitors_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmarket_scout\u001b[38;5;241m.\u001b[39mexecute({\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindustry\u001b[39m\u001b[38;5;124m\"\u001b[39m: industry,\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_company\u001b[39m\u001b[38;5;124m\"\u001b[39m: target_company,\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_company_data\u001b[39m\u001b[38;5;124m\"\u001b[39m: target_company_data\n\u001b[1;32m     52\u001b[0m })\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Step 3: Gather website information for all competitors\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m website_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweb_scraper\u001b[38;5;241m.\u001b[39manalyze_competitors(competitors_data)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Step 4: Run parallel analysis with website data\u001b[39;00m\n\u001b[1;32m     58\u001b[0m parallel_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_parallel_analysis({\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompetitors_data\u001b[39m\u001b[38;5;124m\"\u001b[39m: competitors_data,\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwebsite_data\u001b[39m\u001b[38;5;124m\"\u001b[39m: website_data,\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_company_data\u001b[39m\u001b[38;5;124m\"\u001b[39m: target_company_data\n\u001b[1;32m     62\u001b[0m })\n",
      "Cell \u001b[0;32mIn[24], line 294\u001b[0m, in \u001b[0;36mWebScraper.analyze_competitors\u001b[0;34m(self, competitors_data)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m competitor \u001b[38;5;129;01min\u001b[39;00m competitors_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompetitors\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 294\u001b[0m         results[competitor[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyze_company(competitor[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_error(\n\u001b[1;32m    297\u001b[0m             agent_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWebScraper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    298\u001b[0m             action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalyze_competitor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    299\u001b[0m             error\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m    300\u001b[0m             context\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompetitor\u001b[39m\u001b[38;5;124m\"\u001b[39m: competitor}\n\u001b[1;32m    301\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[24], line 235\u001b[0m, in \u001b[0;36mWebScraper.analyze_company\u001b[0;34m(self, company_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page_type \u001b[38;5;129;01min\u001b[39;00m page_types:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m         page_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_company_page(company_name, page_type)\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m page_url:\n\u001b[1;32m    237\u001b[0m             urls_to_process\u001b[38;5;241m.\u001b[39mappend(page_url)\n",
      "Cell \u001b[0;32mIn[24], line 194\u001b[0m, in \u001b[0;36mWebScraper.search_company_page\u001b[0;34m(self, company_name, page_type)\u001b[0m\n\u001b[1;32m    188\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m: query,\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    191\u001b[0m }\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aiohttp\u001b[38;5;241m.\u001b[39mClientSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint, headers\u001b[38;5;241m=\u001b[39mheaders, params\u001b[38;5;241m=\u001b[39mparams) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m    195\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwebPages\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwebPages\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/agent/lib/python3.13/site-packages/aiohttp/client.py:1425\u001b[0m, in \u001b[0;36m_BaseRequestContextManager.__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _RetType:\n\u001b[0;32m-> 1425\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp: _RetType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coro\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aenter__\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/agent/lib/python3.13/site-packages/aiohttp/client.py:730\u001b[0m, in \u001b[0;36mClientSession._request\u001b[0;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, server_hostname, proxy_headers, trace_request_ctx, read_bufsize, auto_decompress, max_line_size, max_field_size)\u001b[0m\n\u001b[1;32m    728\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m req\u001b[38;5;241m.\u001b[39msend(conn)\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 730\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstart(conn)\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     resp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/agent/lib/python3.13/site-packages/aiohttp/client_reqrep.py:1059\u001b[0m, in \u001b[0;36mClientResponse.start\u001b[0;34m(self, connection)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1058\u001b[0m     protocol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\n\u001b[0;32m-> 1059\u001b[0m     message, payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mread()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m http\u001b[38;5;241m.\u001b[39mHttpProcessingError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[1;32m   1062\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_info,\n\u001b[1;32m   1063\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         headers\u001b[38;5;241m=\u001b[39mexc\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1067\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/agent/lib/python3.13/site-packages/aiohttp/streams.py:671\u001b[0m, in \u001b[0;36mDataQueue.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mcreate_future()\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 671\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (asyncio\u001b[38;5;241m.\u001b[39mCancelledError, asyncio\u001b[38;5;241m.\u001b[39mTimeoutError):\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = await test_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_to_markdown(data, output_file=\"output.md\"):\n",
    "    # Convert JSON to Markdown\n",
    "    def json_to_markdown(data, level=0):\n",
    "        indent = \"  \" * level\n",
    "        lines = []\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                lines.append(f\"{indent}**{key.replace('_', ' ').title()}:**\")\n",
    "                lines.extend(json_to_markdown(value, level + 1))\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                lines.extend(json_to_markdown(item, level + 1))\n",
    "        else:\n",
    "            lines.append(f\"{indent}- {data}\")\n",
    "        return lines\n",
    "\n",
    "    markdown_output = \"\\n\".join(json_to_markdown(data))\n",
    "\n",
    "    # Save to file\n",
    "    try:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(markdown_output)\n",
    "    except Exception as e:\n",
    "        return f\"Error: Could not save to file: {e}\"\n",
    "\n",
    "    return markdown_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "report_name = f\"competitive_analysis_report_{current_timestamp}.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- ```json\\n{\\n  \"executive_summary\": \"The AI development platform market is highly competitive, with major tech companies like Google, Microsoft, and Amazon leading the charge, alongside emerging players such as Hugging Face, Cohere, and AI21 Labs. The market is characterized by a trend towards comprehensive AI service suites, open-source collaboration, and a focus on enterprise-grade security and scalability. Pricing models vary, and there are gaps in simplified pricing, ease of use for non-technical users, and specialized industry solutions. Strategic recommendations include focusing on specific market segments, simplifying pricing, improving user experience, and prioritizing data privacy and security.\",\\n  \"key_findings\": [\\n    \"The AI development platform market is dominated by major tech companies (Google, Microsoft, Amazon) and emerging players (Hugging Face, Cohere, AI21 Labs).\",\\n    \"There is a clear trend towards offering a comprehensive suite of AI services, including foundation models, APIs for various modalities, and tools for the entire ML lifecycle.\",\\n    \"Open-source and community collaboration are significant forces, particularly with Hugging Face\\'s platform.\",\\n    \"Enterprise-grade security, scalability, and compliance are crucial for attracting business users, with Microsoft, Cohere, and AI21 Labs emphasizing these aspects.\",\\n    \"The market is segmented by user needs, with some platforms focusing on developers and researchers (Hugging Face, Google AI) and others targeting enterprises (Microsoft Azure AI, Cohere, AI21 Labs, IBM Watson).\",\\n    \"Pricing models vary, with a mix of free tiers for experimentation, pay-as-you-go for API access, and subscription-based models for advanced features and enterprise solutions.\",\\n    \"There are market gaps in simplified pricing models, ease of use for non-technical users, specialized solutions for specific industries, stronger focus on data privacy and security, and improved explainability and transparency.\",\\n    \"Google AI, Microsoft Azure AI, and Amazon Machine Learning pose the highest competitive threat due to their strong infrastructure, wide range of services, and deep ecosystem integrations.\"\\n  ],\\n  \"detailed_analysis\": {\\n    \"market_patterns\": {\\n      \"description\": \"The AI development platform market is characterized by intense competition, a move towards comprehensive AI service suites, the influence of open-source collaboration, and a strong emphasis on enterprise-grade security and scalability. The market is segmented by user needs and pricing models vary.\",\\n      \"details\": [\\n        \"The AI development platform market is highly competitive, dominated by major tech companies (Google, Microsoft, Amazon) and emerging players (Hugging Face, Cohere, AI21 Labs).\",\\n        \"There\\'s a clear trend towards offering a comprehensive suite of AI services, including foundation models, APIs for various modalities, and tools for the entire ML lifecycle.\",\\n        \"Open-source and community collaboration are significant forces, particularly with Hugging Face\\'s platform.\",\\n        \"Enterprise-grade security, scalability, and compliance are crucial for attracting business users, with Microsoft, Cohere, and AI21 Labs emphasizing these aspects.\",\\n        \"The market is segmented by user needs, with some platforms focusing on developers and researchers (Hugging Face, Google AI) and others targeting enterprises (Microsoft Azure AI, Cohere, AI21 Labs, IBM Watson).\",\\n        \"Pricing models vary, with a mix of free tiers for experimentation, pay-as-you-go for API access, and subscription-based models for advanced features and enterprise solutions.\"\\n      ]\\n    },\\n    \"competitive_advantages\": {\\n      \"description\": \"Each competitor has unique strengths, ranging from deep ecosystem integration to open-source collaboration and enterprise-grade security.\",\\n      \"competitors\": {\\n        \"Google AI\": [\\n          \"Deep integration with Google ecosystem (Android, Chrome, Cloud)\",\\n          \"TPU infrastructure for AI acceleration\",\\n          \"Gemini models with large context windows (up to 2 million tokens)\",\\n          \"Free tier for testing and Google AI Studio usage\",\\n          \"Wide range of AI products and services across different modalities\"\\n        ],\\n        \"Microsoft Azure AI\": [\\n          \"Strong focus on enterprise-grade security, scalability, and compliance\",\\n          \"Integration with Microsoft ecosystem (GitHub, Visual Studio, Azure)\",\\n          \"Azure AI Model Catalog with diverse model options\",\\n          \"Responsible AI tools and guidelines\",\\n          \"Support for various deployment options (SaaS, cloud providers, VPC, on-premises)\",\\n          \"Partnerships with other AI model providers (OpenAI, Hugging Face, etc.)\"\\n        ],\\n        \"Amazon Machine Learning\": [\\n          \"Comprehensive set of ML services and infrastructure\",\\n          \"Wide range of instance types optimized for different AI workloads\",\\n          \"Strong support for open-source frameworks\",\\n          \"Focus on scalability and cost-effectiveness\",\\n          \"AWS DeepRacer League for hands-on learning\",\\n          \"Integration with other AWS services\"\\n        ],\\n        \"Hugging Face\": [\\n          \"Strong focus on open-source and community collaboration\",\\n          \"Large collection of pre-trained models and datasets\",\\n          \"Easy-to-use platform for sharing and deploying ML models\",\\n          \"Support for various modalities (text, image, video, audio, 3D)\",\\n          \"Free tier for public models, datasets, and applications\"\\n        ],\\n        \"Cohere\": [\\n          \"Focus on enterprise-grade security and data protection\",\\n          \"Multilingual models\",\\n          \"Advanced retrieval capabilities\",\\n          \"AI workspace tailored for enterprises\",\\n          \"Customizable AI solutions\",\\n          \"Private deployment options\"\\n        ],\\n        \"AI21 Labs\": [\\n          \"Production-grade Mamba-based model (Jamba)\",\\n          \"RAG as a service (RAGaaS)\",\\n          \"Focus on human-centered AI\",\\n          \"Emphasis on practical and scalable solutions\",\\n          \"Turnkey tools and rigorous security protocols\",\\n          \"Hands-on deployment expertise\"\\n        ],\\n        \"IBM Watson\": [\\n          \"Long history in AI research and development\",\\n          \"Strong focus on enterprise solutions\",\\n          \"watsonx platform for generative AI and machine learning\",\\n          \"Integration with IBM Cloud and other IBM products\",\\n          \"Emphasis on responsible, transparent, and explainable AI\",\\n          \"Wide range of AI applications across different industries\"\\n        ]\\n      }\\n    },\\n    \"market_gaps\": {\\n      \"description\": \"Several gaps exist in the market, including pricing complexity, lack of user-friendliness for non-technical users, and a need for more specialized industry solutions.\",\\n      \"gaps\": [\\n        \"Simplified pricing models: Many platforms have complex pricing structures, which can be a barrier for some users. There\\'s an opportunity for a platform to offer more transparent and predictable pricing.\",\\n        \"Ease of use for non-technical users: While many platforms cater to developers and data scientists, there\\'s a gap in providing user-friendly tools for business users who may not have technical expertise.\",\\n        \"Specialized solutions for specific industries: While some platforms offer solutions for certain industries, there\\'s room for more specialized AI solutions tailored to the unique needs of various sectors.\",\\n        \"Stronger focus on data privacy and security: While some platforms emphasize security, there\\'s a growing need for even stronger data privacy and security features, especially for sensitive data.\",\\n        \"Improved explainability and transparency: Many AI models are still black boxes, and there\\'s a need for improved explainability and transparency to build trust and ensure responsible AI development.\"\\n      ]\\n    },\\n    \"threat_assessment\": {\\n      \"description\": \"The competitive threat landscape is categorized into high and medium threats based on the strength and market position of each competitor.\",\\n      \"high_threats\": [\\n        \"Google AI: Its deep integration with Google\\'s ecosystem, strong infrastructure, and wide range of AI services pose a significant threat to competitors.\",\\n        \"Microsoft Azure AI: Its strong focus on enterprise-grade security, scalability, and integration with the Microsoft ecosystem makes it a formidable competitor.\",\\n        \"Amazon Machine Learning: Its comprehensive set of ML services, infrastructure, and support for open-source frameworks make it a major player in the market.\"\\n      ],\\n      \"medium_threats\": [\\n        \"Hugging Face: Its strong focus on open-source and community collaboration poses a threat to proprietary platforms.\",\\n        \"Cohere: Its focus on enterprise-grade security, advanced retrieval, and private deployment options makes it a strong competitor in the enterprise market.\",\\n        \"AI21 Labs: Its production-grade models, built-in RAG, and focus on reliable generative AI make it a rising threat.\",\\n        \"IBM Watson: Its long history in AI research and development and focus on enterprise solutions makes it a consistent player in the market.\"\\n      ]\\n    },\\n    \"product_analysis\": {\\n      \"description\": \"Detailed analysis of each competitor\\'s product offerings, including key features, unique capabilities, user experience, and product maturity.\",\\n      \"competitors\": {\\n        \"Google AI\": {\\n          \"key_features\": [\\n            \"Gemini API access (various models: 1.5 Flash, 1.5 Pro, 1.0 Pro, Text Embedding 004)\",\\n            \"Google AI Studio for prototyping and testing\",\\n            \"Gemma open models (multi-framework with Keras, fine-tuning in Colab)\",\\n            \"On-device AI (Gemini Nano on Android, Chrome built-in web APIs)\",\\n            \"Responsible GenAI Toolkit, Secure AI Framework\",\\n            \"Code assistance (Android Studio, Chrome DevTools, Colab, Firebase, Google Cloud, JetBrains, Jules, Project IDX, VS Code)\",\\n            \"Vertex AI platform for ML model lifecycle\",\\n            \"AutoML for custom model training\",\\n            \"Natural Language AI, Speech-to-Text, Text-to-Speech, Translation AI APIs\",\\n            \"Vision AI, Video AI APIs\",\\n            \"Document AI for data extraction\",\\n            \"Conversational Agents (Dialogflow)\",\\n            \"Customer Engagement Suite with Google AI\",\\n            \"Gemini Code Assist\",\\n            \"AI Infrastructure (TPUs, GPUs, CPUs)\",\\n            \"Google Kubernetes Engine (GKE)\"\\n          ],\\n          \"unique_capabilities\": [\\n            \"Deep integration with Google ecosystem (Android, Chrome, Cloud)\",\\n            \"TPU infrastructure for AI acceleration\",\\n            \"Gemini models with large context windows (up to 2 million tokens)\",\\n            \"Free tier for testing and Google AI Studio usage\",\\n            \"Grounding with Google Search (paid)\",\\n            \"Wide range of AI products and services across different modalities\"\\n          ],\\n          \"user_experience\": \"Offers a free tier for experimentation, with a focus on developer tools and integration with Google\\'s ecosystem. The pricing structure is complex with different rates for various models and context lengths. The platform seems to cater to both beginners and advanced users, with tools for prototyping and production.\",\\n          \"product_maturity\": \"Mature, with a wide range of products and services. Google AI has a long history in AI research and development, and its offerings are generally considered to be robust and reliable. The integration with Google Cloud and other Google products indicates a high level of product maturity.\"\\n        },\\n        \"Microsoft Azure AI\": {\\n          \"key_features\": [\\n            \"Azure OpenAI Service (access to OpenAI models)\",\\n            \"Azure AI Foundry for generative AI app development\",\\n            \"Azure AI Search for retrieval augmented generation (RAG)\",\\n            \"Azure AI Content Safety for content moderation\",\\n            \"Phi open models (small language models)\",\\n            \"Azure AI Model Catalog (Microsoft, OpenAI, Hugging Face, Meta, Cohere)\",\\n            \"Azure AI Document Intelligence, Speech, Vision, Language, Translator APIs\",\\n            \"Azure Machine Learning for end-to-end ML lifecycle\",\\n            \"Prompt flow for language model workflows\",\\n            \"Integration with GitHub and Visual Studio\",\\n            \"Azure Kubernetes Service (AKS), Azure Container Apps\",\\n            \"Azure Cosmos DB, Azure SQL Database, Azure App Service, Azure Functions\"\\n          ],\\n          \"unique_capabilities\": [\\n            \"Strong focus on enterprise-grade security, scalability, and compliance\",\\n            \"Integration with Microsoft ecosystem (GitHub, Visual Studio, Azure)\",\\n            \"Azure AI Model Catalog with diverse model options\",\\n            \"Responsible AI tools and guidelines\",\\n            \"Support for various deployment options (SaaS, cloud providers, VPC, on-premises)\",\\n            \"Partnerships with other AI model providers (OpenAI, Hugging Face, etc.)\"\\n          ],\\n          \"user_experience\": \"Focuses on enterprise users with a strong emphasis on security, scalability, and integration with Microsoft\\'s ecosystem. The platform offers a comprehensive set of tools for the entire AI lifecycle, from model selection to deployment. The pricing structure is complex, with different rates for various services and models. The platform is designed for developers and data scientists.\",\\n          \"product_maturity\": \"Mature, with a wide range of services and a strong focus on enterprise needs. Microsoft has invested heavily in AI and its Azure AI platform is considered to be a leading solution for businesses.\"\\n        },\\n        \"Amazon Machine Learning\": {\\n          \"key_features\": [\\n            \"Amazon SageMaker for building, training, and deploying ML models\",\\n            \"AWS Deep Learning AMIs and Containers\",\\n            \"Hugging Face, TensorFlow, PyTorch, Apache MXNet, Jupyter on SageMaker\",\\n            \"Amazon EC2 Trn1, P5, Inf2, G5 Instances for AI infrastructure\",\\n            \"Amazon SageMaker HyperPod for distributed training\",\\n            \"Responsible AI tools (Guardrails for Amazon Bedrock, SageMaker Clarify)\",\\n            \"AWS Solutions Library for AI use cases\",\\n            \"AWS DeepRacer League for ML skill development\",\\n            \"Amazon SageMaker Studio Lab for experimentation\"\\n          ],\\n          \"unique_capabilities\": [\\n            \"Comprehensive set of ML services and infrastructure\",\\n            \"Wide range of instance types optimized for different AI workloads\",\\n            \"Strong support for open-source frameworks\",\\n            \"Focus on scalability and cost-effectiveness\",\\n            \"AWS DeepRacer League for hands-on learning\",\\n            \"Integration with other AWS services\"\\n          ],\\n          \"user_experience\": \"Offers a wide range of services and tools for machine learning, with a focus on scalability and cost-effectiveness. The platform caters to both beginners and advanced users, with options for managed services and custom infrastructure. The pricing structure is complex, with different rates for various services and instance types. The platform is designed for developers and data scientists.\",\\n          \"product_maturity\": \"Mature, with a long history in cloud computing and machine learning. Amazon has a strong presence in the AI market and its offerings are generally considered to be robust and reliable.\"\\n        },\\n        \"Hugging Face\": {\\n          \"key_features\": [\\n            \"Hugging Face Hub for hosting and collaborating on models, datasets, and applications\",\\n            \"Transformers library for state-of-the-art ML\",\\n            \"Diffusers library for diffusion models\",\\n            \"Safetensors for storing neural network weights\",\\n            \"Hub Python Library, Tokenizers, PEFT, Transformers.js, timm, TRL, Datasets, Text Generation Inference, Accelerate\",\\n            \"Spaces for sharing ML applications and demos\",\\n            \"Inference Endpoints for deploying models\",\\n            \"Pro Account with advanced features\"\\n          ],\\n          \"unique_capabilities\": [\\n            \"Strong focus on open-source and community collaboration\",\\n            \"Large collection of pre-trained models and datasets\",\\n            \"Easy-to-use platform for sharing and deploying ML models\",\\n            \"Support for various modalities (text, image, video, audio, 3D)\",\\n            \"Free tier for public models, datasets, and applications\",\\n            \"Compute and Enterprise solutions for paid users\"\\n          ],\\n          \"user_experience\": \"Focuses on the open-source community, providing a platform for collaboration and sharing. The platform is easy to use and offers a wide range of tools for building and deploying ML models. The pricing structure is transparent, with different rates for various compute resources. The platform is designed for developers and researchers.\",\\n          \"product_maturity\": \"Relatively mature, with a strong community and a wide range of tools and resources. Hugging Face is a popular platform for AI research and development, and its offerings are generally considered to be innovative and cutting-edge.\"\\n        },\\n        \"Cohere\": {\\n          \"key_features\": [\\n            \"Command models for text generation and analysis\",\\n            \"Embed model for multimodal search and retrieval\",\\n            \"Rerank model for semantic search quality\",\\n            \"Fine-tuning capabilities\",\\n            \"Private and secure AI platform\",\\n            \"Deployment options (SaaS, cloud providers, VPC, on-premises)\",\\n            \"AI solutions for various industries\"\\n          ],\\n          \"unique_capabilities\": [\\n            \"Focus on enterprise-grade security and data protection\",\\n            \"Multilingual models\",\\n            \"Advanced retrieval capabilities\",\\n            \"AI workspace tailored for enterprises\",\\n            \"Customizable AI solutions\",\\n            \"Private deployment options\"\\n          ],\\n          \"user_experience\": \"Focuses on enterprise users with a strong emphasis on security and data privacy. The platform offers a range of models and tools for various AI tasks, with a focus on ease of use and seamless integration. The pricing structure is transparent, with different rates for various models and services. The platform is designed for businesses and developers.\",\\n          \"product_maturity\": \"Relatively mature, with a focus on enterprise solutions. Cohere is a growing company with a strong focus on research and development, and its offerings are generally considered to be innovative and reliable.\"\\n        },\\n        \"AI21 Labs\": {\\n          \"key_features\": [\\n            \"Jamba models (1.5 Mini, 1.5 Large)\",\\n            \"Task-Specific Models (Wordtune API, Contextual Answers API, Embeddings)\",\\n            \"Built-in RAG engine\",\\n            \"Enterprise-grade security\",\\n            \"Dedicated integration team\",\\n            \"Flexible deployment options (SaaS, APIs, partners, private VPC, on-premises)\"\\n          ],\\n          \"unique_capabilities\": [\\n            \"Production-grade Mamba-based model (Jamba)\",\\n            \"RAG as a service (RAGaaS)\",\\n            \"Focus on human-centered AI\",\\n            \"Emphasis on practical and scalable solutions\",\\n            \"Turnkey tools and rigorous security protocols\",\\n            \"Hands-on deployment expertise\"\\n          ],\\n          \"user_experience\": \"Focuses on enterprise users with a strong emphasis on reliability and scalability. The platform offers a range of models and tools for various AI tasks, with a focus on ease of use and seamless integration. The pricing structure is transparent, with different rates for various models and services. The platform is designed for businesses and developers.\",\\n          \"product_maturity\": \"Relatively mature, with a focus on enterprise solutions. AI21 Labs is a growing company with a strong focus on research and development, and its offerings are generally considered to be innovative and reliable.\"\\n        },\\n        \"IBM Watson\": {\\n          \"key_features\": [\\n            \"watsonx AI portfolio (watsonx.ai, watsonx.data, watsonx.governance)\",\\n            \"Watson Assistant for virtual agents\",\\n            \"Watson Orchestrate for workflow automation\",\\n            \"Watson Code Assistant for code generation\",\\n            \"IBM Watson Studio for data science\",\\n            \"Support for open-source frameworks\",\\n            \"Data preparation and visualization\",\\n            \"Model development and training\",\\n            \"End-to-end AI lifecycle management\",\\n            \"Integration with IBM Cloud and other cloud platforms\"\\n          ],\\n          \"unique_capabilities\": [\\n            \"Long history in AI research and development\",\\n            \"Strong focus on enterprise solutions\",\\n            \"watsonx platform for generative AI and machine learning\",\\n            \"Integration with IBM Cloud and other IBM products\",\\n            \"Emphasis on responsible, transparent, and explainable AI\",\\n            \"Wide range of AI applications across different industries\"\\n          ],\\n          \"user_experience\": \"Focuses on enterprise users with a strong emphasis on reliability and scalability. The platform offers a range of models and tools for various AI tasks, with a focus on ease of use and seamless integration. The pricing structure is complex, with different options for licensing and subscriptions. The platform is designed for businesses and developers.\",\\n          \"product_maturity\": \"Mature, with a long history in AI research and development. IBM has a strong presence in the AI market and its offerings are generally considered to be robust and reliable.\"\\n        }\\n      }\\n    },\\n     \"marketing_analysis\": {\\n        \"description\": \"Analysis of each competitor\\'s marketing strategies, including value propositions, messaging tone, target audience, and unique selling points.\",\\n        \"competitors\": {\\n          \"Google AI\": {\\n            \"value_propositions\": [\\n              \"Offers a variety of models (Gemini 1.5 Flash, Gemini 1.5 Pro, Gemini 1.0 Pro, Text Embedding 004) with different capabilities and pricing tiers.\",\\n              \"Provides a free tier for testing purposes and free usage of Google AI Studio.\",\\n              \"Offers pay-as-you-go billing for scaling AI services.\",\\n              \"Provides tools for customizing and fine-tuning models.\",\\n              \"Offers a comprehensive platform (Vertex AI) for building, training, and deploying ML models.\",\\n              \"Provides a wide range of AI products and services, including APIs for speech, text, language, vision, and document processing.\",\\n              \"Offers AI infrastructure with TPUs, GPUs, and CPUs.\",\\n              \"Provides consulting services through the AI Readiness Program.\"\\n            ],\\n            \"messaging_tone\": \"Professional, technical, and focused on capabilities and pricing. Emphasizes innovation, scalability, and responsibility.\",\\n            \"target_audience\": \"Developers, data scientists, and businesses looking to build and deploy AI applications.\",\\n            \"unique_selling_points\": [\\n              \"Wide range of models with varying context windows and capabilities.\",\\n              \"Free tier and free usage of Google AI Studio.\",\\n              \"Integration with Google Cloud Platform and other Google services.\",\\n              \"Strong focus on responsible AI and security.\",\\n              \"Comprehensive platform for the entire ML lifecycle.\"\\n            ]\\n          },\\n          \"Microsoft Azure AI\": {\\n            \"value_propositions\": [\\n              \"Offers a comprehensive platform (Azure AI Foundry) for building and deploying generative AI applications.\",\\n              \"Provides a curated selection of models for various modalities and budgets.\",\\n              \"Offers seamless customization and fine-tuning of models.\",\\n              \"Emphasizes trustworthy AI with safeguards for security, safety, and privacy.\",\\n              \"Provides a unified API for model selection and swapping.\",\\n              \"Offers enterprise-grade security and deployment strategies with Azure OpenAI Service.\",\\n              \"Provides state-of-the-art retrieval augmented generation (RAG) with Azure AI Search.\",\\n              \"Offers a variety of AI services, including Azure OpenAI Service, Azure AI Search, Azure AI Content Safety, Azure AI Document Intelligence, Azure AI Speech, Azure AI Vision, Azure AI Language, and Azure AI Translator.\",\\n              \"Offers a model catalog with foundation models from various providers.\",\\n              \"Provides tools for evaluating and scaling generative AI solutions.\",\\n              \"Offers integration with GitHub and Visual Studio.\"\\n            ],\\n            \"messaging_tone\": \"Professional, enterprise-focused, and emphasizes security, scalability, and flexibility.\",\\n            \"target_audience\": \"Enterprises and developers seeking to build and deploy AI applications with a focus on security and scalability.\",\\n            \"unique_selling_points\": [\\n              \"Azure AI Foundry as an all-in-one toolkit for building AI apps.\",\\n              \"Strong emphasis on trustworthy AI and built-in safety features.\",\\n              \"Seamless customization and fine-tuning of models.\",\\n              \"Integration with Microsoft ecosystem (GitHub, Visual Studio).\",\\n              \"Enterprise-grade security and deployment options.\"\\n            ]\\n          },\\n          \"Amazon Machine Learning\": {\\n            \"value_propositions\": [\\n              \"Offers a comprehensive set of ML services, infrastructure, and deployment resources.\",\\n              \"Provides Amazon SageMaker for building, training, and deploying ML models at scale.\",\\n              \"Offers AWS Deep Learning AMIs and Containers for quick deployment of deep learning environments.\",\\n              \"Supports various frameworks like Hugging Face, TensorFlow, PyTorch, Apache MXNet, and Jupyter.\",\\n              \"Provides high-performance AI infrastructure with EC2 Trn1, P5, Inf2, and G5 instances.\",\\n              \"Offers Amazon SageMaker HyperPod for distributed training at scale.\",\\n              \"Emphasizes responsible AI development with tools like Guardrails for Amazon Bedrock and Amazon SageMaker Clarify.\",\\n              \"Provides resources for developing ML skills, including AWS Solutions Library, AWS DeepRacer League, Amazon SageMaker Studio Lab, and ML tutorials.\"\\n            ],\\n            \"messaging_tone\": \"Technical, focused on scalability, performance, and a comprehensive set of tools and services. Emphasizes innovation and responsible AI.\",\\n            \"target_audience\": \"Enterprises and developers looking for scalable and high-performance ML solutions.\",\\n            \"unique_selling_points\": [\\n              \"Comprehensive set of ML services and infrastructure.\",\\n              \"Support for various ML frameworks.\",\\n              \"High-performance AI infrastructure with specialized EC2 instances.\",\\n              \"Focus on responsible AI development.\",\\n              \"Extensive resources for learning and skill development.\"\\n            ]\\n          },\\n          \"Hugging Face\": {\\n            \"value_propositions\": [\\n              \"Provides a platform for the machine learning community to collaborate on models, datasets, and applications.\",\\n              \"Offers a vast library of pre-trained models, datasets, and applications.\",\\n              \"Provides tools for building and sharing ML projects.\",\\n              \"Offers compute resources for deploying models and applications.\",\\n              \"Provides a free tier for collaboration and exploration.\",\\n              \"Offers paid compute and enterprise solutions for scaling ML projects.\",\\n              \"Provides open-source libraries like Transformers, Diffusers, and Safetensors.\",\\n              \"Offers a Pro account with advanced features and higher rate limits.\",\\n              \"Offers enterprise solutions with security, access controls, and dedicated support.\"\\n            ],\\n            \"messaging_tone\": \"Community-focused, open-source, and emphasizes collaboration and accessibility.\",\\n            \"target_audience\": \"Machine learning researchers, developers, and organizations seeking to collaborate and build AI projects.\",\\n            \"unique_selling_points\": [\\n              \"Large and active community.\",\\n              \"Extensive library of open-source models and datasets.\",\\n              \"Focus on collaboration and sharing.\",\\n              \"Free tier for exploration and development.\",\\n              \"Open-source tooling and libraries.\"\\n            ]\\n          },\\n          \"Cohere\": {\\n            \"value_propositions\": [\\n              \"Offers a platform for private and secure AI, with cutting-edge multilingual models and advanced retrieval.\",\\n              \"Provides three model families: Command, Embed, and Rerank.\",\\n              \"Offers scalable and accurate models for various enterprise use cases.\",\\n              \"Provides built-in retrieval-augmented generation (RAG) for verifiable outputs.\",\\n              \"Emphasizes enterprise-grade security and data protection.\",\\n              \"Offers flexible deployment options, including SaaS, cloud service providers, VPC, and on-premises.\",\\n              \"Provides AI solutions for complex industries like financial services, healthcare, manufacturing, energy, and public sector.\",\\n              \"Offers seamless integration with low-code solutions.\",\\n              \"Provides advanced fine-tuning capabilities.\",\\n              \"Offers collaborative development with specialists.\"\\n            ],\\n            \"messaging_tone\": \"Enterprise-focused, emphasizes security, accuracy, and scalability. Positions itself as a leader in AI for complex industries.\",\\n            \"target_audience\": \"Enterprises seeking secure, scalable, and customizable AI solutions.\",\\n            \"unique_selling_points\": [\\n              \"Focus on private and secure AI.\",\\n              \"Cutting-edge multilingual models and advanced retrieval.\",\\n              \"Built-in RAG for verifiable outputs.\",\\n              \"Flexible deployment options.\",\\n              \"AI solutions for complex industries.\"\\n            ]\\n          },\\n          \"AI21 Labs\": {\\n            \"value_propositions\": [\\n              \"Offers reliable generative AI for enterprise workflows.\",\\n              \"Provides a range of foundation models, including Jamba.\",\\n              \"Offers task-specific models for reading and writing.\",\\n              \"Provides a built-in RAG engine for conversational AI.\",\\n              \"Emphasizes enterprise-grade security and compliance.\",\\n              \"Offers dedicated integration teams for deployment support.\",\\n              \"Provides flexible deployment options (SaaS, APIs, partners, private VPC, on-premises).\",\\n              \"Offers a free trial with credits for new users.\",\\n              \"Provides transparent, usage-based pricing.\"\\n            ],\\n            \"messaging_tone\": \"Professional, enterprise-focused, and emphasizes reliability, scalability, and security. Positions itself as a partner for AI implementation.\",\\n            \"target_audience\": \"Enterprises seeking reliable and scalable generative AI solutions.\",\\n            \"unique_selling_points\": [\\n              \"Built-in RAG engine for conversational AI.\",\\n              \"Emphasis on enterprise-grade security.\",\\n              \"Dedicated integration teams for deployment support.\",\\n              \"Flexible deployment options.\",\\n              \"Transparent, usage-based pricing.\"\\n            ]\\n          },\\n          \"IBM Watson\": {\\n            \"value_propositions\": [\\n              \"Offers a long history of AI innovation, including milestones like Deep Blue and Watson\\'s Jeopardy! victory.\",\\n              \"Provides a portfolio of AI products (watsonx) for training, tuning, and deploying models.\",\\n              \"Offers watsonx.ai for model development and deployment.\",\\n              \"Provides watsonx.data for scaling AI workloads.\",\\n              \"Offers watsonx.governance for responsible AI workflows.\",\\n              \"Provides watsonx Assistant for building AI-powered virtual agents.\",\\n              \"Offers watsonx Orchestrate for automating time-consuming tasks.\",\\n              \"Provides watsonx Code Assistant for AI-powered code recommendations.\",\\n              \"Offers multiple deployment options, including cloud and on-premises.\",\\n              \"Provides consulting services through IBM Garage.\"\\n            ],\\n            \"messaging_tone\": \"Professional, emphasizes a long history of AI innovation, and focuses on enterprise-grade solutions and responsible AI.\",\\n            \"target_audience\": \"Enterprises seeking to leverage AI for various business applications.\",\\n            \"unique_selling_points\": [\\n              \"Long history of AI innovation and expertise.\",\\n              \"Comprehensive portfolio of AI products (watsonx).\",\\n              \"Focus on responsible AI and governance.\",\\n              \"Multiple deployment options.\",\\n              \"Consulting services through IBM Garage.\"\\n            ]\\n          }\\n        }\\n      },\\n      \"technical_analysis\": {\\n        \"description\": \"Technical analysis of each competitor\\'s platform, including tech stack, API capabilities, scalability features, technical advantages, and limitations.\",\\n        \"competitors\": {\\n          \"Google AI\": {\\n            \"tech_stack\": [\\n              \"TensorFlow\",\\n              \"Keras\",\\n              \"TPUs\",\\n              \"GPUs\",\\n              \"CPUs\",\\n              \"Python\",\\n              \"Java\",\\n              \"JavaScript\",\\n              \"SQL\",\\n              \"Go\"\\n            ],\\n            \"api_capabilities\": [\\n              \"Gemini API\",\\n              \"Natural Language API\",\\n              \"Speech-to-Text API\",\\n              \"Text-to-Speech API\",\\n              \"Translation API\",\\n              \"Vision API\",\\n              \"Video API\",\\n              \"Document AI API\",\\n              \"Dialogflow API\"\\n            ],\\n            \"scalability_features\": [\\n              \"Vertex AI Platform\",\\n              \"Google Kubernetes Engine (GKE)\",\\n              \"TPUs, GPUs, and CPUs\",\\n              \"Google Cloud Storage\",\\n              \"BigQuery\",\\n              \"Cloud Run\",\\n              \"AutoML\",\\n              \"Serverless options\",\\n              \"Global infrastructure\"\\n            ],\\n            \"technical_advantages\": [\\n              \"Wide range of AI models including Gemini family and open-source models\",\\n              \"Strong infrastructure with TPUs for high-performance training\",\\n              \"Comprehensive suite of AI services covering various modalities\",\\n              \"Integration with Google Cloud ecosystem\",\\n              \"Free tier for testing and development\",\\n              \"Large context windows (up to 2 million tokens with Gemini 1.5 Pro)\",\\n              \"Code assistance tools (Gemini Code Assist)\",\\n              \"Strong focus on responsible AI\"\\n            ],\\n            \"technical_limitations\": [\\n              \"Pricing can be complex and vary across different models and services\",\\n              \"Some features and models are only available on Vertex AI\",\\n              \"Rate limits can be restrictive for free tier users\",\\n              \"Grounding with Google Search has additional costs\",\\n              \"Context caching has storage limits\",\\n              \"Tuning options are limited for some models\"\\n            ]\\n          },\\n          \"Microsoft Azure AI\": {\\n            \"tech_stack\": [\\n              \"Azure Machine Learning\",\\n              \"Azure Kubernetes Service (AKS)\",\\n              \"GitHub\",\\n              \"Visual Studio\",\\n              \"Python\",\\n              \".NET\"\\n            ],\\n            \"api_capabilities\": [\\n              \"Azure OpenAI Service\",\\n              \"Azure AI Search\",\\n              \"Azure AI Content Safety\",\\n              \"Azure AI Document Intelligence\",\\n              \"Azure AI Speech\",\\n              \"Azure AI Language\",\\n              \"Azure AI Translator\",\\n              \"Azure AI Vision\"\\n            ],\\n            \"scalability_features\": [\\n              \"Azure Kubernetes Service (AKS)\",\\n              \"Azure Container Apps\",\\n              \"Azure Machine Learning\",\\n              \"Azure Cosmos DB\",\\n              \"Azure SQL Database\",\\n              \"Azure App Service\",\\n              \"Azure Functions\",\\n              \"Global infrastructure\",\\n              \"Hybrid and multicloud solutions\"\\n            ],\\n            \"technical_advantages\": [\\n              \"Integration with OpenAI models and other foundation models\",\\n              \"Comprehensive platform for building and deploying generative AI apps\",\\n              \"Strong focus on enterprise-grade security and responsible AI\",\\n              \"Seamless integration with Microsoft ecosystem (GitHub, Visual Studio)\",\\n              \"Model flexibility with a curated selection of models\",\\n              \"Built-in safety features to mitigate risks\",\\n              \"Support for full lifecycle evaluations\"\\n            ],\\n            \"technical_limitations\": [\\n              \"Pricing can be complex and vary across different services\",\\n              \"Some services are still in preview\",\\n              \"Reliance on Azure ecosystem may limit flexibility\",\\n              \"Potential vendor lock-in\"\\n            ]\\n          },\\n          \"Amazon Machine Learning\": {\\n            \"tech_stack\": [\\n              \"Amazon SageMaker\",\\n              \"AWS Deep Learning AMIs\",\\n              \"AWS Deep Learning Containers\",\\n              \"TensorFlow\",\\n              \"PyTorch\",\\n              \"Apache MXNet\",\\n              \"Jupyter\",\\n              \"Amazon EC2 (Trn1, P5, Inf2, G5 instances)\"\\n            ],\\n            \"api_capabilities\": [\\n              \"Amazon SageMaker API\",\\n              \"Hugging Face on Amazon SageMaker\",\\n              \"TensorFlow on AWS\",\\n              \"PyTorch on AWS\",\\n              \"Apache MXNet on AWS\"\\n            ],\\n            \"scalability_features\": [\\n              \"Amazon SageMaker\",\\n              \"Amazon EC2 instances (Trn1, P5, Inf2, G5)\",\\n              \"Amazon SageMaker HyperPod\",\\n              \"AWS Deep Learning AMIs and Containers\",\\n              \"Global infrastructure\"\\n            ],\\n            \"technical_advantages\": [\\n              \"Comprehensive set of ML services and infrastructure\",\\n              \"Purpose-built infrastructure for distributed training (SageMaker HyperPod)\",\\n              \"Support for various ML frameworks (TensorFlow, PyTorch, MXNet)\",\\n              \"Wide range of EC2 instances optimized for AI workloads\",\\n              \"Integration with AWS ecosystem\",\\n              \"Strong focus on responsible AI\"\\n            ],\\n            \"technical_limitations\": [\\n              \"Pricing can be complex and vary across different services and instances\",\\n              \"Steep learning curve for some services\",\\n              \"Can be expensive for large-scale training and inference\",\\n              \"Requires expertise in AWS ecosystem\"\\n            ]\\n          },\\n          \"Hugging Face\": {\\n            \"tech_stack\": [\\n              \"Transformers\",\\n              \"Diffusers\",\\n              \"Safetensors\",\\n              \"Python\",\\n              \"PyTorch\",\\n              \"TensorFlow\",\\n              \"JAX\",\\n              \"JavaScript\"\\n            ],\\n            \"api_capabilities\": [\\n              \"Inference API\",\\n              \"Text Generation Inference\",\\n              \"Hub Python Library\"\\n            ],\\n            \"scalability_features\": [\\n              \"Inference Endpoints\",\\n              \"Spaces Hardware upgrades (CPU, GPU, TPU)\",\\n              \"Persistent Storage options\",\\n              \"Community collaboration platform\"\\n            ],\\n            \"technical_advantages\": [\\n              \"Large community and open-source focus\",\\n              \"Extensive library of pre-trained models and datasets\",\\n              \"Easy deployment of models with Inference Endpoints and Spaces\",\\n              \"Support for various modalities (text, image, audio, video, 3D)\",\\n              \"Free tier for collaboration and experimentation\",\\n              \"Strong focus on accessibility and democratization of AI\",\\n              \"Integration with popular ML frameworks\"\\n            ],\\n            \"technical_limitations\": [\\n              \"Compute resources can be limited for free tier users\",\\n              \"Enterprise features require paid subscriptions\",\\n              \"May require more technical expertise for advanced use cases\",\\n              \"Reliance on community contributions for some models and datasets\"\\n            ]\\n          },\\n          \"Cohere\": {\\n            \"tech_stack\": [\\n              \"Proprietary models\",\\n              \"Transformer architecture\"\\n            ],\\n            \"api_capabilities'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_json_to_markdown(result, report_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
